\documentclass[preprint]{sigplanconf}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{alltt}
\usepackage{url}
\usepackage{natbib}
\usepackage{datetime}
\usepackage{comment}

% from the ICFP website
\bibpunct();A{},
\let\cite=\citep

%include polycode.fmt
%include catch.fmt

% general stuff
\newcommand{\T}[1]{\texttt{#1}}
\newcommand{\C}[1]{\textsf{#1}}
\newcommand{\tup}[1]{\ensuremath{\langle #1 \rangle}}
\newcommand{\mtxt}[1]{\textsf{#1}}

\newcommand{\catch}{\textsc{Catch}}
\newcommand{\newtool}{\anon{\catch07}{\catch}}
\newcommand{\oldtool}{\anon{\catch05}{\textsc{MR-Checker}}}

% examples
\newcounter{exmp}
\setcounter{exmp}{0}
\newcommand{\yesexample}{\addtocounter{exmp}{1}\addvspace{2mm}\noindent\textbf{Example \arabic{exmp}}}
\newcommand{\noexample}{\hfill$\Box$\par\addvspace{2mm}}
\newcommand{\lastexample}{\arabic{exmp}}

\newcommand{\todo}[1]{\textbf{\textsc{Todo:} #1}}

\newcommand{\para}[1]{\vspace{2mm}\noindent\textbf{#1}}

\newcommand{\anon}[2]{#2}

% code blocks
\newenvironment{codepage}
    {\begin{minipage}[h]{\textwidth}\begin{code}}
    {\end{code}\end{minipage}}

\newenvironment{discuss}
    {\noindent\hspace{-1.5mm}\vline\hspace{1mm}\vline\hspace{1mm}\begin{minipage}[h]{\linewidth}}
    {\end{minipage}}


\newenvironment{example}{\yesexample}{\noexample}

\newenvironment{revisit}[1]
    {\addvspace{2mm}\noindent\textbf{Example #1 (revisited)}}
    {\noexample}

\newcommand{\K}{\ensuremath{^\ast}} % kleene star
\newcommand{\D}{\ensuremath{\cdot}} % central dot

\renewcommand{\c}[3]{\tup{\T{#1},\T{#2},\T{\{#3\}}}}
\newcommand{\cc}[2]{\c{#1}{$\lambda$}{#2}}

\newcommand{\s}[1]{\ensuremath{_{\tt #1}}} % subscript, in tt font
\newcommand{\g}[1]{\{#1\}} % group, put { } round it
\newcommand{\U}{\textunderscore}
\newcommand{\vecto}[1]{\overrightarrow{#1\;}}
\newcommand{\gap}{\;\;}
\newcommand{\dom}{\text{dom}}


\begin{document}

\conferenceinfo{ICFP '07}{date, City.} %
\copyrightyear{2007} %
\copyrightdata{[to be supplied]}

\titlebanner{\today{} - \currenttime{}}        % These are ignored unless
\preprintfooter{}   % 'preprint' option specified.

\title{Not All Patterns, But Enough}
\subtitle{ -- an automatic verifier for partial but sufficient pattern matching}

\authorinfo{\anon{Neil Mitchell}{Author 1}}
           {\anon{University of York, UK}{Affiliation 1}}
           {\anon{\http://www.cs.york.ac.uk/$\sim$ndm/}{Contact Details 1}}
\authorinfo{\anon{Colin Runciman}{Author 2}}
           {\anon{University of York, UK}{Affiliation 2}}
           {\anon{http://www.cs.york.ac.uk/$\sim$colin/}{Contact Details 2}}

\maketitle

\begin{abstract}
\begin{discuss}
A Haskell program may fail at runtime with a pattern-match error if the program has any incomplete (non-exhaustive) patterns in definitions or case alternatives. This paper describes a static checker that allows non-exhaustive patterns to exist, yet can verify that a pattern-match error does not occur. We describe a constraint language that can be used to express requirements for safe pattern matches, along with mechanisms to propagate and reason about these constraints between program components.

The checker we develop using these methods obtains minimal preconditions for 8 out of 14 programs in the Imaginary section of the Nofib suite. With minor modifications a further 5 are proven safe. We also check two larger example programs with success, finding several previously unknown errors.
\end{discuss}
\end{abstract}

\category{D.3}{Software}{Programming Languages}

\terms
languages, verification

\keywords
Haskell, automatic verification, functional programming, pattern-match errors, preconditions

\section{Introduction}
\label{sec:introduction}

\begin{discuss}
Often it is useful to define selector functions, which are defined only for arguments matching a specific constructor pattern, for example \C{head} fails on the empty list. Unfortunately programs with incomplete pattern matches may fail at runtime.
\end{discuss}

The problem of pattern match failures is a serious one. The darcs \citep{darcs} project is one of the most successful large scale programs written in Haskell. Taking a look at the darcs bug tracker, 13 problems are errors related to the selector function \C{fromJust} and 19 are direct pattern match failures.

Consider the following example taken from \citep{me:catch_tfp}:

\begin{code}
risers :: Ord alpha => [alpha] -> [[alpha]]
risers [] = []
risers [x] = [[x]]
risers (x:y:etc) = if x <= y then (x:s):ss else [x]:(s:ss)
    where (s:ss) = risers (y:etc)
\end{code}

A sample application of this function is:

\begin{code}
> risers [1,2,3,1,2]
[[1,2,3],[1,2]]
\end{code}

In the last line of the definition, |(s:ss)| is matched against the result of |risers (y:etc)|. If the result is in fact an empty list a pattern match error will be raised. It takes a few moments to check manually that no pattern-match failure can occur -- and a few more to be sure one has not made a mistake! Turning the \C{risers} function over to our checker, the output is:

\begin{verbatim}
> Incomplete pattern on line 5, proven safe
> Program is Safe
\end{verbatim}

In addition the checker produces a set of properties it has proved about Risers, along with an outline of the proofs. Fuller details of how the checking is performed follow in \S\ref{sec:walkthrough}.

\subsection{Contributions}

The contributions of this paper include:

\begin{itemize}
\item Two constraint languages to reason about pattern-match failures with methods for generating and solving appropriate constraints.
\item Details of an implementation which handles the complete Haskell 98 language \citep{haskell} by transforming Haskell 98 programs to a first-order language.
\item Results showing success on a number of small examples drawn from the Nofib suite \citep{nofib}, and for two larger examples, investigating the scalability of the checker.
\end{itemize}

In \citet{me:catch_tfp} a pattern match checker was described with similar aims, which we will refer to as \oldtool{}\anon{\footnote{Although the paper was completed in 2005, publication was delayed}}{}. For \oldtool{} \C{risers} was towards the boundary of what was possible, for \newtool{} it is trivial. Scalability problems have been addressed and \newtool{} handles real Haskell programs, with no restriction on the recursion patterns, higher-order functions or type classes. The underlying constraint mechanism of \newtool{} is radically different from \oldtool{}.

\subsection{Road map}

\S\ref{sec:walkthrough} gives an overview of the checking process for the \C{risers} function. \S\ref{sec:manipulate} introduces a small core functional language and a mechanism for reasoning about this language, \S\ref{sec:constraint} describes two constraint languages. \S\ref{sec:transform} discusses how to transform Haskell to a first-order core language. \S\ref{sec:results} evaluates \newtool{} on various sample programs -- including the Nofib benchmark suite. \S\ref{sec:related} compares our work to other work before \S\ref{sec:conclusion} presents concluding remarks.

\newpage
\section{Walkthrough of Risers}
\label{sec:walkthrough}

This section sketches the process of checking that the \C{risers} function in the Introduction does not crash with a pattern-match error.


\subsection{Conversion to a Core Language}

Rather than analyse full Haskell, \catch{} analyses a first-order Core language, without lambda expressions, partial application or let bindings. A convertor is provided from the full Haskell 98 language to this restricted language -- see \S\ref{sec:transform}. The result of converting the \C{risers} program to Core Haskell, and renaming the identifiers for ease of human reading is shown in Figure \ref{fig:risers_core}.

The type of \C{risers} is polymorphic over types in |Ord|. \catch{} could check \C{risers} assuming that |Ord| methods do not raise pattern match errors, and may return any value. Or a type instance such as |Int| can be specified with a type signature.

\begin{figure}
\begin{code}
risers x = case x of
    [] -> []
    (y:ys) ->  case ys of
         [] -> (y : []) : []
         (z:zs) -> risers2 (risers3 z zs) (y <= z) y

risers2 x y z =  case y of
    True -> (z : snd x) : (fst x)
    False -> (z : []) : (snd x : fst x)

risers3 x y = risers4 (risers (x : y))

risers4 x = case x of
    (y:ys) -> (ys, y)
    [] -> error "Pattern Match Failure, 11:12."
\end{code}
\caption{\C{risers} in the Core language.}
\label{fig:risers_core}
\end{figure}

\subsection{Analysis of \C{risers} -- a brief sketch}

In the Core language every pattern match covers all possible constructors of the appropriate type. The alternatives for constructor cases not originally given are calls to \C{error}. The analysis starts by finding calls to \C{error}, then tries to prove that these calls will not be reached. The one \C{error} call in \C{risers4} is avoided under the precondition (see \S\ref{sec:precond}):

\begin{code}
risers4, x :< (:)
\end{code}

That is, all callers of \C{risers4} must supply an argument which is a |(:)|-constructed value. For the proof that this precondition holds, two properties are required (see \S\ref{sec:backward}):

\begin{code}
x :< (:)  => (risers x       ) :< (:)
True      => (risers2 x y z  ) :< (:)
\end{code}

\noindent The first property says that if the argument to \C{risers} is a |(:)|-constructed value, the result will be. The second states that the result from \C{risers2} is always |(:)|-constructed.

\section{Pattern Match Analysis}
\label{sec:manipulate}

This section explains the methods used to calculate preconditions for functions. A basic constraint language is introduced, along with operations upon it.

\subsection{Reduced expression language}
\label{sec:core}

\begin{figure}
\begin{code}
type CtorName  =  String
type FuncName  =  String
type VarName   =  String
type Selector  =  (CtorName, Int)

data Func  =  Func FuncName [VarName] Expr

data Expr  =  Var   VarName
           |  Make  CtorName  [Expr]
           |  Call  FuncName  [Expr]
           |  Case  Expr      [Alt]

data Alt   =  Alt CtorName [VarName] Expr
\end{code}
\caption{Core Data Type.}
\label{fig:core}
\end{figure}

\begin{figure}
\begin{code}
data Value = Bottom | Value CtorName [Value]

eval :: Expr -> Value
eval x = case  hnf x of
               Nothing      -> Bottom
               Just (c,cs)  -> Value c (map eval cs)

-- Nothing corresponds to bottom
hnf :: Expr -> Maybe (CtorName, [Expr])
hnf (Make  x   xs    )  =  Just (x, xs)
hnf (Call  x   xs    )  |  x == "error"  = Nothing
                        |  otherwise     = hnf (instantiate x xs)
hnf (Case  on  alts  )  =  listToMaybe [hnf (subst (zip vs cs) e)
       | Just (c,cs) <- [hnf on], Alt n vs e <- alts, c == n]

subst :: [(VarName, Expr)] -> Expr -> Expr
subst r (Var   x     ) = fromJust (lookup x r)
subst r (Make  x xs  ) = Make  x (map (subst r) xs)
subst r (Call  x xs  ) = Call  x (map (subst r) xs)
subst r (Case  x xs  ) = Case (subst r x)
    [Alt n vs (subst r e) | Alt n vs e <- xs]
\end{code}
\caption{Semantics for Core expressions.}
\label{fig:semantics}
\end{figure}

Syntax for the core language is given in Figure \ref{fig:core}. Our Core language is a little more restrictive than the core languages typically used in compilers \citep{ghc_core}. It is first order, has only simple case statements, and only algebraic data types. All case statements have alternatives for all constructors, with error being introduced where a pattern-match error would otherwise occur.

The evaluation strategy is lazy. A semantics is outlined in Figure \ref{fig:semantics}. The |hnf| function evaluates an expression to head normal form. The |subst| function substitutes free variables that are the result of a |Case| expression. Laziness is a useful property as it allows $\beta$-reduction to be performed by the checker.

\subsubsection{Operations on Core}

\begin{figure}
\begin{code}
ctors        :: CtorName  -> [CtorName]
arity        :: CtorName  -> Int
var          :: VarName   -> Maybe (Expr, Selector)
instantiate  :: FuncName  -> [Expr] -> Expr
isRec        :: Selector  -> Bool
\end{code}
\caption{Operations on Core.}
\label{fig:core_operations}
\end{figure}

Figure \ref{fig:core_operations} gives the signatures for helper functions over the core data type. Every constructor has an arity, which can be obtained with the \C{arity} function. To determine alternative constructors the \C{ctors} function can be used, for example |ctors True = {False, True}| and |ctors [] = {[], :}|. The \C{var} function returns \C{Nothing} for a variable bound as an argument of a top-level function, and |Just (e,(c,i))| for a variable bound as the |i|th component in the |c|-constructed alternative of case-expression |e|. The function \C{instantiate} corresponds to $\beta$ reduction. The |isRec (c,n)| function returns true if the constructor |c| has a recursive |n|th component.

\subsubsection{Algebraic Abstractions of Primitive Types}
\label{sec:abstraction}

\begin{comment}
Natural numbers are often encoded by Peano numerals, and this idea can easily be extended to integers:

\begin{code}
data Pos  = One | Succ Pos
data Int  = Minus Pos | Zero | Plus Pos
\end{code}

Although this abstraction of \C{Int} captures all the underlying detail of the number system, the underlying constraint systems discussed in \S\ref{sec:constraint} would be unable to distinguishing between any pair of numbers both greater than 2, or both less than -2.
\end{comment}

The Core language only has algebraic data types. \catch{} allows for primitive types such as characters and numbers by abstracting them into algebraic types. For example, the integer abstraction used in \catch{} is simply:

\begin{code}
data Int = Neg | Zero | One | Pos
\end{code}

In our experience, numbers are most often constrained to be a natural, or to be non-zero. Addition or subtraction of one is the most common operation. Though very simple, the abstraction models the common properties and operations quite well.

Possible abstractions of characters include:

\begin{code}
data Char = Char
data Char = Alpha | Digit | White | Other
data Char = 'A' .. 'Z' | 'a' .. 'z' | '0' .. '9' | Other
data Char = '\0' | '\1' | '\2' ..
\end{code}

\noindent The simplest abstraction is that all characters are the same. A refinement partitions characters based on the character testing functions predefined in Haskell. Refining the model further distinguishes common characters only. Finally, each character could be represented distinctly. \catch{} employs the first abstraction by default, but provides a flag to allow alternatives. In practice, few additional programs are proven safe with a more refined character abstraction.

The final issue of abstraction relates to primitive functions in the \C{IO} monad, such as \C{getArgs} (which returns the command-line arguments), or \C{readFile} (which reads from the file-system). In most cases an IO function is modelled as returning \textit{any} value of the correct type, using a function primitive to the checker.


\subsection{Constraint Essentials and Notation}
\label{sec:constraints}

An expression in our first-order core language evaluates to a data structure. A constraint describes a set of data structures to which a value may belong. If a component within a data structure evaluates to $\bot{}$, there are no constraints upon it.

If |x| is an expression and |c| is a constraint we write |x :< c| to assert that the value of expression |x| must be a member of the set satisfying |c|. It is convenient to have a notation for constraints upon named arguments of named functions: we write |f, x :< c| to assert that argument |x| of |f| must be in the set satisfying |c|. These atomic constraints can be combined using logical operators $\wedge{}$ and $\vee{}$ to form propositional constraints.

Several underlying constraint models are possible. To keep the introduction of the algorithms simple we first use \textit{basic pattern constraints} (\S\ref{sec:basic}). We then describe \textit{regular expression constraints} in \S\ref{sec:regexp} -- a variant of the constraints used in \oldtool{}. Finally we present \textit{multi-pattern constraints} in \S\ref{sec:multipattern} -- used in \newtool{} to enable scaling to much larger problems.

\subsection{Bounded Constraints}
\label{sec:bounded}

\begin{discuss}
We require that for any type, there are finitely many constraints. This property is used for termination proofs.

Mention that this lemma is required for termination.

This approach works provided no selectors have a type that is structurally greater than the defined type, and no higher-kinded type variables are used. If the presence of definitions involving these features, either a translation can be made to a permitted form, or \C{isRec} can conservatively return True.

More formally, there exists a function |bounded t| such that the number of non-recursive selectors in any path of any value of type |t| is less than this value.

\begin{code}
forall x,t,p `o`  x :: t && p `elem` paths x =>
                  length (filter (not . isRec) p) < bounded t
\end{code}

A path is a list of selectors, the \C{paths} function returns all paths in a given data structure -- a potentially infinite number.
\end{discuss}


\subsection{Basic Pattern (BP) Constraints}
\label{sec:basic}

\begin{figure}
\begin{code}
type Constraint = [Pattern]

data Pattern  =  Con CtorName [Pattern]
              |  Any
\end{code}
\caption{Basic pattern constraints.}
\label{fig:basic}
\end{figure}

\begin{figure}
\begin{code}
data Assert a = a :< Constraint

notin :: CtorName -> Constraint
(|>) :: Selector -> Constraint -> Constraint
(<|) :: CtorName -> Prop (Assert Int)
\end{code}
\caption{Constraint operations.}
\label{fig:constraint}
\end{figure}

For simplicity, our analysis framework will be introduced using basic pattern constraints (BP-constraints). A BP-constraint directly corresponds to Haskell pattern matching. A constraint is a finite set of patterns represented as in Figure \ref{fig:basic}. A given data structure |d| satisfies a constraint |ps| if |d| matches at least one pattern in |ps|. True and false constraints can be represented as |[Any]| and |[]| respectively. The BP-constraint language is limited in expressivity. For example it is impossible to state that all the elements of a boolean list are True. However, even with this limited constraint language the Risers example can be proven safe.

Some operations must be provided in every constraint implementation. Signatures are given in Figure \ref{fig:constraint}. \C{Assert} is the type of the |(:<)| condition, introduced in \ref{sec:constraints}. The lifting and splitting operators |(||>)| and |(<||)| are discussed in \S\ref{sec:backward}. The \C{notin} function generates a constraint which does \textit{not} match the constructor given. For example, for the BP-constraint language:

\begin{code}
notin c = map f (delete c (ctors c))
   where f x = Pattern x (replicate (arity c) Any)
\end{code}

With unbounded recursion in patterns, the BP-constraint language does \textit{not} have finitely many constraints per type. We could limit the depth of patterns, but instead ignore the issue for now. Both constraint systems in \ref{sec:constraint} have the finiteness property.

\subsection{Preconditions for Pattern Safety}
\label{sec:precond}

\begin{figure}
\begin{code}
pre :: Expr -> Prop (Assert Expr)
pre (Var   x         ) = True
pre (Make  _   xs    ) = and (map pre xs)
pre (Call  fn  xs    ) = preFunc fn xs && and (map pre xs)
pre (Case  on  alts  ) = pre on && and [f c e | Alt c vs e <- alts]
    where f c e = (on :< notin c) || pre e

preFunc :: FuncName -> [Expr] -> Prop (Assert Expr)
\end{code}
\caption{Precondition of an expression, \C{pre}.}
\label{fig:precondition}
\end{figure}

Our intention is that for every function, constraints on the arguments form a precondition for pattern safety. The precondition for \C{error} is False. A program is safe if the precondition on \C{main} is True. The process of analysis can be seen as deriving these preconditions. Given a function \C{preFunc} that returns the precondition on a function's arguments, a function to determine the precondition for safe evaluation of an \textit{expression} can be specified as \C{pre} in Figure \ref{fig:precondition}.

\subsubsection{Stable Preconditions}

\begin{discuss}
The preconditions given by |preFunc| should have the following property:

\begin{code}
forall fn, xs `o` preFunc fn xs => pre (instantiate fn xs)
\end{code}

That is, the preconditions assigned to each function by \C{preFunc} must imply the preconditions calculated on the instantiation of that function with its arguments. One conservative implementation is that all calls to \C{preFunc} return False. A perfect precondition would be such that |preFunc f xs == pre (instantiate f xs)|.
\end{discuss}

\begin{figure}
\begin{code}
for fn `elem` funcs do conds(fn) <- (fn /= "error")
loop
    for fn `elem` funcs do
        conds'(fn) <- conds(fn) && pre (instantiate fn vs)
    if conds' == conds then break
    conds <- conds'
end loop
    where
        preFunc fn xs = cond(fn)[vs1/xs1 .. vs_n/xs_n]
        vs = ... -- free variables
\end{code}
\caption{Precondition calculation.}
\label{fig:precond_fixp}
\end{figure}

The iterative algorithm for calculating preconditions is given in Figure~\ref{fig:precond_fixp}. Initially all preconditions are assumed to be True, apart from the \C{error} precondition, which is False. New preconditions are calculated for every function, until no condition changes. In each iteration the preconditions become more restrictive, so given a constraint model with a finite number of terms, termination is guaranteed. A more efficient algorithm tracks dependencies between preconditions, and performs the minimum amount of recalculation. Finding strongly connected components in the static call graph of a program would allow parts of the program to be checked separately.

\subsubsection{Preconditions and Laziness}

The \C{pre} function defined in Figures \ref{fig:precondition} does not exploit laziness. The \C{Call} equation demands that preconditions hold on \textit{all} arguments -- only correct if a function is strict in all arguments. For example, the precondition on |False user_and error "here"| is False, when it should be True. In general, preconditions may be more restrictive than necessary. However, investigation of a range of examples suggests that inlining |(user_and)| and |(user_or)| captures many of the common cases where laziness would be required.


\subsection{Manipulating constraints}
\label{sec:backward}

\begin{figure}
\begin{code}
back :: Assert Expr -> Prop (Assert Expr)
back (Var   x         :< k) = on :< (c |> k)
    where Just (on, c) = var x
back (Make  c   xs    :< k) = replaceVars xs (c <| k)
back (Case  on  alts  :< k) = and [f c e | Alt c vs e <- alts]
    where f c e = (on :< notin c) || (e :< k)
back (Call  fn  xs    :< k) = replaceVars xs (property fn k)

backs :: Prop Assert -> Prop (Assert VarName)
property :: FuncName -> Constraint -> Prop (Assert Int)

replaceVars :: [Expr] -> Prop (Assert Int) -> Prop (Assert Expr)
replaceVars xs p = mapProp (\(i:<k) -> (xs!!i) :< k) p
\end{code}
\caption{Specification of backward analysis, \C{back}}
\label{fig:backward}
\end{figure}

Constraints on expressions other than argument variables can be rewritten using the \C{back} function, detailed in Figure \ref{fig:backward}. The \C{backs} function repeatedly applies \C{back} until all constraints are on arguments. The \C{replaceVars} function takes a propositional constraint over a set of index variables, |v1..v_n|, and replaces each variable with a corresponding expression from an indexed list.

\para{The \C{Var} rule} applies to variables bound by patterns in case alternatives. It lifts a conditions on a bound variable to the subject of the |case| expression. The | ||>| operator lifts a constraint on one part of a data structure to a constraint on the entire data structure. For BP-constraints | ||>| can be defined:

\begin{code}
(c,i) |> k = map extend k
    where
    extend x = Pattern c (anys i ++ [x] ++ anys (n-i))
    anys j = replicate j Any
    n = arity c
\end{code}

\para{The \C{Make} rule} deals with an application of a constructor. The |<||| operator splits a constraint on an entire structure into a combination of constraints on each part.

\begin{code}
c <| k = propOr (map f k)
    where
    f Any = Any
    f (Pattern c2 xs) = c2 == c &&
        and (zipWith (:<) [0..] (map (:[]) xs))
\end{code}

\begin{discuss}
\para{The \C{Case} rule} generates a conjunct for each alternative. The generated condition says either the subject of the case analysis has a different constructor (so this particular alternative is not executed in this circumstance), or the right hand side of the alternative is safe given the conditions for this expression.
\end{discuss}

\para{The \C{Call} rule} relies on the key property:

\begin{code}
forall fn, xs, k `o`  backs (Call fn xs :< k) =>
                      backs (instantiate fn xs :< k)
\end{code}

That is, the result is at least as restrictive as if the function was inlined at this point. Two instantiation strategies can be used: (1) Use $\beta$ substitution (\C{instantiate}) replacing the call with the body of |fn|. (2) Create fresh variables for each element in |xs|, solve this new problem, and then instantiate the results. \oldtool{} used method 1. \newtool{} uses method 2, as it is more amenable to finding a fixed point, and allows a cache of results to be built from which future questions can be answered.

\begin{figure}
\begin{code}
ps <- [(fn,k) | fn <- funcs, k <- constraints]
for (fn,k) `elem` ps do prop(fn,k) := True
loop
    for (fn,k) `elem` p do
        prop'(fn,k) := prop(fn,k) && backs (instantiate fn vs :< k)
    if prop' == prop then break
    prop <- prop'
end loop
    where
        property fn k = prop(fn,k)
\end{code}
\caption{Fixed point calculation for \C{property}.}
\label{fig:property_fixp}
\end{figure}

\begin{discuss}
Answers can be calculated by obtaining a fixed point as detailed in Figure \ref{fig:property_fixp}. The function \C{property} generates fresh variables for each of the arguments, and solves this new problem. As in the precondition calculation in \S\ref{sec:precond}, each result is anded with its existing value. As there are finitely many constraint/function pairs termination is guaranteed. Here again, a speed up can be obtained by tracking the dependencies between constraints.
\end{discuss}

\subsection{Semantics of Constraints}

A key function in the semantics of constraints tests whether a value satisfies a constraint. The |(||>)| operator already provides all the necessary information:

\begin{code}
data Value = Value CtorName [Value]

satisfies :: Value -> Constraint -> Bool
satisfies (Value c xs) = mapProp f (c <| k)
    where f (i :< k2) = satisfies (xs !! i) k2
\end{code}

Now we can express properties that the other constraint operations must have:

\begin{code}
forall v, k, i  `o` satisfies v ((c,i) |> k)
                => (c2 /= c || satisfies (x !! i) k)
    where Value c2 xs = v

forall v, c `o` satisfies v (notin c) => (valueCtor v /= c)
    where valueCtor (Value c2 xs) = c2
\end{code}

\noindent Note that both properties are implications, not equivalences, as constraints may be more restrictive than necessary.

\section{Richer Constraint Systems}
\label{sec:constraint}

There are various ways of defining a richer constraint system. Here we outline two -- one adapted from \oldtool{}, the other used in \newtool{}. Neither is strictly more powerful than the other; each is capable of expressing constraints that the other cannot express.

\subsection{Regular Expression (RE) Constraints}
\label{sec:regexp}

\begin{figure}
\begin{code}
data Constraint = RegExp :- [CtorName]

type RegExp = [Atom]

data Atom  =  Atom  Selector
           |  Star  [Selector]

notin :: CtorName -> Constraint
notin c = [] :- delete c (ctors c)

(|>) :: Selector -> Constraint -> Constraint
p |> (r :- cs) = integrate p r :- cs

(<|) :: CtorName -> Constraint -> Prop (Assert Int)
c <| (r :- cs) = (ewp r => c `elem` cs) &&
    and (zipWith f (paths c) [0..])
    where
    f p x = case  differentiate p r of
                  Nothing  -> True
                  Just r2  -> x :< (r2 :- cs)

ewp :: RegExp -> Bool
ewp x = all isStar x
   where  isStar (Star  _) = True
          isStar (Atom  _) = False

integrate :: Selector -> RegExp -> RegExp
integrate p r | not (isRec p)  = Atom p : r
integrate p (Star ps:r)        = Star (nub (p:ps) : r)
integrate p r                  = Star [p] : r

differentiate :: Selector -> Path -> Maybe Path
differentiate p [] = Nothing
differentiate p (Atom  r:rs)  | p == r     = Just rs
                              | otherwise  = Nothing
differentiate p (Star  r:rs)  | p `elem` r  = Just (Star r:rs)
                              | otherwise   = differentiate p rs
\end{code}
\caption{RE-constraints.}
\label{fig:regexp}
\end{figure}

\oldtool{} used regular expressions in constraints. A data type for regular expression based constraints (RE-constraints), along with the essential operations upon it is given in Figure \ref{fig:regexp}. In a constraint of the form |(r :- cs)|, |r| is a regular expression and |cs| is a set of constructors. Such a constraint is satisfied by a data structure |d| if every well-defined application to |d| of a sequence of selectors described by |r| reaches a constructor in the set |cs|. That is:

\begin{code}
d :< (r :- c) <=> (forall l `elem` lang r `o` defined d l => ctor d l `elem` c)
\end{code}

If no sequence of selectors has a well-defined result then the constraint is vacuously true.

Concerning the helper functions needed to define | ||>| and |<||| in Figure \ref{fig:regexp}, the differentiation operation is defined by \citet{conway:regexp}; integration is its inverse; \C{ewp} is the empty word property.

\oldtool{} regular expressions were unrestricted and quickly grew to an unmanageable size, thwarting analysis of larger programs. In general, a regular expression takes one of six forms:\\ \\
\begin{tabular}{ll}
|s+t|  & union of regular expressions |s| and |t| \\
|s^.t| & concatenation of regular expressions |s| then |t| \\
|s^*|  & any number (possibly zero) occurrences of |s| \\
\C{x}  & a selector, i.e. |(:,0)| for the head of a list \\
       & We use \C{hd} to abbreviate |(:,0)| and \C{tl} to abbreviate |(:,1)|. \\
0      & the language is the empty set \\
1      & the language is the set containing the empty string
\end{tabular} \\

\begin{discuss}
\newtool{} restricts REs to concatenations of selectors, or stars of unions:
\end{discuss}

\[\begin{array}{lllll}
r & = & 1     & || & |a^.r| \\
a & = & \C{x} & || & |u^*| \\
u & = & \C{x} & || & \C{x} + u
\end{array}\]

\begin{discuss}
In addition, all selectors for which \C{isRec} returns true must be under \C{Star}, all which are not must be \C{Atom}, and \C{Star}s cannot be concatenated. These restrictions are motivated by three observations:
\end{discuss}

\begin{itemize}
\item \begin{discuss} Constructor-sets, because of static typing and the restricted form of regular expression, must all be of the same type. (In \oldtool{} expressions such as |hd^*| could arise.)\end{discuss}

\item There are finitely many regular expressions for any type. Combined with the finite number of constructors, this property is enough to guarantee termination when computing a fixed-point iteration on constraints.

\item The restricted REs with 0 are closed under integration and differentiation. (The 0 alternative is catered for by the \C{Maybe} return type in the differentiation. As |0 :- c| always evaluates to True, |(<||)| replaces \C{Nothing} by True.)
\end{itemize}

\begin{example}
|(head xs)| is safe if |xs| evaluates to a non-empty list. The RE-constraint is: |xs :< (1 :- {:})|
\end{example}

\begin{example}
|(map head xs)| is safe if |xs| evaluates to a list of non-empty lists. The RE-constraint is: |xs :< (tl^* ^. hd :- {:})| If |xs| is empty, it still satisfies the constraint. If |xs| is infinite then the condition applies to all elements, constraining an infinite number.
\end{example}

\begin{example}
|(map head (reverse xs))| is safe if every item in |xs| is |(:)|-constructed, or if |xs| is infinite -- so \C{reverse} does not terminate. The RE-constraint is: |xs :< (tl^* ^. hd :- {:}) || x :< (tl^* :- {:})|
\end{example}

\subsubsection{Finite Number of RE-Constraints}
\label{sec:finite_re}

We require that for any type, there are finitely many constraints (see \S\ref{sec:bounded}). We can model types as:

\begin{code}
type Type  = [Ctor]
type Ctor  = [Maybe Type]
\end{code}

Each \C{Type} has a number of constructors. For each constructor \C{Ctor}, every component has either a recursive type (represented as \C{Nothing}) or a non-recursive type |t| (represented as |Just t|). As each non-recursive type is structurally smaller than the original, a function that recurses on the type will terminate. We define a function \C{count} which takes a type and returns the number of possible RE-constraints.

\begin{code}
count :: Type -> Integer
count t = 2 ^ rec * (2 ^ ctor + sum (map count nonrec))
    where
    rec = length (filter isNothing (concat t))
    nonrec = [x | Just x <- concat t]
    ctor = length t
\end{code}

The |2^rec| term corresponds to the number of possible constraints under \C{Star}. The |2^ctor| term accounts for the case where the selector path is empty.


\subsection{RE-Constraint Propositions}
\label{sec:re-propositions}

\catch{} computes over propositional formulae with constraints as atomic propositions. Among other operators on propositions, they are compared for equality to obtain a fixed point. All the fixed-point algorithms given in this paper stop once equal constraints are found. We use Binary Decision Diagrams (BDD) \citep{lee:bdd} to make these equality tests fast. Since the complexity of performing an operation is often proportional to the number of atomic constraints in a proposition, we apply simplification rules to reduce this number. Three of the nineteen rules are:

\para{Exhaustion:} In the constraint |x :< (r :- {:,[]})| the condition lists all the possible constructors. Because of static typing |x| must be one of these constructors. Any such constraint simplifies to True.

\para{And merging:} The conjunction |e :< (r :- c1) && e :< (r :- c2)| can be replaced by |e :< (r :- (c1 `union` c2))|.

\para{Or merging:} The disjunction |e :< (r1 :- c1) |||| e :< (r2 :- c2)| can be replaced by |e :< (r2 :- c2)| if |r1 `subseteq` r2 && c1 `subseteq` r2|.


\subsection{Multipattern (MP) Constraints \& Simplification}
\label{sec:multipattern}

\begin{figure}
\begin{code}
type Constraint = [Val]

data Val      =  [Pattern] :* [Pattern]
              |  Any
data Pattern  =  Pattern CtorName [Val]

-- useful auxiliaries, non recursive selectors
nonRecs :: CtorName -> [Int]
nonRecs c = [i | i <- [0..arity c-1], not (isRec (c,i))]

-- a complete Pattern on |c|
complete :: CtorName -> Pattern
complete c = Pattern c (map (const Any) (nonRecs c))

notin :: CtorName -> Constraint
notin c = [map complete (delete c cs) :* map complete cs]
    where cs = ctors c

(|>) :: Selector -> Constraint -> Constraint
(c,i) |> k = notin c ++ map f k
    where
    f Any = Any
    f (ms1 :* ms2) | isRec (c,i) = [complete c] :* merge ms1 ms2
    f v =  [Pattern c [if i == j then v else Any | j <- nonRecs c]]
           :* map complete (ctors c)

(<|) :: CtorName -> Constraint -> Prop (Assert Int)
c <| vs = or (map f vs)
    where
    (rec,non) = partition (isRec . (,) c) [0..arity c-1]

    f Any = propTrue
    f (ms1 :* ms2) = or [g vs1 | Pattern c1 vs1 <- ms1, c1 == c]
        where g vs =  and (zipWith (:<) non (map (:[]) vs)) &&
                      and (map (:< [ms2 :* ms2]) rec)

(`mergeVal`) :: Val -> Val -> Val
(a1 :* b1) `mergeVal` (a2 :* b2)  = merge a1 a2 :* merge b1 b2
Any        `mergeVal` x           = x
x          `mergeVal` Any         = x

merge :: [Pattern] -> [Pattern] -> [Pattern]
merge  ms1 ms2 = [Pattern c1 (zipWith (`mergeVal`) vs1 vs2) |
       Pattern c1 vs1 <- ms1, Pattern c2 vs2 <- ms2, c1 == c2]
\end{code}
\caption{MP-constraints.}
\label{fig:enumeration}
\end{figure}

Although RE-constraints are capable of solving many problems, they suffer from a problem of scale. As programs become more complex the size of the propositions grows quickly, slowing the checker unacceptably. Multipattern constraints (MP-constraints, defined in Figure \ref{fig:enumeration}) are an alternative which scales better.

MP-constraints are similar to BP-constraints, but can constrain an infinite number of items. A value |v| satisfies a constraint |p1 :* p2| if |v| itself satisfies |p1| and all its recursive components satisfy |p2|. Each of |p1| and |p2| is given as a set of matches -- as in \S\ref{sec:basic} but each \C{Pattern} only specifies the values for the non-recursive selectors, all recursive selectors are handled by |p2|.

\begin{revisit}{1} safe evaluation of |(head xs)| requires |xs| to be non-empty. The MP-constraint on |xs| is: |{(:) Any} :* {[], (:) Any}| This constraint is longer than the corresponding RE-constraint because it makes explicit that the \C{hd} field is unrestricted.
\end{revisit}

\begin{revisit}{2} safe evaluation of |(map head xs)| requires |xs| to be a list of non-empty lists. The MP-constraint on |xs| is:

\smallskip
\par\noindent |{[], (:) ({(:) Any} :* {[], (:) Any})} :*|
\par\noindent |{[], (:) ({(:) Any} :* {[], (:) Any})}|
\end{revisit}

\begin{revisit}{3} |(map head (reverse x))| requires |xs| to be a list of non-empty lists \textit{or} infinite. The MP-constraint for an infinite list is: |{(:) Any} :* {(:) Any}|
\end{revisit}

MP-constraints also have simplification rules.  Two of the eight rules are:

\para{|[Val]| simplification:} Given a list of |Val|, if the value |Any| is in this list, the list is equal to |[Any]|. If a value occurs more than once in the list, one copy can be removed.

\para{|Val| simplification:} If both |p1| and |p2| cover all constructors and all their components have |Any| as their constraint, the constraint |p1 :* p2| can be replaced with |Any|.


\subsubsection{Finitely Many MP-Constraints per Type}

As in \S\ref{sec:finite_re}, we show there are finitely many constraints per type by defining a \C{count} function:

\begin{code}
count :: Type -> Integer
count t = 2 ^ val t

val t = 1 + 2 * 2 ^ (pattern t)

pattern t = sum (map f t)
    where f c = product [count t2 | Just t2 <- c]
\end{code}

The \C{val} function counts the number of possible \C{Val} constructions. The \C{pattern} function performs a similar role for \C{Pattern} constructions.


\subsection{MP-Constraint Propositions and Uncurrying}

A big advantage of MP-constraints is that if two constraints on the same expression are combined at the proposition level, they can be reduced into one constraint:

\begin{code}
(e :< v1) || (e :< v2) = e :< (v1 ++ v2)
(e :< v1) && (e :< v2) = e :< [a `mergeVal` b | a <- v1, b <- v2]
\end{code}

\noindent This ability to combine constraints on equal expressions can be exploited further by translating the program to be analysed. After applying \C{backs}, all constraints will be in terms of the arguments to a function. So if all functions took exactly one argument then \textit{all} the constraints associated with a function body could be collapsed into one. We therefore \textit{uncurry} all functions.

\begin{example}
\begin{code}
(user_or) x y = case  x of
                      True   -> True
                      False  -> y
\end{code}

\noindent can be translated into:

\begin{code}
(user_or) a = case  a of
                    (x,y) -> case  x of
                                   True    -> True
                                   False   -> y
\end{code}
\end{example}

Combining MP-constraint reduction rules with the uncurrying transformation makes \C{Assert VarName} equivalent in power to \C{Prop (Assert VarName)}. This simplification reduces the number of different propositional constraints, making fixed-point computations faster. In the RE-constraint system uncurrying would do no harm, but it would be of no use. None of the RE simplification rules is able to reduce distinct components in a tuple.

\subsection{Comparison of RE and MP Constraints}

Both RE-constraints and MP-constraints are capable of expressing a wide range of value-sets, but neither subsumes the other. We give examples where one constraint language can differentiate between a pair of values, and the other cannot.

\begin{example}
Let |v1 = (T:[])| and |v2 = (T:T:[])| and consider the MP-constraint |{(:) Any} :* {[]}|. This constraint is satisfied by |v1| but not by |v2|. No proposition over RE-constraints can separate these two values.
\end{example}

\begin{example}
Consider a data type |Tree alpha|:

\begin{code}
data Tree alpha  =  Branch {left :: Tree alpha, right :: Tree alpha}
                 |  Leaf alpha
\end{code}

\noindent and two values of the type |Tree Bool|

\begin{code}
v1 = Branch (Leaf True   ) (Leaf False  )
v2 = Branch (Leaf False  ) (Leaf True   )
\end{code}

\noindent The RE-constraint |(left^* :- True)| is satisfied by |v1| but not |v2|. No MP-constraint separates the two values.
\end{example}

\begin{discuss}
\newtool{} uses MP-constraints, as they scale to much larger examples.
\end{discuss}

\section{Converting Haskell to a First-order Core}
\label{sec:transform}

The full Haskell language is unwieldy for analysis. As noted in \S\ref{sec:core}, the checker works instead on a simplified language, a core to which other Haskell programs can be reduced.

\subsection{Yhc Core}

\begin{comment}
-- a simple variant of lambda calculus without types, but with source position information. Yhc works by applying basic desugaring transformations, without optimisation. This simplicity ensures the generated PosLambda is close to the original Haskell in its structure. Each top-level function in a source file maps to a top-level function in the generated PosLambda, retaining the same name.

However, PosLambda has constructs that have no direct representation in Haskell. For example, there is a FatBar construct \cite{spj:implementation}, used for compiling pattern matches which require fall through behaviour. The PosLambda language
\end{comment}

To generate core representations of programs, it is natural to start with a full Haskell compiler, and we chose Yhc \citep{Yhc}, a fork of nhc \citep{nhc}. The core language of Yhc, PosLambda, was intended only as an internal representation, and exposes certain details that are specific to the compiler. We have therefore introduced a new Core language to Yhc to which PosLambda can easily be translated. All names are fully qualified. Haskell's type classes have been removed (see \S\ref{sec:dict}). Only top-level functions remain; all local functions have been lambda lifted. All constructor applications are fully saturated. Pattern matching occurs only in case expressions; alternatives match only the top level constructor and are exhaustive, including an \C{error} alternative if necessary.


\subsection{The Dictionary Transformation}
\label{sec:dict}

Most transformations in Yhc operate within a single function definition. The only phases which require information about more than one function are type checking and the transformation, used to implement type classes \citep{wadler:type_classes}. The dictionary transformation introduces tuples (or \textit{dictionaries}) of methods passed as additional arguments to class-polymorphic functions. Haskell also allows subclassing. For example, \C{Ord} requires \C{Eq} for the same type. In such cases the dictionary transformation generates a nested tuple: the \C{Eq} dictionary is a component of the \C{Ord} dictionary.

\begin{example}
\begin{code}
f :: Eq alpha => alpha -> alpha -> Bool
f x y = x == y user_or x /= y
\end{code}

\noindent is translated by Yhc into

\begin{code}
f :: (alpha -> alpha -> Bool, alpha -> alpha -> Bool) -> alpha -> alpha -> Bool
f dict x y = (user_or) (((==) dict) x y) (((/=) dict) x y)

(==) (a,b) = a
(/=) (a,b) = b
\end{code}

The \C{Eq} class is implemented as two selector functions, |(==)| and |(/=)|, acting on a method table. For different types of |alpha|, different method tables are provided.
\end{example}

The dictionary transformation is a global transformation. In Example \lastexample{} the \C{Eq} context in \C{f} not only requires a dictionary to be accepted by \C{f}; it requires all the callers of \C{f} to pass a dictionary as first argument. An alternative approach to implementing type classes, given in \cite{jones:dictionary_free}, does not rely on higher order functions. Although this approach might suit \catch{} better, we re-used the method already implemented in Yhc.

\subsection{First-Order Haskell}

The analysis presented in \S\ref{sec:manipulate} operates on a first-order language. In order to analyse full Haskell, we transform Haskell to a first-order language. We briefly consider three alternative methods.

\subsubsection{Reynold's style defunctionalization}

Reynold's style defunctionalization \citep{reynolds:defunc} is the seminal method for generating a first-order equivalent of a higher-order program.

\begin{example}
\begin{code}
map fn x = case  x of
                 []      -> []
                 (a:as)  -> fn a : map fn as
\end{code}

\noindent Defunctionalization works by creating a data type to represent all values that |fn| may take anywhere in the whole program. For instance, it might be:

\begin{code}
data Functions = Head | Tail

apply Head  x = head  x
apply Tail  x = tail  x

map fn x = case  x of
                 []      -> []
                 (a:as)  -> apply fn a : map fn as
\end{code}

Now all calls to |map head| are replaced by |map Head|.
\end{example}

Defunctionalized code is still type safe, but type checking would require a dependently typed language. This presents no problem for \catch{}, which does not use type information. The unacceptable aspect is the creation of an \C{apply} function, whose meaning is excessively general, introducing a bottleneck through which various properties must be proven. Asking questions such as \textit{``Is the result of \C{apply} an empty-list?''}, requires a lot of computation.

\catch{} only uses Reynold's style defunctionalization if all other methods fail.


\subsubsection{Specialisation}

\oldtool{} used a different technique to remove higher-order functions: specialisation. A mutually recursive group of functions can be specialised in \underline{their $n$th argument} if in all recursive calls this argument is invariant. Examples of common functions whose applications can be specialised include \C{map}, \C{filter}, \C{foldr} and \C{foldl}. When a function can be specialised, the expression passed as the $n$th argument has all its free variables passed as extra arguments, and is expanded in the specialised version. All recursive calls within the new function are then renamed.

\begin{example}
\begin{code}
map fn xs = case  xs of
                  []      -> []
                  (a:as)  -> fn a : map fn as

adds x n = map (add n) x
\end{code}

\noindent is transformed into:

\begin{code}
map_adds n xs = case  xs of
                      []      -> []
                      (a:as)  -> add n a : map_adds n as

adds x n = map_adds n x
\end{code}
\end{example}

Specialisation alone is sufficient for many examples, but it cannot cope with point-free code, and does not deal with many forms of dictionaries.

\subsubsection{Specialisation with Inlining}

The power of specialisation is greatly increased if it is combined with inlining, and applied selectively to higher-order functions.

\begin{figure}
\begin{code}
data Expr  =  ... -- as in Figure {\ref{fig:core}}
           |  Part   Int FuncName [Expr]
           |  Apply  Expr [Expr]

Part 0 fn xs == Call fn xs
Apply (Part n fn xs) ys == Part (n - length ys) fn (xs ++ ys)
\end{code}
\caption{Augmented Core syntax.}
\label{fig:core_ho}
\end{figure}

In order to permit a higher-order program to be represented, the Core language is augmented with additional constructs, as shown in Figure \ref{fig:core_ho}. The \C{Apply} constructor represents an unsaturated function call, or a variable to be used as the function. The \C{Part} constructor is used to represent unsaturated function calls, leaving the normal \C{Call} constructor to represent saturated calls. A \C{Part} construction records how many arguments are needed.

\begin{figure}
\begin{code}
isHO :: Expr -> Bool
isHO (Part n _ _)    = n > 0
isHO (Make _ xs)     = any isHO xs
isHO (Case on alts)  = any (isHO . snd) alts
isHO _               = False
\end{code}
\caption{Tests for the firstifier.}
\label{fig:isHO}
\end{figure}

The algorithm for removing higher-order functions has two components, specialisation and inlining. We apply the specialise rule until a fixed point is reached, then apply the inline rule once. We repeat these two steps until a fixed point is reached. Given an appropriate \C{fix} function, \C{firstify} can be implemented as:

\begin{code}
firstify :: Program -> Program
firstify = fix (inline . fix specialise)
\end{code}

\para{The \C{inline} stage} inlines each \C{Call} for which the body passes the \C{isHO} test, defined in Figure \ref{fig:isHO}. If this process causes a function to no longer be called from the root of the program, then the function is removed after inlining.

\para{The \C{specialise} stage} takes every expression of the form |Call fn xs| where |any isHO xs|, and generates a specialised version of the function |fn| with all functional arguments in |xs| frozen in, and all others passed normally. Assuming that a function |fn| results in the specialised version |fn2|, then the translation would be:

\begin{code}
transform (Call fn xs) | any isHO xs =
    Call fn2 (concatMap f xs)
    where
    fn2 = generate fn xs

    f x = if isHO x then freeVars x else [x]
\end{code}

The combination of specialisation and inlining is powerful. We have encountered few examples where it fails -- mainly artificial tests created specifically to break the approach! Either the entire body is an application of the functional argument (in which case it will be inlined), or it must occur as the argument to a function (in which case it is specialised). There are only two places left for functional arguments to be used: (1) As the subject of a |case| expression. But this situation is impossible as all |case| expressions must choose over a data value. (2) Inside an \C{Apply} with another functional argument variable as the function. This situation is very rare in practice.

\begin{discuss}
In some circumstances a program may have an infinite number of specialisations. In reality, this is rare, but if necessary we revert to Reynold's style defunctionalization.
\end{discuss}

\section{Results and Evaluation}
\label{sec:results}

The best way to see the power of Catch is by example. \S\ref{sec:safety} discusses in general how some programs may need to be modified to obtain provable safety. \S\ref{sec:imaginary} investigates all the examples from the Imaginary section of Nofib, \S\ref{sec:filepath} investigates a library (FiniteMap) and \S\ref{sec:hscolour} investigates a larger real-world program (HsColour).


\subsection{Modifications for Verifiable Safety}
\label{sec:safety}

\begin{discuss}
Take a typical benchmark, Primes. The \C{main} function is:
\end{discuss}

\begin{code}
main = do  [arg] <- getArgs
           print $ primes !! (read arg)
\end{code} % $

The first unsafe pattern here is |[arg] <- getArgs|, as \C{getArgs} is a primitive which may return any value. \catch{} correctly returns False as the precondition to \C{main}!

The next step that may fail is when \C{read} is applied to an argument extracted from getArgs. This argument is entirely unknown, and \C{read} is a sufficiently complicated function that although it can be modelled by Catch, there is no possibility for getting an appropriate abstraction which models the failure case. As a result, any program which calls \C{read} is unsafe, according to \catch{}! Using \C{reads}, which indicates failure properly, a program may still be checked  successfully.

As a result the programs have been rewritten, and \catch{} is directed to check the function:

\begin{code}
compute x = print $ primes !! x
\end{code}

\todo{JUMP}

Take the following example:

\begin{code}
average xs = sum xs / length xs
\end{code}

If |xs| is |[]| then a division by zero occurs, modelled in  In \catch{} as a pattern-match error. One small local change could be made which would remove this pattern match error:

\begin{code}
average xs = if null xs then 0 else sum xs / length xs
\end{code}

Now if |xs| is |[]|, the program simply returns 0, and no pattern match error occurs. While this program is now safe, this safety has come at a cost -- the code no longer behaves as it did before -- replacing $\bot{}$ with the non-sensical value 0.

Pattern-match errors can be avoided in two ways:

\para{Widen the domain of definition:} In the example, we widen the domain of \C{average}, adding a special test guarding the division. This fix is local and fairly simple, and requires only small code changes in the definition.

\para{Narrow the domain of application:} In the example, we have narrowed the domain of application of division. To narrow the domain of \C{average} we would investigate the code that calls \C{average}, ensuring that |[]| is not passed as the first argument. This fix requires a deeper understanding of the flow of the program, and depending on the overall design, may require substantial work at each application. The end result is likely to be a cleaner body of code.

Another alternative to prove the example is to widen the domain of division, using a safe variant:

\begin{code}
safeDiv x y = if y == 0 then 0 else x / y
\end{code}

In the following sections, where modifications were required, we prefer to make the minimum number of changes. Consequently, we have widened the domain of definition.

\subsection{Nofib Benchmark Tests}
\label{sec:imaginary}

\begin{table}
\caption{Table of results}
\label{tab:results}

\smallskip

\textbf{Name} is the name of the checked program (a starred name indicates that changes were needed before safe pattern-matching could be verified);
\textbf{Src} is the number of lines in the original source code;
\textbf{Core} is the total number of lines of Yhc Core, including all functions used from libraries;
\textbf{First} is the number of lines after firstification, just before analysis;
\textbf{Err} is the number of calls to \C{error} (missing pattern cases);
\textbf{Pre} is the number of functions which have a precondition which is not simply `True';
\textbf{Sec} is the time taken for transformations and analysis;
\textbf{Mb} is the maximum residency of the checker at garbage collection time.

\smallskip\smallskip

\begin{tabular*}{\linewidth}{lrrrrrrlll}
\hspace{-2mm} \textbf{Name} & \textbf{Src} & \textbf{Core} & \textbf{First} & \textbf{Err} & \textbf{Pre} & \textbf{Sec} & \textbf{Mb} \\
\vspace{-1ex} \\
\hspace{-2mm} Bernoulli*                   & 35 & 1616 &  652 & 5 & 11 & 4.1 & 0.8 \\
\hspace{-2mm} Digits of E1*  \hspace{-3mm} & 44 &  957 &  377 & 3 &  8 & 0.3 & 0.6 \\
\hspace{-2mm} Digits of E2   \hspace{-3mm} & 54 & 1179 &  455 & 5 & 19 & 0.5 & 0.8 \\
\hspace{-2mm} Exp3-8                       & 29 &  220 &  163 & 0 &  0 & 0.1 & 0.1 \\
\hspace{-2mm} Gen-Regexps*   \hspace{-3mm} & 41 & 1006 &  776 & 1 &  1 & 0.3 & 0.4 \\
\hspace{-2mm} Integrate                    & 39 & 2466 &  364 & 3 &  3 & 0.3 & 1.9 \\
\hspace{-2mm} Paraffins*                   & 91 & 2627 & 1153 & 2 &  2 & 0.8 & 1.9 \\
\hspace{-2mm} Primes                       & 16 &  302 &  241 & 6 & 13 & 0.2 & 0.1 \\
\hspace{-2mm} Queens                       & 16 &  648 &  283 & 0 &  0 & 0.2 & 0.2 \\
\hspace{-2mm} Rfib                         &  9 & 1918 &  100 & 0 &  0 & 0.1 & 1.7 \\
\hspace{-2mm} Tak                          & 12 &  209 &  155 & 0 &  0 & 0.1 & 0.1 \\
\hspace{-2mm} Wheel Sieve 1* \hspace{-3mm} & 37 & 1221 &  570 & 7 & 10 & 7.5 & 0.9 \\
\hspace{-2mm} Wheel Sieve 2* \hspace{-3mm} & 45 & 1397 &  636 & 2 &  2 & 0.3 & 0.6 \\
\hspace{-2mm} X2n1                         & 10 & 2637 &  331 & 2 &  5 & 1.8 & 1.9 \\
\vspace{-1ex} \\
\hspace{-2mm} FiniteMap*    \hspace{-3mm} & 670 & 1484 & 1829 & 13 & 17 & 1.6 & 1.0 \\
\hspace{-2mm} HsColour*     \hspace{-3mm} & 823 & 5379 & 5060 & 4 &  9 & 2.1 & 2.7 \\
\hline
\end{tabular*}
\end{table}

The entire Nofib suite \citep{nofib} is large. We concentrate on the `Imaginary' section. These programs are all under a page of text, excluding any Prelude or library definitions used, and particularly stress list operations and numeric computations.

Results are given in Table \ref{tab:results}. Only four programs contain no calls to \C{error} as all pattern-matches are exhaustive. Four programs use the list-indexing operator |(!!)|, which requires the index to be non-negative and less than the length of the list; \catch{} can only prove this condition if the list is infinite. Eight programs include applications of either \C{head} or \C{tail}, most of which can be proven safe. Seven programs have incomplete patterns, often in a |where| binding and \catch{} performs well on these. Nine programs use division, with the precondition that the divisor must not be zero; most of these can be proven safe.

Three programs have preconditions on the \C{main} function, all of which state that the argument must be a natural number. In all cases the generated precondition is a necessary one -- if the input violates the precondition then pattern-match failure will occur.

We now discuss the four programs which required changes, along with the Digits of E2 program -- a program with complex pattern matching that \catch{} is able to prove safe.

\paragraph{Bernoulli}

This program has one instance of |tail (tail x)|. MP-constraints are unable to express that a list must be of at least length two, so \catch{} conservatively strengthens this to the condition that the list must be infinite -- a condition that Bernoulli does not satisfy. One remedy is to replace |tail (tail x)| with |drop 2 x|. After this change, the program still has several non-exhaustive pattern matches, but all are proven safe.


\paragraph{Digits of E1}

This program contains the following equation:

\begin{code}
ratTrans (a,b,c,d) xs |
  ((signum c == signum d) || (abs c < abs d)) &&
  (c+d)*q <= a+b && (c+d)*q + (c+d) > a+b
     = q:ratTrans (c,d,a-q*c,b-q*d) xs
  where q = b `div` d
\end{code}

\noindent \catch{} is able to prove that the division by |d| is only unsafe if both |c| and |d| are zero, but it is not able to prove that this invariant is maintained. Using \C{safeDiv} from \S\ref{sec:safety} the program is proved safe.

\paragraph{Gen-Regexps}

This program expects valid regular expressions as input. Ways of crashing the program include entering |""|, |"["|, |"<"| and lots of other inputs. One potential error comes from |head . lines|, which can be replaced by |takeWhile (/= '\n')|. Two potential errors take the form |(a,_:b) = span f x|. At first glance this pattern definition is similar to the one in \C{risers}. But here the pattern is only safe if for one of the elements in the list |x|, |f| returns True. The test |f| is actually |(/= '-')|. Using the abstraction for |Char| as detailed in \S\ref{sec:abstraction}, |(/=)| \underline{over |Char| always has an unknown result}, making this pattern impossible to prove safe. With the amendment  |(a,b) = safeSpan f x|, where \C{safeSpan} is defined as:

\begin{code}
safeSpan p x = (a, drop 1 b) where (a,b) = span p x
\end{code}

\noindent \catch{} verifies pattern safety.

\paragraph{Wheel Sieve 1}

This program defines a data type \C{Wheel}, and a function \C{sieve}:

\begin{code}
data Wheel = Wheel Int [Int]

sieve :: [Wheel] -> [Int] -> [Int] -> [Int]
\end{code}

The lists are infinite, and the integers are positive, but the program is too complex for \catch{} to infer these properties in full. To prove safety a variant of \C{mod} is required which does not raise division by zero and a pattern in \C{notDivBy} has to be completed. Even with these two modifications, \catch{} takes 7.5 seconds to check the other non-exhaustive pattern matches.


\paragraph{Wheel Sieve 2}

This program has similar datatypes and invariants, but much greater complexity. \catch{} is able to prove very few of the necessary invariants. Only after widening the domain of definition in three places -- replacing |tail| with |drop 1|, |head| with a version returning a default on the empty list, and |mod| with a safe variant is \catch{} able to prove safety.


\paragraph{Paraffins}

Again the program can only be validated by Catch after modification. There are two reasons: laziness and arrays. Laziness allows the following odd-looking definition:

\begin{code}
radical_generator n = radicals undefined
  where radicals unused = big_memory_computation
\end{code}

If \C{radicals} had a zero arity definition it would be computed once and retained as long as there are references to it. To prevent this behaviour, a dummy argument (\C{undefined}) is passed. If the analysis was more lazy (as discussed in \S\ref{sec:precond}) then this example would succeed using \catch{}. As it is, simply changing \C{undefined} to |()| resolves the problem.

The Paraffins program uses the function |array :: Ix a => (a, a) -> [(a, b)] -> Array a b| which takes a list of index/value pairs and builds an array. The precondition on this function is that all indexes must be in the range specified. This precondition is too complex for Catch, but simply using \C{listArray}, which takes a list of elements one after another, the program can be validated. Use of \C{listArray} actually makes the program shorter and more readable. The array indexing operator |(!)| is also troublesome. The precondition requires that the index is in the bounds given when the array was constructed, something \catch{} does not currently model.


\paragraph{Digits of E2}

This program is quite complex, featuring a number of possible pattern-match errors. To illustrate, consider the following fragment:

\begin{code}
  where  carryguess = d `div` base
         remainder = d `mod` base
         nextcarry:fraction = carryPropagate (base+1) ds
\end{code}

\noindent There are three potential pattern-match errors in as many lines. Two of these are the calls to |div| and |mod|, both requiring |base| to be non-zero. A possibly more subtle pattern match error is the |nextcarry:fraction| left-hand side of the third line. \catch{} is able to prove that none of these pattern matches fails. Now consider:

\begin{code}
e =  ("2."++) $
     tail . concat $
     map (show.head) $
     iterate (carryPropagate 2 . map (10*) . tail) $
     2:[1,1..]
\end{code}

\noindent Two uses of \C{tail} and one of \C{head} occur in quite complex functional pipelines. \catch{} is again able to prove that no pattern-match fails.

\subsection{The FiniteMap library}
\label{sec:filepath}

The FiniteMap library is based on \citep{adams:sets}, and was included in the Haskell standard libraries for many years.The initial challenge to checking a library is that there is no \C{main} function. \catch{} provides the |(||||||)| operator to resolve this issue. A \C{main} function can be constructed as follows:

\begin{code}
main = emptyFM ||| unitFM ||| listToFM ||| ...
\end{code}

\begin{discuss}
The |(||||||)| operator takes two functions, and using Haskell type classes saturates them with enough arguments, then checks that a crash does not result.
\end{discuss}

The library manipulates trees, and has 14 distinct possible pattern match errors. Of these \catch{} is able to prove that 13 are unreachable. The final issue detected stems from the code:

\begin{code}
delFromFM (Branch key ...) del_key  | del_key  >   key = ...
                                    | del_key  <   key = ...
                                    | del_key  ==  key = ....
\end{code}

At first glance this appears to be a complete pattern, the law of trichotomy states that one of these patterns must hold. Unfortunately, the Haskell \C{Ord} class does not have these restrictions. By writing a dubious \C{Ord} class, a crash can be provoked. This issue is not necessarily a coding problem -- authors are allowed to assume that \C{Ord} is indeed an ordering, but perhaps a more defensive approach should have been taken. By moving to the \C{compare} function, the code becomes faster and does not suffer a potential pattern match error.

The paper on which this library is based does not contain such a pattern, leaving the deletion function as homework for the reader.


\subsection{The HsColour Program}
\label{sec:hscolour}

Artificial benchmarks may never have been intended to be fail-proof. But a real program, with real users, should \textit{never} fail with a pattern-match error. We have taken the HsColour program\footnote{http://www.cs.york.ac.uk/fp/darcs/hscolour/} and analysed it using \newtool{}. HsColour comprises of 12 modules, is 4 years old and has had patches from 6 different people.
We have contributed patches back to the author of HsColour, with the result that the development version can be proved free from pattern-match errors.

\catch{} required 4 small patches to successfully check the entire program. Details are given in Table \ref{tab:results}. Of the 4 patches, 3 were genuine pattern-match errors which could be tripped by constructing unexpected input. The issues were: (1) \C{read} was called on a preferences file from the user, this could crash given a malformed preferences file; (2) by giving the document consisting of the a single double quote character \T{"}, and passing the ``-latex'' flag, a crash occurred; (3) by giving the document \T{(`)}, namely open bracket, backtick, close bracket, and passing ``-html -anchor'' a crash occurred. The one patch which did not (as far as we are able to ascertain) fix a real bug could still be considered an improvement, and was minor in nature (a single line).


\section{Related Work}
\label{sec:related}

\begin{discuss}
This paper follows very much the same spirit as \cite{me:catch_tfp} -- the goals are the same and the approach is similar. The comparison to the previous paper has been made throughout , but the two main improvements are support for full Haskell, and scalability.
\end{discuss}

\subsection{Mistake Detectors}

There has been a long history of writing tools to analyse programs to detect potential bugs, going back to the classic C Lint tool \citep{lint}. In the functional arena there is the Dialyzer tool \citep{dialyzer} for Erlang \citep{erlang}. The aim is to have a static checker that works on unmodified code, with no additional annotations. However, a key difference is that in Dialyzer all warnings indicate a genuine problem that needs to be fixed. Because Erlang is a dynamically typed language, a large proportion of Dialyzer's warnings relate to mistakes a type checker would have detected.

\subsection{Proving Incomplete Patterns Safe}

Despite the seriousness of the problem of pattern matching, there are very few tools for checking pattern-match safety. The closest work we are aware of is \citep{esc_haskell}, where ESC/Haskell is introduced. The ESC/Haskell approach requires the programmer to give explicit preconditions and contracts which the program obeys. Contracts have more expressive power than our constraints -- one of the examples involves an invariant on an ordered list, something beyond \catch{}. But the programmer has more work to do. At the time of writing there is no publicly available version of ESC/Haskell. So far as we know, it does not yet support full Haskell, lacks features such as type classes, and handles only small examples. A future version of ESC/Haskell may address these limitations, allowing a fuller comparison to be made.


\subsection{Eliminating Incomplete Patterns}

One way to guarantee that a program does not crash with an incomplete pattern is to ensure that all pattern matching is exhaustive. The GHC compiler \citep{ghc} has an option flag to warn of any incomplete patterns. Unfortunately the Bugs (12.2.1) section of the manual notes that the checks are sometimes wrong, particularly with string patterns or guards, and that this part of the compiler ``needs an overhaul really'' \citep{ghc}. Also the GHC checks are only local.

\begin{discuss}
A more radical approach is to design a \textit{total} functional language without pattern-match errors (or non-termination) \citep{turner:total}. Incomplete pattern matches are banned. But as we have seen in \S\ref{sec:imaginary}, Haskell programs have a reasonably high number of incomplete pattern matches, but may still be safe.

The \catch{} tool could perhaps allow the exhaustive pattern matching restriction to be lifted somewhat. The \catch{} tool will always report success if there are no inexhaustive patterns, so a more generous requirement would be for the \catch{} tool to succeed on the program.
\end{discuss}

\subsection{Type System Safety}

\begin{discuss}
Another way of checking pattern matching is to define a slightly more precise type system. In this light the checker can be compared to the tree automata work done on XML and XSL \citep{static_xslt}, which can be seen as an algebraic data type and a functional language. Another soft typing system with similarities is by \citet{aiken:type_infer}, on the functional language FL. This system tries to assign a type to each function using a set of constructors, for example \C{head} takes the type \C{Cons} and not \C{Nil}.
\end{discuss}

\begin{figure}
\begin{code}
data Cons; data Unknown
newtype List a t = List [a]

nil :: List a Unknown
nil = List []

cons :: a -> [a] -> List a Cons
cons a as = List (a:as)

fromList :: [a] -> List a Unknown
fromList xs = List xs

safeTail :: List a Cons -> a
safeTail (List (a:as)) = as
\end{code}
\caption{A \C{safeTail} function with Phantom types.}
\label{fig:phantom}
\end{figure}

Types can sometimes be used to explicitly encode invariants on data in functional languages. One approach is the use of \textit{phantom types} \citep{fluet:phantom}, for example a safe variant of \C{tail} can be written as in Figure \ref{fig:phantom}. The \C{List} type is not exported, ensuring that all lists with a \C{Cons} tag are indeed non-empty. The values \C{Cons} and \C{Unknown} are phantom types -- they exist only at the type level, not at the value level.

\begin{figure}
\begin{code}
data ConsT a; data NilT

data List a t where
    Cons  :: a -> List a b -> List a (ConsT b)
    Nil   :: List a NilT

safeTail :: List a (ConsT t) -> List a t
safeTail (Cons a b) = b

fromList :: [a] -> (forall t. List a t -> r) -> r
fromList []      fn = fn Nil
fromList (x:xs)  fn = fromList xs (\sl -> fn (Cons x sl))
\end{code}
\caption{A \C{safeTail} function using GADT's.}
\label{fig:gadt}
\end{figure}

Using GADT's \citep{spj:gadt}, an encoding of lists can be written as in Figure \ref{fig:gadt}. Notice that \C{fromList} requires a locally quantified type. The type-directed approach can be pushed much further with \textit{dependent types}, which allow types to depend on values. There has been much work on dependent types, using undecidable type systems \citep{epigram}, using extensible kinds \citep{omega} and using type systems restricted to a decidable fragment \citep{xi:dependent_practical}. The downside to all these type systems is that they require the programmer to make explicit annotations, and require the user to learn new techniques for computation.


\section{Conclusions and Future Work}
\label{sec:conclusion}

\begin{discuss}
A static checker for potential pattern-match errors in Haskell has been specified and implemented. This checker is capable of determining preconditions under which a program with non-exhaustive patterns executes without failing due to a pattern-match error. A section of the Nofib suite has been tested, with encouraging results -- all but one program can be proved safe, many without modification. Two larger real-world examples have been checked: new bugs were discovered and fixed in one of them, which is now verified to be pattern safe.

There are two main avenues of future work. (1) \textit{Extend the power of the checker.} Possible ways to increase power include the addition of explicit annotations of properties, or a more powerful constraint language, with special purpose constraints (e.g. a linear constraint system \todo{Colin wants citation}). If the programmer was able to specify more restrictive preconditions, or properties, or invariants, then these may be able checked with \catch{}. If the power of the constraint system was increased then richer program annotations could be used.

(2) \textit{Use \catch{} to solve other problems.} Absence of pattern-match failure could be used to inform an optimising compiler. The properties generated could also be used to generate more efficient code.
\end{discuss}

% \appendix
% \section{Appendix Title}
%
% Here is the text of the appendix, if you need one.

\acks

The first author is an EPSRC-supported PhD student.

\bibliographystyle{plainnat}

\small
\bibliography{catch}



\end{document}
