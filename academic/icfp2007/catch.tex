\documentclass[preprint]{sigplanconf}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{alltt}
\usepackage{url}
\usepackage{natbib}
\usepackage{datetime}
\usepackage{comment}

%include polycode.fmt
%include catch.fmt

% general stuff
\newcommand{\T}[1]{\texttt{#1}}
\newcommand{\C}[1]{\textsf{#1}}
\newcommand{\tup}[1]{\ensuremath{\langle #1 \rangle}}
\newcommand{\mtxt}[1]{\textsf{#1}}

% examples
\newcounter{exmp}
\setcounter{exmp}{1}
\newcommand{\yesexample}{\subsubsection*{Example \arabic{exmp}}\addtocounter{exmp}{1}}
\newcommand{\noexample}{\hfill$\Box$}

\newcommand{\todo}[1]{\textbf{\textsc{Todo:} #1}}

% code blocks
\newenvironment{code}{\begin{alltt}\small}{\end{alltt}}
\newenvironment{codepage}
    {\begin{minipage}[h]{\textwidth}\begin{code}}
    {\end{code}\end{minipage}}

\newenvironment{example}{\yesexample}{\noexample}

\newcommand{\K}{\ensuremath{^\ast}} % kleene star
\newcommand{\D}{\ensuremath{\cdot}} % central dot

\renewcommand{\c}[3]{\tup{\T{#1},\T{#2},\T{\{#3\}}}}
\newcommand{\cc}[2]{\c{#1}{$\lambda$}{#2}}

\newcommand{\s}[1]{\ensuremath{_{\tt #1}}} % subscript, in tt font
\newcommand{\g}[1]{\{#1\}} % group, put { } round it
\newcommand{\U}{\textunderscore}
\newcommand{\vecto}[1]{\overrightarrow{#1\;}}
\newcommand{\gap}{\;\;}
\newcommand{\dom}{\text{dom}}


\begin{document}

\conferenceinfo{ICFP '07}{date, City.} %
\copyrightyear{2007} %
\copyrightdata{[to be supplied]}

\titlebanner{\today{} - \currenttime{}}        % These are ignored unless
\preprintfooter{}   % 'preprint' option specified.

\title{Safe Pattern Matching}
\subtitle{A Static Checker for Haskell}

\authorinfo{Neil Mitchell}
           {York}
           {ndm}
\authorinfo{Colin Runciman}
           {York}
           {colin}

\maketitle

\begin{abstract}
A Haskell program may fail at runtime with a pattern-match error if the program has any incomplete (non-exhaustive) patterns in definitions or case alternatives. This paper describes a static checker that allows non-exhaustive patterns to exist, yet ensures that a pattern-match error does not occur. It describes a constraint language that can be used to reason about pattern matches, along with mechanisms to propagate these constraints between program components.

The checker obtains minimal preconditions for 11 out of 14 programs in the Imaginary section of the Nofib suite\citep{nofib}. With minor modifications the remaining 3 are proven safe.
\end{abstract}

% \category{CR-number}{subcategory}{third-level}

% \terms
% term1, term2

% \keywords
% keyword1, keyword2

\section{Introduction}
\label{sec:introduction}

Often it is useful to define pattern matches which are incomplete, for example \C{head} fails on the empty list. Unfortunately programs with incomplete pattern matches may fail at runtime.

Consider the following example:

\begin{code}
risers :: Ord alpha => [alpha] -> [[alpha]]
risers [] = []
risers [x] = [[x]]
risers (x:y:etc) = if x <= y then (x:s):ss else [x]:(s:ss)
    where (s:ss) = risers (y:etc)
\end{code}

A sample execution of this function would be:

\begin{code}
> risers [1,2,3,1,2]
[[1,2,3],[1,2]]
\end{code}

In the last line of the definition, |(s:ss)| is matched against the output of \C{risers}. If |risers (y:etc)| returns an empty list a pattern match error will be raised. It takes a few moments to check this program manually -- and a few more to be sure one has not made a mistake!

Turning the \C{risers} function over to the checker developed in this paper, the output is:

\begin{code}
> Incomplete pattern on line 5, proven safe
> Whole program is Safe
\end{code}

In addition the checker produces a set of properties it has proved about Risers, along with an outline of the proof. Fuller details of how the checking is performed follow in \S\ref{sec:walkthrough}.

\subsection{A Real World Problem}

The problem of pattern match failures in Haskell is a serious one. The darcs \citep{darcs} project is one of the most successful large scale programs written in Haskell for non-Haskell developers. Taking a look at their bug tracker, 13 problems are errors related to \C{fromJust} and 19 are direct pattern match failures.

\subsection{Contributions}

The contributions of this paper include:

\begin{itemize}
\item Two constraint languages to reason about pattern match failures
\item A mechanism for generating and solving appropriate constraints
\item Details of an implementation which handles the complete Haskell 98 language
\item Results, showing success on a number of unmodified examples
\end{itemize}

In \citet{me:catch_tfp} a pattern match checker was described with similar aims. All the issues list in the future work section have been addressed in this paper. In particular the checker is robust enough to handle real Haskell programs, with no restriction on the recursion patterns, higher order function use or type classes. The underlying mechanism of the first pattern match checker bears only passing resemblance to the mechanism described in this paper.

\subsection{Road map}

All data structures and equations are presented in Haskell \citep{haskell}, although should be accessible to all readers.

\S\ref{sec:walkthrough} gives an overview of the checking process for the \C{risers} function. \S\ref{sec:manipulate} introduces a small core functional language and a mechanism for reasoning about this language, \S\ref{sec:constraint} covers a constraint language. \S\ref{sec:transform} discusses how to transform Haskell to this core language.

\S\ref{sec:results} evaluates Catch on various sample programs -- including the Nofib benchmark suite. \S\ref{sec:related} compares our work to other work before \S\ref{sec:conclusion} presents concluding remarks.

\section{Walkthrough of Risers}
\label{sec:walkthrough}

This section details the process of checking that the \C{risers} function in the Introduction does not crash with a pattern match error.

\subsection{Modifying Risers for analysis}

\todo{Tidy up this section}

There is a restriction on the starting functions that Catch can analyse -- they must not have any argument of functional type. The reason for this restriction is to reduce the complexity in the analysis engine.

In Haskell,type classes are typically implemented as dictionaries \citep{wadler:type_classes}. All functions with qualified polymorphic types have one or more dictionaries of class methods as auxiliary arguments. The result is that the function to analyse must not have any outstanding context.

\todo{Jump's around a bit here}

The type of the \C{risers} function is |risers :: Ord alpha => [alpha] -> [[alpha]]| -- which can easily be made first-order by making a new function \C{main} the root of the analysis, and providing a concrete type:

\begin{code}
main :: [Int] -> [[Int]]
main x = risers x
\end{code}

\subsection{Conversion to a Core Language}

Rather than analyse full Haskell, the Catch tool analyses a first-order Core language, without lambda expressions, partial application or let bindings. A convertor is provided from the full Haskell 98 language to this restricted language. Full details are provided in section \S\ref{sec:transform}.

After converting the \C{risers} program to Core Haskell, and renaming the identifiers for ease of human reading, the resulting Core is as shown in Figure \ref{fig:risers_core}. The case expressions in the functions \C{risers4} and \C{risers2} correspond to the pattern match in the |where|. The function |<=| in this example has been specialised to the Int data type, unlike the standard |(<=)| operator which is a member of the \C{Num} class.

\begin{figure}
\begin{code}
risers3 x y = risers4 (risers (x : y))

risers4 x = case x of
    (y:ys) -> (ys, y)
    [] -> error "Pattern Match Failure, 11:12."

risers x = case x of
    [] -> []
    (y:ys) ->  case ys of
         [] -> (y : []) : []
         (z:zs) -> risers2 (risers3 z zs) (y <= z) y

risers2 x y z =  case y of
    True -> (z : snd x) : (fst x)
    False -> (z : []) : (snd x : fst x)
\end{code}
\caption{\C{risers} in the Core language.}
\label{fig:risers_core}
\end{figure}

\subsection{Analysis}

In the Core language every pattern match covers all possible constructors of the appropriate type. The alternatives for constructor cases not originally given are calls to \C{error}. The analysis first starts by finding calls to \C{error}, then tries to prove that these calls will not be reached. In the \C{risers} example there is only one \C{error} call, corresponding to the |where| pattern match.

Only one precondition is required for this example:

\begin{code}
forall risers4 `o` x :< (:)
\end{code}

This condition states that all callers of \C{risers4} must supply an argument which is a |(:)| constructed value. To construct a proof, two properties are required::

\begin{code}
x :< (:)  => (risers x       ) :< (:)
True      => (risers2 x y z  ) :< (:)
\end{code}

The first property says that provided the input to \C{risers} is a |(:)| constructed value, the output will be. The second states that the output from \C{risers2} is always a |(:)|. Given the precondition, and the properties, a proof \C{risers} does not reach \C{error} can be constructed.

\todo{Cite the specific bits from the analysis}

\section{Pattern Match Analysis}
\label{sec:manipulate}

This section explains the underlying constraint system in Catch, focusing on how the constraints are put together to express properties of expressions and the data structures they evaluate to.

\subsection{Reduced expression language}
\label{sec:core}

\begin{figure}
\begin{code}
type CtorName  =  String
type FuncName  =  String
type VarName   =  String
type Field     =  (CtorName, Int)

data Expr  =  Var   VarName
           |  Make  CtorName  [Expr]
           |  Call  FuncName  [Expr]
           |  Case  Expr      [Alt]

data Alt = Alt CtorName [VarName] Expr
\end{code}
\caption{Core Data Type.}
\label{fig:core}
\end{figure}

\begin{figure}
\begin{code}
ctors     :: CtorName  -> [CtorName]
arity     :: CtorName  -> Int
var       :: VarName   -> Maybe (Expr, Field)
isRec     :: Field     -> Bool
\end{code}
\caption{Miscellaneous functions to manipulate Core.}
\end{figure}

Syntax for the reduced expression language is given in figure \ref{fig:core}. The Core language chosen for this purpose is fairly standard \citep{ghc_core}, but with additional restrictions. The language is first order, has only simple case statements, and only algebraic data types. The evaluation strategy is lazy. All case statements are syntactically complete, with error being introduced where a pattern match error would occur. An \C{Alt} data type has the constructor to match, a list of variables to bind to and an expression to execute.

Every constructor has an arity, which can be obtained with the \C{arity} function. To determine all constructors in a set the \C{ctors} function can be used, for example |ctors True = {False, True}| and |ctors [] = {[], :}|. The \C{var} function returns \C{Nothing} for variables which are arguments, and \C{Just} with the expression on the \C{Case} expression and the field.

\subsubsection{\C{isRec} function}

\todo{Expand this section}

Detail the purpose of the \C{isRec} function, what it does, and why. Also mention that co-recursive types are disallowed.

Invariant, given a finite typed structure, there must be no chain of fields which are not \C{isRec} one after another.

\subsubsection{Abstraction}

\begin{figure}
\begin{code}
data Int = Neg | Zero | Pos

x     + Zero  = x
Zero  + x     = x
x     + y     | x == y = x
_     + _     = any0

x     - Zero  = x
Zero  - Zero  = Zero
Zero  - Neg   = Pos
Zero  - Pos   = Neg
Neg   - Pos   = Neg
Pos   - Neg   = Pos
_     - _     = any0
\end{code}
\caption{Abstract implementation of integers}
\label{fig:abstract_int}
\end{figure}

The Core language only has algebraic data types, specifically the types missing from a general purpose functional programming language are text characters, integers (bounded and unbounded) and floating point numbers. Catch allows for these programs by abstracting them into algebraic data constructors. Different programs may require different abstractions for the various pattern matches.

Natural numbers are often encoded by Peano numerals, and this idea can easily be extended to integers:

\begin{code}
data Nat  = Zero | Succ Nat
data Int  = Minus Nat | Zero | Plus Nat
\end{code}

This abstraction captures all the underlying detail of the number system, but it is not detail that can be used during the analysis -- due to the underlying constraint systems discussed in \S\ref{sec:constraints}. A more practically motivated example is given in Figure \ref{fig:abstract_int}, this abstraction is used in practice. The \C{any0} method is an internally implemented method of Catch, assumed to be any demonic value, of the correct type.

\todo{Add notes on One as an abstract value}

The abstraction of characters is often more practically interesting. The Haskell language standard calls for Unicode character literals -- and depending on the compiler the number of distinct characters varies between 256 and 1114111. There are several possible abstractions of characters:

\begin{code}
data Char = Char
data Char = Alpha | Digit | White | Other
data Char = 'A' .. 'Z' | 'a' .. 'z' | '0' .. '9' | Other
data Char = '\0' | '\1' | '\2' ..
\end{code}

The simplest abstraction is that all characters are the same. A slightly more refined abstraction is to partition the characters based on the character testing functions provided in the \C{Char} module of Haskell. Refining the model further can give special status to characters that commonly occur, giving all uncommon characters the \C{Other} value. Finally, given that \C{Char} is a finite enumeration, the entire range of characters could each be represented distinctly.

In practice, programs often require different levels of abstraction for characters. Many will accept the most basic abstraction, others require certain literals to be represented precisely - for example a noughts and crosses\footnote{Tic-tac-toe, for US readers.} program matches on 'X' and 'O'.

The final issue of abstraction relates to primitive functions, for example the \C{getArgs} function which returns the command line arguments, or the \C{readFile} which reads from the underlying filesystem. In most cases the most appropriate model is to return \C{any0}.


\subsection{Constraints}

An expression evaluates to a (potentially infinite) data structure, or to $\bot{}$ caused by either non-termination of pattern match error. If an expression does evaluate to a data structure, then a constraint states the possible forms of data value it may take. A constraint is a set of data structures that a value may match. In order to practically represent a constraint, there are several different concrete representations -- these will be discussed in \S\ref{sec:constraints}.

Given a mechanism of expressing a set of constructors, $E(x) \in C$ states that the expression $x$ must, when evaluated be a member of the set defined by $C$. Since the Core language is typed, the constraint $C$ will only refer to a set of well typed terms. These atomic constraints can be built up into a proposition of constraints, each possibly on different variables.

Since free variables are bound by function calls, it is possible to scope a constraint by making it a proposition. These can then be placed into a proposition. For example, $\forall f, x \in C$ states that for the function $f$, the argument $x$ must be in the set $C$. To take a concrete example, one particular example is $\forall \C{risers4}, x \in \g{\_ : \_}$.

These constraints are sufficient to express many useful properties. Several underlying models are possible, and will be discussed later.

\subsection{Constraints - Pattern Matching}

\begin{figure}
\begin{code}
type Constraint = [Match]

data Match  =  Match CtorName [Match]
            |  Any

data Req = Expr :< Constraint

notin :: CtorName -> Constraint
(|>) :: Field -> Constraint -> Constraint
(<|) :: Expr -> Constraint -> Prop Req
\end{code}
\caption{Constraint language}
\label{fig:constraint}
\end{figure}

The simplest constraint language can be seen as that which corresponds to Haskell pattern matching. A constraint is a set of pattern matches, following the data structure given in figure \ref{fig:constraint}. A given data structure can be said to be a member of this constraint if there exists a match which would allow the data structure.

This constraint language is limited in expressivity -- for example it is impossible to state in a finite space that all the elements of a list are the literal True. However, even with this limited constraint language the Risers example can be proven safe.

The analysis framework will be introduced using these constraints as a suitable example -- but in  reality these constraints are not used. A \C{Req} is an expression and a constraint pair, stating that the expression must evaluate to something in the constraint. There are 3 operations which must be provided on every constraint implementation, whose signatures are given in figure \ref{fig:constraint}. |(||>)| and |(<||)| are discussed in \S\ref{sec:backward}. The \C{notin} function generates a constructor which does \textit{not} match the constructor given. For example, an implementation for the simple constraint language would be:

\begin{code}
notin c = map f (delete c (ctors c))
   where f x = Match x (replicate (arity c) Any)
\end{code}

An assumption made about the constraint system is that for any given finite type, there exist only a finite number of constraints. This constraint system violates property -- consider a recursive data structure such as a list, an infinite number of type-correct patterns are possible. This problem could be fixed by limiting the depth, although is not done to allow the principles to be expressed more clearly.

\subsection{Safety Preconditions}
\label{sec:precond}

\begin{figure}
\begin{code}
pre :: Expr -> Prop Req
pre (Var   x         ) = True
pre (Make  _   xs    ) = and (map pre xs)
pre (Call  fn  xs    ) = preFunc fn xs && and (map pre xs)
pre (Case  on  alts  ) = pre on && and [f c e | Alt c vs e <- alts]
    where f c e = (on :< notin c) || pre e

preFunc :: FuncName -> [Expr] -> Prop Req
\end{code}
\caption{Precondition of an expression, \C{pre}}
\label{fig:precondition}
\end{figure}

There exists a precondition for every function such that if this precondition holds, then all function calls made will have their preconditions respected. The precondition on the \C{error} function is False. The process of analysis can be seen as deriving these preconditions.

A program is safe if the precondition on \C{main} is True. Given a function that returns the precondition on a functions arguments \C{preFunc}, a function to determine the precondition on an expression can be specified as \C{pre} in figure \ref{fig:precondition}. A fixed point is found iff:

\begin{code}
forall fn, xs `o` preFunc fn xs => pre (instantiate fn xs)
\end{code}

This property states that the preconditions assigned to each function by \C{preFunc} must imply the preconditions calculated on the instantiation of a function with its arguments. One such fixed point is that all calls to \C{preFunc} return False. A perfect precondition would be such that |preFunc f xs == pre (instantiate f xs)|.

The way preconditions are found for functions is that initially all functions are considered to have the precondition True, apart from error, which has the precondition False. For each precondition that has changed (initially error) all those functions which call this function have their bodies checked, assuming their current precondition. If the precondition has changed, then the new precondition is anded with the current one, and marked as changed. Once all preconditions are stable, a fixed point has been found.

The reason for anding with the previous value is to ensure termination -- the and ensures that the value only ever becomes more restrictive, and therefore given a constraint model with a finite number of terms, is guaranteed to terminate.

A fixed point can be obtained more quickly by first propagating the functions at the leaf of the call tree. Once these have found a fixed point, it is only possible for the functions they call to require further computation with this expression. This technique is implemented by constructing a call graph, and taking a flattening to get an effective order for propagation, and can substantially reduce the amount of work required.

The given computation of preconditions does not respect laziness, in particular the \C{Call} equation demands the preconditions to hold on all its arguments -- only correct if a function is strict on its arguments. For example the precondition on |False && error "here"| is False, when it should be True. This definition is more restrictive, so is still sound. The analysis relies on laziness for transformations.

To introduce laziness an evaluation method could be added, which would require an additional fixed point computation, and complicate both the analysis and the generated constraints. In practice simply inlining |(&&)| and |(||||)| achieves most of the benefits that a laziness analysis would provide.


\subsection{Manipulating constraints}
\label{sec:backward}

Constraints on arguments to a function can be considered as preconditions to that function. All other constraints can be reduced using the process denoted by a function \C{back}, which takes a constraint and returns a predicate over constraints. This function is detailed in Figure~\ref{fig:backward}. The \C{backs} function repeatedly applies these rules until the only remaining constraints will be either on arguments to the function.

\begin{figure}
\begin{code}
back :: Req -> Prop Req
back (Var   x         :< k) = on :< (c |> k)
    where Just (on, c) = var x
back (Make  c   xs    :< k) = Make c xs <| k
back (Call  fn  xs    :< k) = template k fn xs
back (Case  on  alts  :< k) = and [f c e | Alt c vs e <- alts]
    where f c e = (on :< notin c) || back (e :< k)

backs :: Prop Req -> Prop Req
\end{code}
\caption{Specification of backward analysis, \C{back}}
\label{fig:backward}
\end{figure}

\begin{description}

\item[The \C{Var} rule] applies only to variables which are bound by case expressions. This rule moves the condition from the bound variable to the subject of the |case| expression which introduced it. The |(||>)| operation can be seen as extending a constraint from being on one small part of the data structure, to being on the entire data structure. The implementation of this operation on the simple constraint type is now given:

\begin{code}
(c,i) |> k = map extend k
    where
    extend x = Match c (anys i ++ [x] ++ anys (n-i))
    anys j = replicate j Any
    n = arity c
\end{code}

\item[The \C{Make} rule] deals with an application of a constructor. The |(<||)| operator changes to the constraint from one on the entire structure, to one on each of the parts of the constructor.

\begin{code}
(Make c xs) <| k = or (map f k)
    where
    f Any = Any
    f (Match c2 xs2) = c2 == c &&
        and (zipWith (:<) xs (map (:[]) xs2))
\end{code}

\item[The \C{Call} rule] relies on properties being derived on the function being called, and applied here. This process is explained in \S\ref{sec:template}, using a process called templating.

\item[The \C{Case} rule] generates a conjunct for each alternative. The generated condition says either the subject of the case analysis has a different constructor (so this particular alternative is not executed in this circumstance), or the right hand side of the alternative is safe given the conditions for this expression.
\end{description}


\subsection{Templating}
\label{sec:template}

When a constraint of the form |Call fn xs :< k| is found, this constraint needs to be reduced to a proposition of constraints on the expressions in |xs|. There are two strategies that can be used. One method is to use the $\beta$ substitution rule (\C{instantiate}) from $\lambda$-calculus, and replace the call with the body of |fn|. Another alternative is to create fresh variables for each element in |xs|, solve this new problem, and then instantiate the results.

An earlier version of the Catch tool chose the former method, but the current version chooses the latter. The later is more amenable to finding a fixed point, and allows for a cache of the results to be built from which future questions can be answered. This cache of results can be seen as a set of axioms, and indeed this cache is saved to a file for future examination after Catch has finished.

The primary template mechanism, and its essential condition are given as:

\begin{code}
template :: Constraint -> FuncName -> [Expr] -> Prop Req

forall fn, xs `o` p xs => backs (instantiate fn xs :< k)
    where p = template k fn
\end{code}

The condition can be read as given a function call and a constraint, the resulting condition on the arguments must ensure that the condition holds on the instantiated body.

The mechanism used to calculate a fixed point is to execute the template function for one iteration, on the body of the function instantiated with free variables for each argument. After the result is obtained, it is anded with the existing value. If any other template is invoked within the template call then this call must be considered at the same time as the current template. Each template is checked in turn until all templates do not change.

Given that there are a finite number of constraints, there are only a finite number of constraint/function pairs, which guarantees that this function terminates.

\section{Real Constraints}
\label{sec:constraint}

There are various interpretations of constraints, here we outline two that have been used in various versions of Catch. Neither is strictly more powerful than the other, both are capable of expressing constraints that the other system cannot.

\subsection{Semantics of Constraints}

Before discussing various types of constraint, it is useful to have a way of expressing which values a constraint accepts. It turns out that the |(||>)| operator introduced earlier already provides all the necessary information:

\begin{code}
data Value = Value CtorName [Value]

accept :: Value -> Constraint -> Bool
accept v k = backs (f v :< k)
    where f (Value c xs) = Make c (map f xs)
\end{code}

If we assume that the |(||>)| operator reduces a constraint to the arguments to the \C{Make}, then this operator results in only constraints on \C{Make} being generated -- since all children are guaranteed to be \C{Make}. This observation allows a simple encoding of the semantics of a constraint system.

\subsection{Regular Expression Based}

\begin{figure}
\begin{code}
data Constraint = RegExp :! [CtorName]

type RegExp = [Atom]

data Atom  =  Atom  Field
           |  Star  [Field]

notin :: CtorName -> Constraint
notin c = [] :- delete c (ctors c)

(|>) :: PathName -> Constraint -> Constraint
p |> (r :- c) = integrate p r :- c

(<|) :: Expr -> Constraint -> Prop Req
(Make c xs) <| (r :- cs) = (ewp r => c `elem` cs) &&
    and (zipWith f (paths c) xs)
    where
    f p x = case  differentiate p r of
                  Nothing -> True
                  Just r2 -> x :< (r2 :- cs)

ewp :: RegExp -> Bool
ewp x = all isStar x
   where  isStar (Star  _) = True
          isStar (Atom  _) = False

integrate :: PathName -> RegExp -> RegExp
integrate p r | not (isRec p) = Atom p : r
integrate p (Star ps:r) = Star (nub (p:ps) : r)
integrate p r = Star [p] : r

differentiate :: PathName -> Path -> Maybe Path
differentiate p [] = Nothing
differentiate p (Atom  r:rs)  | p == r     = Just rs
                              | otherwise  = Nothing
differentiate p (Star  r:rs)  | p `elem` r  = Just (Star r:rs)
                              | otherwise   = differentiate p rs
\end{code}
\caption{Regular expression based constraints}
\label{fig:regexp}
\end{figure}

The original Catch tool used regular expression based constraints. A data type for the constraint, along with the essential operations upon it is given in Figure \ref{fig:regexp}. Given a constraint |(r :- c)|, |r| is referred to as the regular expression, and |c| is referred to as the set of constructors. Such a constraint matches those data values such that any well-defined application of a path of selectors described by |r| must reach a constructor in the set |c|.

The meaning of a constraint is defined by:

\begin{code}
e :< (r :- c) <=> (forall l `elem` lang r `o` defined e l => ctor e l `elem` c)
\end{code}

Here |lang r| is the language represented by the regular expression |r|; \C{defined} returns true if a path selection is well-defined; and \C{ctor} gives the constructor used to create the data, after following the path. Since |lang r| is potentially infinite, this property cannot be checked by enumeration.

If no path selection is well-defined then the constraint is vacuously true.

A regular expression is defined as:\\ \\
\begin{tabular}{ll}
|s+t| & union of regular expressions |s| and |t| \\
|s^.t| & concatenation of regular expressions |s| then |t| \\
|s^*|  & any number (possibly zero) occurrences of |s| \\
\C{x} & a path, such as \C{hd} or \C{tl} \\
0 & the language is the empty set \\
1 & the language is the set containing the empty string
\end{tabular} \\

The differentiation operation is that as defined by \citet{conway:regexp}, and called quotient in some text books. The empty word property (\C{ewp}) is equivalent to |lang 1 `subseteq` lang r|, and can be calculated simply from a regular expression. Integration is merely the inverse of differentiation -- not usually treated as a separate operation in most regular expression systems.

In the original version of Catch regular expressions were based on the full regular expression language. However this implementation is limited to concatenation of atoms, or stars of unions. Formally the context free grammar is:

\[\begin{array}{lllll}
r & = & 1     & || & |a^.r| \\
a & = & \C{x} & || & |u^*| \\
u & = & \C{x} & || & \C{x} + u
\end{array}\]

There are additional restrictions on regular expressions: all paths which are recursive must be under \C{Star} and all which are not must be \C{Atom}; two \C{Star}'s are not allowed to be concatenated to each other.

This restricted regular expression language, combined with 0, is closed under integration and differentiation. The 0 alternative is catered for by the \C{Maybe} return type in the differentiation. The constraint |0 :- c| always evaluates to True, so in |(<||)|, \C{Nothing}\ is replaced by True.

The constructors, because of static typing and the restricted form of regular expression, must all be of the same type.  Given a finite type, these restrictions are enough to ensure that there are only a finite number of regular expressions. Combined with the finite number of constructors, this property is enough to guarantee finiteness, which satisfies the termination properties assumed on constraints.

\todo{More examples here?}

Two examples of constrains are:

\begin{code}
e :< (1 :- {:})
\end{code}

This constraint says that the expression must be a cons.

\begin{code}
e :< (tl ^* ^. hd :- {:})
\end{code}

This constraint says that a list must consist of |(:)| constructed elements. If the list is empty, it still satisfies the constraint. If the list is infinite then the condition applies to all elements, constraining an infinite number. This constraint was not possible with the simple constraint framework introduced earlier.

\subsection{Propositions on Regular Expression Constraints}

The underlying type that is manipulated by Catch is not a constraint, but a proposition of constraints. These propositions are compared for equality to obtain a fixed point, and are variously manipulated by the system. Since the complexity of performing an operation is often proportional to the number of constraints in the proposition, having simplification rules can lead to a big win.

From the definition of the constraints it is possible to construct a number of identities which can be used for simplification.

\begin{description}
\item[Exhaustive conditions:] in the constraint |e :< (r :- [":","[]"])| the condition lists all the possible constructors, if |e| reaches weak head normal form then because of static typing |e| must be one of these constructors, therefore this constraint simplifies to True.

\item[Constraint merging:] given |e :< (r :- c1) `and` e :< (r :- c2)|, this constraint can be replaced by |e :< (r :- (c1 `union` c2))|.

\item[More] \todo{more constraints are in the source code, move a few of the better ones to this document}.
\end{description}

In addition to having useful simplification rules, by having a more accurate equality test, a fixed point can be reached earlier. A standard algebraic proposition is useful, but testing for the equality of two propositions is a challenge. If you have an ordering over constraints, then you can use a Binary Decision Diagram (BDD) which guarantees a unique normal form for each proposition. The other advantage of BDD's is that (assuming every term is independent) there will be no terms remaining which do not directly effect the truth of a proposition. In this constraint system terms are not independent, but a BDD would still achieve a useful reduction.

The Catch system operates primarily on algebraic propositions, using the simplification rules where possible. Factorisation and expansion of propositions is not performed -- they can lead to additional simplification but can also potentially increase the proposition size. Before equality is performed, the proposition is first turned into a BDD and normalised -- which helps to obtain a fixed point quicker.

\subsection{Enumeration Based}


\begin{figure}
\begin{code}
type Constraint = [Val]

data Val   =  Part :* Part
           |  Any
data Part  =  Part [(CtorName, [Val])]

-- useful auxiliary, the non-recursive paths
fields :: [CtorName] -> [PathName]
fields = filter (not . isRec) . concatMap paths

notin :: CtorName -> Constraint
notin c = map f (delete c cs) :* map f cs
    where  cs = ctors c
           f c = (c, replicate (arity c) Any)

-- TODO, update these to use the new data structure
(|>) :: Field -> Constraint -> Constraint
p |> vs = notin c ++ map f vs
    where
    c = pathCtor p; cs = ctors c; xs = fields cs
    as = zip xs (repeat True)

    f Any = Any
    f (v1 :* v2) | isRec p = ([c] :| as) :* (v1 `mergePart` v2)
    f v = ([c] :| fs) :* (cs :| as)
        where fs = [(x, if x == p then v else Any) |  x <- xs]

(<|) :: Expr -> Constraint -> Prop Req
(Make c xs) <| vs = or (map f vs)
    where
    f Any = Any
    f ((c1 :* v1) :| r) = c `elem` c1 && and (zipWith g (paths c) xs)
        where g p x = if isRec p then r :| r else lookup p v1

(`mergeVal`) :: Val -> Val -> Val
(a1 :* b1) `mergeVal` (a2 :* b2) = (mergePart a1 a2) :* (mergePart b1 b2)

mergePart :: Part -> Part -> Part
mergePart (c1 :| v1) (c2 :| v2) = (c1 `intersect` c2) :| zipWith f v1 v2
    where f (p1,v1) (p2,v2) = assert (p1 == p2) (v1 `mergeVal` v2)
\end{code}
\caption{Enumeration Based Constraints}
\label{fig:enumeration}
\end{figure}

The enumeration based constraints are implemented as in figure \ref{fig:enumeration}. These constraints are similar to those given as the basic example model of constraints, but can constrain an infinite number of items, and can be represented in a finite space.

Each constraint has two parts, the first part (before the |:*|) represents the constructor and the non-recursive fields. The second part represents all recursive fields. Given a constraint |((c1 :* v1) :|| (c2 :* v2))|, each |c| is a set of constructors, and each |v| is a mapping from non-recursive field names to values. In this example, |c1| is the set of constructors allowed in the value, and |c2| is the set allowed after following \textit{any number} of recursive fields. Similarly, |v1| represents the fields allowed following non-recursive field names from the root and |v2| lists the field names from the recursive values.

To take the examples from the regular expression based constraints:

\begin{code}
cons = ([":"] :| [("hd",Any)]) :* (["[]",":"] :| [("hd",Any)])
\end{code}

This constraint states that the first constructor is a |(:)|. Note that it is a lot more verbose than the regular expression one. This constraint is much more explicit -- even though no conditions are introduced on \C{hd}, the mapping from \C{hd} to \C{Any} is given explicitly.

A simple form of pretty printing is employed to reduce the complexity of constraints for the user. \todo{describe here}

Taking the next example from regular expressions:

\begin{code}
(["[]",":"] :| [("hd",cons)]) :* (["[]",":"] :| [("hd",cons)])
\end{code}

This example reuses the \C{cons} constraint introduced before, in the interest of brevity. Again this constraint is much larger than the regular expression, partly because it fills out all the details.

The proof of finiteness for these constraints is relatively easy, given a finite type.

One operation on enumeration constraints that is not given here is normalisation. One way to optimise a constraint would be to remove duplicates. Using the relationship between constraints there are additional reductions that are possible. The normalisation operation is complex, but not difficult. The use of normalisation allows for a fixed point to be found more rapidly.

\subsection{Enumeration Based Propositions}

The enumeration constraints have different power to the regular expression constraints, but their primary advantage comes when they are combined into propositions. The advantage is that given two constraints on the same expression, combined at the proposition level, they can be reduced into one constraint:

\begin{code}
(e :< v1) || (e :< v2) = e :< (v1 ++ v2)
(e :< v1) && (e :< v2) = e :< [a `mergeVal` b | a <- v1, b <- v2]
\end{code}

Here the list comprehension in the |(&&)| rule is being used to generate the cross-product of the two lists.

The power to reduce constraints on equal expressions can be exploited further by translation of the program at the beginning. If every function takes exactly one argument, which guarantees that all expressions will result in being reduced to the same expression, as they will all be reduced to variables, and only one variable is present in each function. The easy way to perform this translation is to introduce tuples where a function would have more than one argument.

Combining these two properties means that a \C{Prop Req} is equivalent in power to \C{Req}. Exploiting this fact can lead to a faster computation of a fixed point.

The introduction of tuples would not harm the regular expression constraint system, due to the simplification rules present, but would not eliminate propositions. None of the simplification rules are able to optimise between discrete components in a tuple, leading to no benefit at the cost of a slightly obfusticated program.

\subsection{Comparison of Constraints}

Both constraint systems are capable of expressing a wide range of values, but neither subsumes the other. To give an example of the power, we give examples of two concrete values where one constraint language can differentiate between them, and the other cannot.

Take the following two values: |(T:[])| and |(T:T:[])|, using the enumeration based constraint of |(([":"] :| [("hd",Any)]) :* (["[]"] :| [("hd",Any)]))| the first matches but the second does not. Using propositions over regular expression constraints there is no way to differentiate between these two values.

Taking a tree data structure:

\begin{code}
data Tree a  =  Branch {left :: Tree a, right :: Tree a}
             |  Leaf a

v1 = Branch (Leaf True   ) (Leaf False  )
v2 = Branch (Leaf False  ) (Leaf True   )
\end{code}

Taking the values |v1| and |v2|, the regular expression constraint |([Star ["left"]] :- ["True"])| matches |v1| but not |v2|. Using enumeration based constraints, there is no way to differentiate between these two values.

The final Catch system uses enumeration based constraints, for their propositional simplification rules, which is critical in obtaining a fixed point quickly.

\section{Converting Haskell to Core}
\label{sec:transform}

The full Haskell language is a bit unwieldy for analysis. In particular the syntactic sugar complicates analysis by introducing more types of expression to consider. The checker works instead on a simplified language, a core to which other Haskell programs can be reduced. This core language is a functional language, making use of case expressions, function applications and algebraic data types. The abstract syntax tree of expressions is discussed in \S\ref{sec:core}.

\subsection{Yhc Core}

In order to generate a simplified language, it is natural to start with a full Haskell compiler, and we chose Yhc \citep{Yhc}, a fork of nhc \citep{nhc}. The internal language of Yhc is called PosLambda -- a simple variant of lambda calculus without types, but with source position information. Yhc works by applying basic desugaring transformations, without optimization transformations. This simplicity ensures the generated PosLambda is close to the original Haskell in its structure. Each top level function in a source file maps to a top level function in the generated PosLambda, retaining the same name.

PosLambda has constructs that have no direct representation in Haskell, for example there is a FatBar construct, which is used for compiling pattern matches which require fall through behaviour. The PosLambda language was always intended as an internal representation, and exposes certain details that are specific to the compiler. We have introduced a new Core language to Yhc, intended as a simple subset of Haskell where possible, on to which PosLambda can easily be mapped. This Core language is not explicitly typed, and has few constructs. We have also written a library, Yhc.Core, which is used by Yhc to generate these Core files, and by external programs to load and manipulate the generated Core.

The importance of this Core language is not what remains, but what has been removed.

\begin{itemize}
\item No syntactic sugar such as list comprehensions, \T{do} notation etc.
\item Only simple \T{case} statements, matching only the top level constructor
\item All \T{case} statements are complete, including an \T{error} call if necessary
\item All names are fully qualified
\item Haskell's type classes have been removed (see \ref{sec:dict})
\item Only top level functions remain, all local functions have been lambda lifted
\item All constructor applications are fully saturated
\end{itemize}

\subsection{The Dictionary Transformation}

Most of the transformations in Yhc operate at a local level, either on a single function at a time or on a small expression within a function. The only analysis/transformation phases which require information about more than one function are type checking and the dictionary transformation, used to implement type classes \citep{wadler:type_classes}.

\yesexample

Take the following code:

\begin{code}
f :: Eq alpha => alpha -> alpha -> Bool
f x y = x == y || x /= y
\end{code}

is translated by Yhc into

\begin{code}
f :: (alpha -> alpha -> Bool, alpha -> alpha -> Bool) -> alpha -> alpha -> Bool
f dict x y = (||) (((==) dict) x y) (((/=) dict) x y)

(==) (a,b) = a
(/=) (a,b) = b
\end{code}

The \C{Eq} class consists of two functions, |(==)| and |(/=)|. Depending on the type of the |alpha| variable in the function \C{f}, different code will be executed for the two functions.

\noexample

The dictionary transformation generates a tuple, containing the functions to use for the type class, and passes this structure into the \C{f} function as an additional argument. The additional argument can be seen as the dictionary, with functions being looked up in the dictionary to obtain the correct behaviour at runtime.

The dictionary transformation is a global transformation. The \C{Eq} context in \C{f} requires a dictionary to be accepted by \C{f}, and requires all the callers of \C{f} to pass a dictionary as well.

The type class mechanism in Haskell allows certain classes to require other classes, for example a definition of \C{Ord} requires a definition of \C{Eq} for the same type. In response the dictionary transformation generates a nested tuple, where the \C{Eq} dictionary is given as a member of the \C{Ord} dictionary.

\subsection{First Order Haskell}

Within a simple Core language, there are 2 main features that Haskell possesses that may complicate analysis: laziness and higher-order functions. It is possible to convert Haskell to be a first order language by using defunctionalization, and it is possible to convert Haskell to be a lazy language by continuation passing. For the purposes of this checker, laziness is a nice property, as it allows the code to be transformed with greater ease. Higher order functions are less helpful, and obscure the flow of control within a program -- their removal is beneficial.

\subsection{Reynold's style defunctionalization}

Reynold's style defunctionalization \citep{reynolds:defunc} is a simple method of generating a first order program from a higher order one. Taking the following example:

\begin{code}
map f x = case  x of
                []      -> []
                (a:as)  -> f a : map f as
\end{code}

Here \C{f} is a higher order input, of type |(a -> b)|. Defunctionalisation works by creating a data type to represent all possible f's, and using that. For example:

\begin{code}
data Functions = Head | Tail

apply Head  x = head  x
apply Tail  x = tail  x

map f x = case  x of
                []      -> []
                (a:as)  -> apply f a : map f as
\end{code}

Now all calls to map head would be replaced with |map Head|, and \C{Head} is a first order value. This scheme can easily be extended to currying, let us take the \C{add} function, which adds two \C{Int}'s:

\begin{code}
data Functions = Head | Tail | Add0 | Add1 Int

apply Add0      n  = Add1 n
apply (Add1 a)  b  = add a b
\end{code}

There are a couple of things to note about this approach. One is that although the transformed code is still type safe, to do type checking would require a dependently typed language. Although unfortunate, it presents no problem for our checker, which does not use type information. The unacceptable aspect is the creation of an apply function, whose meaning is excessively general. This transformation introduces a bottleneck through which various properties must be proven. Asking questions, such as is the result of \C{apply} a |[]| value, are confusing.

Reynold's style defunctionalization is a suitable method for removing higher order functions, but is not ideal.

\subsection{Specialisation}

One technique that can be used to remove higher order functions, and indeed was employed in the original version of Catch, is specialisation. A mutually recursive group of functions can be specialised in their $n$th argument if in all recursive calls this
argument is invariant.

Examples of common functions whose applications can be specialised include \T{map}, \T{filter}, \T{foldr} and \T{foldl}.

When a function can be specialised, the expression passed as the $n$th argument has all its free variables passed as extra arguments, and is expanded in the specialised version. All recursive calls within the new function are then renamed.

\yesexample

\begin{code}
map f xs = case  xs of
                 []      -> []
                 (a:as)  -> f a : map f as

adds x n = map (add n) x
\end{code}

\noindent is transformed into:

\begin{code}
map_adds n xs = case  xs of
                      []      -> []
                      (a:as)  -> add n a : map_adds n as

adds x n = map_adds n x
\end{code}\noexample

Although this firstification approach is not complete by any means, it is sufficient for a large range of examples. This approach has the unfortunate effect of specialising more than just higher order functions, which can lead to obscured code. It also has complications with dictionaries and point-free style code.

\subsection{Higher Order Removal}

A new technique of higher order function removal has been developed. Although this technique is not complete, in practice it is sufficiently powerful. The algorithm combines specialisation with inlining, and only removes higher order functions.

The algorithm proceeds as follows. First two additional constructions are introduced into the expression data type:

\begin{code}
data Expr  =  ...
           |  Part Int FuncName [Expr]
           |  Apply Expr [Expr]

Part 0 fn xs == Call fn xs
Apply (Part n fn xs) ys == Part (n - length ys) fn (xs ++ ys)
\end{code}

The \C{Apply} constructor is simply application, which allows an unsaturated function call, or a variable to be used as the function. The \C{Part} constructor is used to represent unsaturated function calls, leaving the normal \C{Call} constructor to represent saturated calls. The \C{Part} keeps track of how many missing arguments it has. To express a higher-order program requires \C{Apply}. Annotating a higher order program with \C{Part} as appropriate is a simple operation. This algorithm assumes that \C{Make} is always fully saturated -- which is easy to achieve with the introduction of new functions as necessary.

\begin{figure}
\begin{code}
isHO :: Expr -> Bool
isHO (Part n _ _)    = n > 0
isHO (Make _ xs)     = any isHO xs
isHO (Case on alts)  = any (isHO . snd) alts
isHO _               = False
\end{code}
\caption{Tests for the firstifier}
\label{fig:firstify}
\end{figure}

The algorithm has two separate stages, specialisation and inlining. The algorithm applies the specialise rule until a fixed point is reached, then applies the inline rule once. The algorithm repeats these two steps until a fixed point is reached. Given an appropriate \C{fix} function, \C{firstify} can be implemented as:

\begin{code}
firstify = fix (inline . fix specialise)
\end{code}

\paragraph{Inline:} The \C{inline} stage of the algorithm is much simpler than the specialisation aspect, and hence is treated first. A function is inlined once if the body passes the \C{isHO} test. If this process causes a function to no longer be called from the root of the program, then the function is removed after inlining.

\paragraph{Specialise:} The \C{specialise} stage of the algorithm takes expressions of the form |Call fn xs| where |any isHO xs|, and generates a specialised version of the function \C{fn} where all the higher order elements are frozen in, and all the other variables are passed normally.

Assuming that a function |fn| results in the specialised version |fn'|, then the translation would be:

\begin{code}
transform (Call fn xs) | any isHO xs = Call fn' (concatMap f xs)
    where
    fn' = generate fn xs

    f x  | isHO x     = freeVars x
         | otherwise  = [x]
\end{code}

This technique is powerful -- there is no program without a higher order initial input which will not have a rule available for firing. The fact that a higher order function has ``no where to hide'' can be seen easily. A higher order function must either be the first expression in a function (in which case it will be inlined), or it must be the argument to a function (in which case it is specialised). There are only two places left for a higher order function:

\begin{enumerate}
\item As the subject of a |case| expression. This situation is not possible due to static typing -- all |case| expressions must choose over a data value.

\item Inside an \C{Apply} with a variable as the function. This situation does not happen if all other higher order functions have been removed, and hence is safe.
\end{enumerate}

One remaining issue is the \C{seq} primitive of Haskell, which has no natural translation to Core, and is usually expressed as a primitive function. The \C{seq} command is special in that it is a primitive which is polymorphic in both arguments. If a higher order function is the first argument of \C{seq} then the \C{seq} can be removed, and replaced with the second argument. If the second argument is a higher order function, but the first function is guaranteed not to be, then this call can be replaced with a |case| expression. In Haskell |case x of _ -> y| would not evaluate |x|, however in the Core language the subject of a |case| is always evaluated.

The next tricky point is \C{undefined}, consider the program:

\begin{code}
main = f undefined
f x = x 1
\end{code}

This program passes \C{undefined} as a function, which is allowable as \C{undefined} has a fully polymorphic result. This problem can be worked around by treating \C{undefined} (and by extension, \C{error}) as higher order functions.

And finally, it is possible to construct a program which has an infinite number of specialisations:

\begin{code}
data Wrap a = Wrap (Wrap a) | Value a

main = f head

f x = f (Wrap x)
\end{code}

This pattern requires higher order values to be placed in a recursive data structure, then an algorithm which grows the data structure to operate on it. In reality, higher order functions are rarely placed in any data structure other than a tuple. This situation can be detected, and a fallback to Reynold's style defunctionalisation is possible.

\section{Results and Evaluation}
\label{sec:results}

The best way to see the power of Catch is by example. This section first starts with artificial small benchmarks which demonstrate the power of Catch. The next section covers the Nofib suite, large examples which we have checked with the Catch tool -- with pleasing results.

The Catch tool works by analysing the \C{main} function, and allows addition variables to be passed in, which preconditions are generated on. If a standard Haskell program is passed in, then the \C{main} function takes no arguments and is of type |main :: IO ()| -- this scheme works equally well.

Catch additionally produces a log containing the entire program in Core form before analysis begins, including the effect of abstraction and firstification, a list of all the template axioms calculated and the preconditions on every function.

\subsection{Small Examples}

\begin{example}
\begin{code}
main x = head x
> forall main `o` x :< [ (:) ]
\end{code}

The program infers that \C{head} crashes on the empty list, then propagates this precondition.
\end{example}

\begin{example}
\begin{code}
main x = map head x
> forall main `o` x :< [ [] + : (:) _ :* [] + : (:) _ ]
\end{code}

This program infers the precondition on \C{head} as before, then obtains a fixed point because of the recursive call in \C{map}. Behind the scenes this program generates the specialised version of \C{mapHead}. The final condition on \C{main} is that the constraint must be a |[]| or a |:| at any level, but if it is a |:| then the first element must be a |:| itself.
\end{example}

\begin{example}
\begin{code}
main x = map head (reverse x)
> forall main `o` x :<  [  [] + (:) :* (:)
>                       |  [] + : (:) _ :* [] + : (:) _]
\end{code}

This example shows the use of \C{reverse}, which is defined in terms of \C{foldl}, and is therefore not directly recursive. The second condition produced is the same as the previous example. The first condition says that either the list is empty, or the list is infinite -- an infinite list is safe because \C{reverse} is tail strict, and therefore no crash will ever occur, as the program will loop forever.
\end{example}

\begin{example}
\begin{code}
main x = tail (tail x)
> forall main `o` x :< [ (:) :* (:) ]
\end{code}

Finally we give an example where Catch fails. The abstraction employed for lists cannot differentiate between the 2 element list (the minimal precondition for the preceding example) and the infinite list (the precondition given). This situation is unfortunate, but a minor modification to \C{drop 2} would ensure safety.
\end{example}

\subsection{The Nofib Benchmark}

The whole Nofib benchmark \citep{nofib} is too large for the authors to take the time to investigate all the programs, so instead the imaginary section has been chosen. These programs are all under a page of text, and particularly stress list operations and numeric computations. All the benchmark code is available online\footnote{\texttt{http://darcs.haskell.org/nofib/imaginary/}, as of February 2007}.

To take a typical benchmark, Primes, the \C{main} function is:

\begin{code}
main = do  [arg] <- getArgs
           print $ primes !! (read arg)
\end{code} % $

The first unsafe pattern here is |[arg] <- getArgs|, as \C{getArgs} is a primitive whose implementation is opaque to the system. As a result the system will always return False as the precondition to main.

The next step where many fail is that \C{read} is applied to an argument extracted from getArgs. Firstly this argument is entirely unknown. Secondly, \C{read} is a sufficiently complicated function that although it can be modelled by Catch, there is no possibility for getting an appropriate abstraction which models the failure case. As a result, any program which calls \C{read} is unsafe, according to Catch. Using \C{reads}, which indicates failure properly, can allow a program to still be checked.

Catch can accurately calculate these preconditions, but their repetitiveness throughout the suite obscures the ``interesting'' pattern matches that also exist. As a result the programs have been rewritten to:

\begin{code}
main x = print $ primes !! (read x)
\end{code} % $

\begin{figure}
\begin{tabular}{||l||rrr||rr||r||l||l||l||}
\hline
Program       &\multicolumn{3}{c||}{Code Size} & \multicolumn{2}{c||}{Pre} & Rank & Changes \\
\hline
Bernoulli     & 35 & 1616 &  652 & 5 & 11 & 33 & 1 Bad  \\
Digits of E1  & 44 &  957 &  377 & 3 &  8 & 10 & 1 Good \\
Digits of E2  & 54 & 1179 &  455 & 5 & 19 & 28 & None   \\
Exp3-8        & 29 &  220 &  163 & 0 &    &    & None   \\
Gen-Regexps   & 41 & 1006 &  776 & 1 &  1 &  1 & 4 Good \\
Integrate     & 39 & 2466 &  364 & 3 &  3 &  6 & None   \\
Paraffins     & 91 & 2627 & 1153 & 2 &  2 &  * & 2 Bad  \\
Primes        & 16 &  302 &  241 & 6 & 13 & 15 & None   \\
Queens        & 16 &  648 &  283 & 0 &    &    & None   \\
Rfib          &  9 & 1918 &  100 & 0 &    &    & None   \\
Tak           & 12 &  209 &  155 & 0 &    &    & None   \\
Wheel Sieve 1 & 37 & 1221 &  570 \\
Wheel Sieve 2 & 45 & 1397 &  636 \\
X2n1          & 10 & 2637 &  331 & 2 &  5 &  6 & None   \\
\hline
\end{tabular}
\caption{Table of results}
\label{fig:results}
\end{figure}

The raw results are given in Figure \ref{fig:results}. Where changes were required to prove the program, the table gives the statistics for the modified program.

\begin{description}
\item[Code Size] represents the number of lines of code. The first figure is the number of lines of Haskell written for this benchmark. The second figure is the total number of lines of Yhc Core, including all functions used from libraries, when pretty printed. The final figure is the number of lines after firstification, just before analysis.

\item[Pre] is the number of calls to error, followed by the number of functions which have a precondition which is not True.

\item[Rank] is a rough measurement of the complexity of the pattern match analysis. This value is the number of calls to functions with preconditions. This value does not relate to the time required to check the program.

\item[Changes] is the number of modifications that had to be made to the program to make it validate. Changes are marked as good where either the program was genuinely unsafe, or was too complex for the authors of this paper to determine safety manually. Bad changes are those which were necessary because of limitations of the tool -- these are discussed.
\end{description}

Of the programs 4 used the |(!!)| function, which requires the index to be non-negative and less than the length of the list -- in practice Catch can only prove this condition if the list being indexed on is infinite. 8 programs used either \C{head} or \C{tail}, most of which can be proven safe. There are 7 programs incomplete patterns, often as a result of |where| binding -- Catch performs well on these. In this particular benchmark suite there are 9 programs which use division, with the precondition that the divisor must not be zero. Most of these can be proven safe. Only 4 programs in this benchmark have no pattern match errors.

Three programs have preconditions on the \C{main} function, all of which state that the argument must be a natural number, one such program is Primes. In all cases the generated precondition is minimal -- if the input violates the precondition and pattern match failure will occur.

We will now discuss the 4 programs which required changes, along with the Digits of E2 program -- the other program with the highest rank.

\paragraph{Bernoulli}

This program has one instance of |tail (tail x)|, which is beyond the capabilities of Catch. Replacing this expression with |drop 2 x| allows all other pattern matches to be proven. Even with this change, the program still has a high level of potential pattern match errors -- all of which can be proven.

\paragraph{Digits of E1}

This program contains the following equation:

\begin{code}
ratTrans (a,b,c,d) xs |
  ((signum c == signum d) || (abs c < abs d)) &&
  (c+d)*q <= a+b && (c+d)*q + (c+d) > a+b
     = q:ratTrans (c,d,a-q*c,b-q*d) xs
  where q = b `div` d
\end{code}

Catch is able to prove that the division by |d| is only unsafe if both |c| and |d| are zero. Proving that this invariant is maintained is beyond the capabilities of either Catch or the authors. A slight modification is possible:

\begin{code}
  where q = if d == 0 then 0 else b `div` d
\end{code}

This expression gives the same behavior where the previous program would have, but $\bot{}$ is now replaced by a defined (although potentially nonsensical) value.

\paragraph{Gen-Regexps}

This program was written expecting valid input, and crashes on even slight variations from what is expected. Ways of crashing the program include entering |""|, |"["|, |"<"| and lots of other inputs.

One potential error comes from |head . lines|, which can be replaced by |takeWhile (/= '\n')| to have the same effect, apart from on the empty list, where it returns |""|.

One pattern match error is can be fixed by adding a default clause.

Two errors of the form |(a,_:b) = span f x| can be fixed by changing them to |(a,b) = safeSpan f x|, where \C{safeSpan} is defined as:

\begin{code}
safeSpan p x = (a, drop 1 b)
    where (a,b) = span p x
\end{code}

This example shows that a program written with no regard for pattern match safety can be made safe, without excessive code alterations.

\paragraph{Paraffins}

The Paraffins program cannot be validated by Catch, without unfortunately large modifications. The first error, which can be fixed easily, is the use of undefined to prevent a space leak:

\begin{code}
radical_generator n = radicals undefined
  where radicals unused = big_memory_computation
\end{code}

In Haskell if a function is defined with no arguments it is considered a constant applicative form (CAF), and has its results computed only once in the program. To prevent something which allocates a lot of memory from having this behaviour, a dummy argument can be passed. The \C{undefined} is merely serving as a dummy argument, but if evaluated would result in a pattern match error. If the analysis was lazy (as discussed in \S\ref{sec:precond}) then this example would succeed using Catch, although as currently implemented simply changing \C{undefined} to |()| removes the potential error.

The Paraffins program also uses the function |array :: Ix a => (a, a) -> [(a, b)] -> Array a b| which takes a list of index/value pairs and builds an array. The precondition on this function is that all indexes must be in the range specified and all arguments must be valid. This precondition is too complex for Catch, but simply using \C{listArray}, which takes a list of elements one after another, the program can be validated. In the particular example used in Paraffins, the use of \C{listArray} leads to shorter and more readable code.

The final troublesome error in Paraffins is the use of the |(!)| indexing operator. The precondition requires that the index is in the bounds given when the array was constructed, something Catch does not currently model.

This program shows that one weak area of Catch is code that uses arrays, fortunately in Haskell arrays are a relatively uncommon data structure.

\paragraph{Digits of E2}

This program is particularly complex, featuring a number of possible pattern match errors. Catch is particularly effective at the job of spotting pattern match errors:

\begin{code}
  where  carryguess = d `div` base
         remainder = d `mod` base
         nextcarry:fraction = carryPropagate (base+1) ds
         dCorrected = d + nextcarry
\end{code}

Here there are 3 potential pattern match errors in a relatively small amount of code. Two of these are the calls to |div| and |mod|, both requiring |base| to be non-zero. A possibly more subtle pattern match error is the |nextcarry:fraction| left hand side of the 3rd line -- humans examining source code can easily miss these pattern bindings. Catch is able to prove all of these patterns safe.

\begin{code}
e =  ("2."++) $
     tail . concat $
     map (show.head) $
     iterate (carryPropagate 2 . map (10*) . tail) $
     2:[1,1..]
\end{code}

This fragment has two calls to \C{tail}, and one to \C{head}. They are mixed in a relatively complex manner, including as a higher order functional pipeline in the case of \C{iterate} and \C{map}. Understanding and manually validating this fragment would be tricky, but Catch is able to do so automatically.


\section{Related Work}
\label{sec:related}

\subsection{Proving Incomplete Patterns Safe}

Despite the seriousness of the problem of pattern matching, the topic of pattern match error detection and elimination has not been studied extensively. The two papers which offer the most promising start in this area are the approach by \cite{me:catch_tfp} and ESC/Haskell \citep{esc_haskell}. The first paper attempts to define an automatic inference that pattern matches are safe, similar to the analysis presented in this paper. Unfortunately the restrictions in that paper make it an interesting starting point, but nothing more. The analysis in this paper differs radically in many design decisions -- particularly in relation to fixed points and constraints. Perhaps the introductory risers example is most clearly illustrates the differences -- for the original paper risers was towards the boundary of what was possible, for this paper it is trivial.

The ESC/Haskell approach requires the programmer to give explicit preconditions and contracts which the program obeys. This approach is likely to result in a checker with more expressive power in the end (one of the examples involves an invariant on an ordered list, something beyond Catch), but requires the programmer to spend more time doing the validation. It should be noted that the preconditions derived by Catch, along with the properties, could easily be turned into ESC/Haskell annotations, allowing both tools to be used together.

\subsection{Eliminating Incomplete Patterns}

One simple way to guarantee that a program does not crash with an incomplete pattern is to ensure that there are no pattern match errors in the code. The GHC compiler  \citep{ghc} has a warning flag to detect incomplete patterns, named \T{-fwarn-incomplete-patterns}. Adding this flag when compiling risers results in a message that ``Pattern matches are non-exhaustive''. Unfortunately the Bugs (12.2.1) section of the manual notes that the checks are sometimes wrong, particularly with string patterns or guards, and that this part of the compiler ``needs an overhaul really'' \citep{ghc_manual}. But the GHC checks are only local. Using an incomplete function from a library, such as \C{head}, gives no warning. If the function \C{head} is defined, then it raises a warning.

There have been attempts at defining what it means for a program to be complete with regard to pattern matches \citep{maranget:pattern_warnings}, along with associated implementations.

One approach for programming is to design a functional language without pattern match errors or non-termination \cite{turner:total}. In this approach incomplete pattern matches are banned, suggesting this restriction will ``force you to pay attention to exactly those corner cases which are likely to cause trouble''. The results in \S\ref{sec:nofib} show that in sample Haskell programs there are a reasonably high number of incomplete pattern matches, which are still safe. Using the Catch tool this restriction could be reduced.

\subsection{A Mistake Detector}

One tool in most Haskell programmers arsenals is the QuickCheck tool \citep{quickcheck}. This tool allows properties to be specified which are randomly tests. In many cases these properties may be similar to those derived by the templating mechanism -- if the Catch tool had a way of directly entering these conditions then perhaps the simpler properties could be proved automatically.

There has been a long history of writing tools to analyse programs to detect potential bugs, going back to the classic C Lint tool \citep{lint}. In the functional arena the Dialyzer \citep{dialyzer} for Erlang \citep{erlang} performs a similar function. The aim is to have a static checker that works on unmodified code, with no additional annotations. However, a key difference is that in Dialyzer all warnings indicate a genuine problem that needs to be fixed. Because Erlang is a dynamically typed language, a large proportion of Dialyzer's warnings relate to mistakes a type checker would have detected.

Part of the mistake detection aspect can be seen as defining a slightly more restrictive type system. In this light the checker can be compared to the tree automata work done on XML \citep{xml} and XSL \citep{static_xslt}, which can be seen as an algebraic data type and a functional language. Another soft typing system with similarities is by Aiken \cite{aiken:type_infer}, on the functional language FL. This system tries to assign a type to each function using a set of constructors, for example \T{head} is given just \T{Cons} and not \T{Nil}.

\subsection{Type System Safety}

\begin{figure}
\begin{code}
data Cons; data Unknown
newtype List a t = List [a]

nil :: List a Unknown
nil = List []

cons :: a -> [a] -> List a Cons
cons a as = List (a:as)

fromList :: [a] -> List a Unknown
fromList xs = List xs

safeTail :: List a Cons -> a
safeTail (List (a:as)) = as
\end{code}
\caption{A safe \C{head} function with Phantom types.}
\label{fig:phantom}
\end{figure}

One method which is frequently used to encode invariants on data in functional languages is the type system. One approach is the use of Phantom types \citep{fluet:phantom}, for example a safe variant of \C{head} can be written as in Figure \ref{fig:phantom}. In this example the \C{List} data structure would not be exported, ensuring that all lists with a \C{Cons} tag are indeed non-empty. The values \C{Cons} and \C{Unknown} are phantom types -- they exist only in the type level, not the value level.

\begin{figure}
\begin{code}
data ConsT a; data NilT

data List a t where
    Cons  :: a -> List a b -> List a (ConsT b)
    Nil   :: List a NilT

safeTail :: List a (ConsT t) -> List a t
safeTail (Cons a b) = b

fromList :: [a] -> (forall t. List a t -> r) -> r
fromList []      fn = fn Nil
fromList (x:xs)  fn = fromList xs (\sl -> fn (Cons x sl))
\end{code}
\caption{A safe \C{tail} function using GADT's.}
\label{fig:gadt}
\end{figure}

Another method of encoding addition type information which is becoming increasingly popular in Haskell is the use of GADT's \citep{gadt}. Using this technique, sometimes referred to as first class Phantom Types, an encoding of lists can be written as in Figure \ref{fig:gadt}. Unlike the first method, the \C{fromList} method requires the complexity of existential types.

The type directed method can be pushed much further with dependant types, which allow types to depend on values. This approach allows safe versions of \C{head} and \C{tail} to be written. There has been much work on type systems, using undecidable type systems \citep{cayenne, epigram}, using extensible kinds \citep{omega} and using type systems resctricted to a decidable fragment \citep{xi:dependent_practical}.

The downside to all these type systems is that they require the programmer to make explicit annotations, and require the user to learn new techniques for computation.


\section{Conclusions and Future Work}
\label{sec:conclusion}

A static checker for potential pattern-match errors in Haskell has been specified and implemented. This checker is capable of determining preconditions under which a program with non-exhaustive patterns executes without failing due to a pattern-match error. A section of the Nofib suite has been tested, with encouraging results -- all but one program can be proved safe, most without any modification at all.

There are two main avenues of future work to extend to the mechanisms introduced in this paper. There first is to extend the power of the checker. Possible directions to increase the power include the addition of explicit annotations of properties, or a more powerful constraint language. The constraint languages introduced could be augmented with special purpose constraints, designed to tackle particular problems. One particular constraint system that may prove fruitful is a linear inequality constraint system.

The next direction for future work would be using the Catch tool to solve other problems. The direct result of the absence of pattern match failure could be used to feed information to an optimising compiler. The properties generated could also be used to generate more efficient code. If the programmer was able to specify more restrictive preconditions, or properties, or invariants, then these may be able checked with Catch. If the power of the constraint system was increased then richer program annotations could be used.

As the Catch tool stands, it is already capable of detecting and proving the absence of pattern match failures in sample programs. We hope this tool will become a part of the standard arsenal of Haskell programmers.


% \appendix
% \section{Appendix Title}
%
% Here is the text of the appendix, if you need one.

\acks

The first author is a PhD student supported by a studentship from the Engineering and Physical Sciences Research Council of the UK.

\bibliographystyle{plainnat}
\bibliography{catch}



\end{document}
