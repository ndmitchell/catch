\documentclass[preprint]{sigplanconf}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{alltt}
\usepackage{url}
\usepackage{natbib}
\usepackage{datetime}
\usepackage{comment}

% from the ICFP website
\bibpunct();A{},
\let\cite=\citep

%include polycode.fmt
%include catch.fmt

% general stuff
\newcommand{\T}[1]{\texttt{#1}}
\newcommand{\C}[1]{\textsf{#1}}
\newcommand{\tup}[1]{\ensuremath{\langle #1 \rangle}}
\newcommand{\mtxt}[1]{\textsf{#1}}

\newcommand{\catch}{\textsc{Catch}}

% examples
\newcounter{exmp}
\setcounter{exmp}{0}
\newcommand{\yesexample}{\addtocounter{exmp}{1}\addvspace{2mm}\noindent\textbf{Example \arabic{exmp}}}
\newcommand{\noexample}{\hfill$\Box$\par\addvspace{2mm}}
\newcommand{\lastexample}{\arabic{exmp}}

\newcommand{\todo}[1]{\textbf{\textsc{Todo:} #1}}

\newcommand{\para}[1]{\vspace{2mm}\noindent\textbf{#1}}

% code blocks
\newenvironment{codepage}
    {\begin{minipage}[h]{\textwidth}\begin{code}}
    {\end{code}\end{minipage}}

\newenvironment{discuss}
    {\noindent\hspace{-1.5mm}\vline\hspace{1mm}\vline\hspace{1mm}\begin{minipage}[h]{\linewidth}}
    {\end{minipage}}


\newenvironment{example}{\yesexample}{\noexample}

\newenvironment{revisit}[1]
    {\addvspace{2mm}\noindent\textbf{Example #1 (revisited)}}
    {\noexample}

\newcommand{\K}{\ensuremath{^\ast}} % kleene star
\newcommand{\D}{\ensuremath{\cdot}} % central dot

\renewcommand{\c}[3]{\tup{\T{#1},\T{#2},\T{\{#3\}}}}
\newcommand{\cc}[2]{\c{#1}{$\lambda$}{#2}}

\newcommand{\s}[1]{\ensuremath{_{\tt #1}}} % subscript, in tt font
\newcommand{\g}[1]{\{#1\}} % group, put { } round it
\newcommand{\U}{\textunderscore}
\newcommand{\vecto}[1]{\overrightarrow{#1\;}}
\newcommand{\gap}{\;\;}
\newcommand{\dom}{\text{dom}}


\begin{document}

\conferenceinfo{ICFP '07}{date, City.} %
\copyrightyear{2007} %
\copyrightdata{[to be supplied]}

\titlebanner{\today{} - \currenttime{}}        % These are ignored unless
\preprintfooter{}   % 'preprint' option specified.

\title{Not All, But Enough}
\subtitle{ -- an automatic verifier for partial but sufficient pattern matching in Haskell}

\authorinfo{Neil Mitchell}
           {University of York, UK}
           {http://www.cs.york.ac.uk/$\sim$ndm/}
\authorinfo{Colin Runciman}
           {University of York, UK}
           {http://www.cs.york.ac.uk/$\sim$colin/}

\maketitle

\begin{abstract}
A Haskell program may fail at runtime with a pattern-match error if the program has any incomplete (non-exhaustive) patterns in definitions or case alternatives. This paper describes a static checker that allows non-exhaustive patterns to exist, yet can verify that a pattern-match error does not occur. We describe a constraint language that can be used to express requirements for safe pattern matches, along with mechanisms to propagate and reason about these constraints between program components.

The checker we develop using these methods obtains minimal preconditions for 10 out of 14 programs in the Imaginary section of the Nofib suite\citep{nofib}. With minor modifications a further 3 are proven safe. We also check two larger example programs with success, finding several previously unknown errors.
\end{abstract}

% \category{CR-number}{subcategory}{third-level}

% \terms
% term1, term2

% \keywords
% keyword1, keyword2

\section{Introduction}
\label{sec:introduction}

\begin{discuss}
Often it is useful to define selector functions, which are defined only for arguments matching a specific constructor pattern, for example \C{head} fails on the empty list. Unfortunately programs with incomplete pattern matches may fail at runtime.
\end{discuss}

The problem of pattern match failures in Haskell is a serious one. The darcs \citep{darcs} project is one of the most successful large scale programs written in Haskell. Taking a look at the darcs bug tracker, 13 problems are errors related to the selector function \C{fromJust} and 19 are direct pattern match failures.

Consider the following example:

\begin{code}
risers :: Ord alpha => [alpha] -> [[alpha]]
risers [] = []
risers [x] = [[x]]
risers (x:y:etc) = if x <= y then (x:s):ss else [x]:(s:ss)
    where (s:ss) = risers (y:etc)
\end{code}

A sample application of this function is:

\begin{code}
> risers [1,2,3,1,2]
[[1,2,3],[1,2]]
\end{code}

In the last line of the definition, |(s:ss)| is matched against the result of |risers (y:etc)|. If the result is in fact an empty list a pattern match error will be raised. It takes a few moments to check manually that no pattern-match failure can occur -- and a few more to be sure one has not made a mistake! Turning the \C{risers} function over to the checker developed in this paper, the output is:

\begin{verbatim}
> Incomplete pattern on line 5, proven safe
> Program is Safe
\end{verbatim}

In addition the checker produces a set of properties it has proved about Risers, along with an outline of the proofs. Fuller details of how the checking is performed follow in \S\ref{sec:walkthrough}.

\subsection{Contributions}

The contributions of this paper include:

\begin{itemize}
\item Two constraint languages to reason about pattern-match failures.
\item A mechanism for generating and solving appropriate constraints.
\item Details of an implementation which handles the complete Haskell 98 language \citep{haskell}.
\item A mechanism for transforming Haskell 98 programs to a first-order language.
\item Results showing success on a number of small examples drawn from the Nofib suite \citep{nofib}, and for a couple of complete programs, investigating the scalability of the checker.
\end{itemize}

In \citet{me:catch_tfp} a pattern match checker was described with similar aims, which we will refer to as \catch05\footnote{Although the paper was completed in 2005, publication was delayed}. All the issues listed in the future work section of that paper have been addressed in this paper. \catch07 is robust enough to handle real Haskell programs, with no restriction on the recursion patterns, higher-order functions or type classes. Scalability problems have been addressed and \catch07 is able to check much larger programs than \catch05: for \catch05 risers was towards the boundary of what was possible, for \catch07 it is trivial. The underlying constraint mechanism of \catch07 is radically different from \catch05.

\subsection{Road map}

\S\ref{sec:walkthrough} gives an overview of the checking process for the \C{risers} function. \S\ref{sec:manipulate} introduces a small core functional language and a mechanism for reasoning about this language, \S\ref{sec:constraint} covers a constraint language. \S\ref{sec:transform} discusses how to transform Haskell to this core language. \S\ref{sec:results} evaluates Catch on various sample programs -- including the Nofib benchmark suite. \S\ref{sec:related} compares our work to other work before \S\ref{sec:conclusion} presents concluding remarks.

\section{Walkthrough of Risers}
\label{sec:walkthrough}

This section sketches the process of checking that the \C{risers} function in the Introduction does not crash with a pattern-match error.


\subsection{Conversion to a Core Language}

Rather than analyse full Haskell, \catch{} analyses a first-order Core language, without lambda expressions, partial application or let bindings. A convertor is provided from the full Haskell 98 language to this restricted language. Full details are provided in \S\ref{sec:transform}. After converting the \C{risers} program to Core Haskell, and renaming the identifiers for ease of human reading, the resulting Core is as shown in Figure \ref{fig:risers_core}.
 
The type of \C{risers} is polymorphic over types in |Ord|. \catch{} could check \C{risers} assuming that |Ord| methods do not raise pattern match errors, and may return any value. A type instance such as |Int| can be specified with a type signature.

\begin{figure}
\begin{code}
risers x = case x of
    [] -> []
    (y:ys) ->  case ys of
         [] -> (y : []) : []
         (z:zs) -> risers2 (risers3 z zs) (y <= z) y

risers2 x y z =  case y of
    True -> (z : snd x) : (fst x)
    False -> (z : []) : (snd x : fst x)

risers3 x y = risers4 (risers (x : y))

risers4 x = case x of
    (y:ys) -> (ys, y)
    [] -> error "Pattern Match Failure, 11:12."
\end{code}
\caption{\C{risers} in the Core language.}
\label{fig:risers_core}
\end{figure}

\subsection{Analysis}

In the Core language every pattern match covers all possible constructors of the appropriate type. The alternatives for constructor cases not originally given are calls to \C{error}. The analysis starts by finding calls to \C{error}, then tries to prove that these calls will not be reached. In the \C{risers} example there is only one \C{error} call, corresponding to the |where| pattern match.

A single precondition is generated (see \S\ref{sec:precond}):

\begin{code}
risers4, x :< (:)
\end{code}

That is, all callers of \C{risers4} must supply an argument which is a |(:)|-constructed value. For the proof that this precondition suffices, two properties are required (see \S\ref{sec:backward}):

\begin{code}
x :< (:)  => (risers x       ) :< (:)
True      => (risers2 x y z  ) :< (:)
\end{code}

\noindent The first property says that if the argument to \C{risers} is a |(:)|-constructed value, the result will be. The second states that the result from \C{risers2} is always |(:)|-constructed.

\section{Pattern Match Analysis}
\label{sec:manipulate}

This section explains the methods used to calculate preconditions for functions. A basic constraint language is introduced, along with operations upon it.

\subsection{Reduced expression language}
\label{sec:core}

\begin{figure}
\begin{code}
type CtorName  =  String
type FuncName  =  String
type VarName   =  String
type Selector  =  (CtorName, Int)

data Func  =  Func FuncName [VarName] Expr

data Expr  =  Var   VarName
           |  Make  CtorName  [Expr]
           |  Call  FuncName  [Expr]
           |  Case  Expr      [Alt]

data Alt   =  Alt CtorName [VarName] Expr
\end{code}
\caption{Core Data Type.}
\label{fig:core}
\end{figure}

\begin{figure}
\begin{code}
data Value = Bottom | Value CtorName [Value]

eval :: Expr -> Value
eval x = case  hnf x of
               Nothing      -> Bottom
               Just (c,cs)  -> Value c (map eval cs)

-- Nothing corresponds to bottom
hnf :: Expr -> Maybe (CtorName, [Expr])
hnf (Make  x   xs    )  =  Just (x, xs)
hnf (Call  x   xs    )  |  x == "error"  = Nothing
                        |  otherwise     = hnf (instantiate x xs)
hnf (Case  on  alts  )  =  listToMaybe [hnf (subst (zip vs cs) e)
       | Just (c,cs) <- [hnf on], Alt n vs e <- alts, c == n]

subst :: [(VarName, Expr)] -> Expr -> Expr
subst r (Var   x     ) = fromJust (lookup x r)
subst r (Make  x xs  ) = Make  x (map (subst r) xs)
subst r (Call  x xs  ) = Call  x (map (subst r) xs)
subst r (Case  x xs  ) = Case (subst r x)
    [Alt n vs (subst r e) | Alt n vs e <- xs]
\end{code}
\caption{Semantics for Core expressions.}
\label{fig:semantics}
\end{figure}

Syntax for the core language is given in Figure \ref{fig:core}. Our Core language is a little more restrictive than the core languages typically used in compilers \citep{ghc_core}. The language is first order, has only simple case statements, and only algebraic data types. All case statements have alternatives for all constructors of a particular type, with error being introduced where a pattern-match error would otherwise occur. An \C{Alt} represents a constructor to match, a list of variables bound to constructor arguments and an expression to evaluate.

The evaluation strategy is lazy. A semantics is outlined in Figure \ref{fig:semantics}. The |hnf| function evaluates an expression to head normal form. The |subst| function substitutes free variables that are the result of a |Case| expression. Laziness is a useful property as it allows $\beta$-reduction to be performed by the checker.

\subsubsection{Operations on Core}

\begin{figure}
\begin{code}
ctors        :: CtorName  -> [CtorName]
arity        :: CtorName  -> Int
var          :: VarName   -> Maybe (Expr, Selector)
instantiate  :: FuncName  -> [Expr] -> Expr
isRec        :: Selector  -> Bool
\end{code}
\caption{Operations on Core.}
\label{fig:core_operations}
\end{figure}

Figure \ref{fig:core_operations} gives the signatures for helper functions over the core data type. Every constructor has an arity, which can be obtained with the \C{arity} function. To determine alternative constructors the \C{ctors} function can be used, for example |ctors True = {False, True}| and |ctors [] = {[], :}|. The \C{var} function returns \C{Nothing} for a variable bound as an argument of a top-level function, and |Just (e,(c,i))| for a variable bound as the |i|th component in the |c|-constructed alternative of case-expression |e|. The function \C{instantiate} corresponds to $\beta$ reduction.

The |isRec (c,n)| function returns true if the constructor |c| has a recursive element at position |n|. This function is used to limit the size of constraints. Taking the example of a \C{List}:

\begin{code}
data List alpha = Nil | Cons alpha (List alpha)
\end{code}

\C{Cons} has two selectors, the first is of type |alpha| and is not recursive, the second is of type |List alpha| and is recursive.

\subsubsection{Algebraic Abstractions of Primitive Types}
\label{sec:abstraction}

The Core language only has algebraic data types. \catch{} allows for primitive types such as characters and numbers by abstracting them into algebraic data types.

\begin{comment}
Natural numbers are often encoded by Peano numerals, and this idea can easily be extended to integers:

\begin{code}
data Pos  = One | Succ Pos
data Int  = Minus Pos | Zero | Plus Pos
\end{code}

Although this abstraction of \C{Int} captures all the underlying detail of the number system, the underlying constraint systems discussed in \S\ref{sec:constraint} would be unable to distinguishing between any pair of numbers both greater than 2, or both less than -2.
\end{comment}

For example, the integer abstraction used in \catch{} is simply:

\begin{code}
data Int = Neg | Zero | One | Pos
\end{code}

In our experience, numbers are most often constrained to be a natural, or to be non-zero. Addition or subtraction of one is the most common operation. Though very simple, the abstraction models the common properties and operations quite well.

Possible abstractions of characters include:

\begin{code}
data Char = Char
data Char = Alpha | Digit | White | Other
data Char = 'A' .. 'Z' | 'a' .. 'z' | '0' .. '9' | Other
data Char = '\0' | '\1' | '\2' ..
\end{code}

\noindent The simplest abstraction is that all characters are the same. A refinement partitions characters based on the character testing functions provided in the \C{Char} module of Haskell. Refining the model further can give special status to characters that commonly occur, giving all uncommon characters the \C{Other} value. Finally, each character could be represented distinctly. \catch{} employs the first abstraction by default, but provides a flag to allow alternative abstractions to be selected. This choice is motivated by simplicity -- few additional programs are proven safe with a more refined character abstraction.

The final issue of abstraction relates to primitive functions in the \C{IO} monad, such as \C{getArgs} (which returns the command-line arguments), or \C{readFile} (which reads from the file-system). In most cases a IO function is modelled as returning \textit{any} value of the correct type, using a function primitive to the checker.


\subsection{Constraint Essentials and Notation}
\label{sec:constraints}

\begin{discuss}
An expression in our first-order core language evaluates to a (potentially infinite) data structure, possibly containing $\bot{}$ caused by either non-termination or pattern match error. For all parts of an expression that do not evaluate to $\bot{}$, a constraint states the possible forms of data value it may take. A constraint describes a set of data structures to which a value may belong.
\end{discuss}

If |x| is an expression and |c| is a constraint we write |x :< c| to assert that the value of expression |x| must be a member of the set defined by |c|. Since the Core language is typed, the constraint |c| will only refer to a set of well-typed terms. Free variables can be bound by function calls. It is therefore convenient to have a notation for constraints upon named arguments of named functions: we write |f, x :< c| to assert that argument |x| of |f| must be in the set described by |c|. These atomic constraints can be combined using logical operators $\wedge{}$ and $\vee{}$ to form propositional constraints.

Several underlying constraint models are possible. To keep the introduction of the algorithms simple we use basic pattern constraints which are described in \S\ref{sec:basic}. We then describe regular expression constraints in \S\ref{sec:regexp} -- a variant of the constraints used in \catch05. Finally we present multi-pattern constraints in \S\ref{sec:multipattern} -- used in \catch07 to enable scaling to much larger problems.

\subsection{Bounded Constraints}
\label{sec:bounded}

\begin{discuss}
We require that for any type, there are finitely many constraints. This property is used for termination proofs.

Mention that this lemma is required for termination.

This approach works provided no selectors have a type that is structurally greater than the defined type, and no higher-kinded type variables are used. If the presence of definitions involving these features, either a translation can be made to a permitted form, or \C{isRec} can conservatively return True.

More formally, there exists a function |bounded t| such that the number of non-recursive selectors in any path of any value of type |t| is less than this value.

\begin{code}
forall x,t,p `o`  x :: t && p `elem` paths x =>
                  length (filter (not . isRec) p) < bounded t
\end{code}

A path is a list of selectors, the \C{paths} function returns all paths in a given data structure -- a potentially infinite number.
\end{discuss}


\subsection{Basic Pattern (BP) Constraints}
\label{sec:basic}

\begin{figure}
\begin{code}
type Constraint = [Pattern]

data Pattern  =  Con CtorName [Pattern]
              |  Any
\end{code}
\caption{Basic pattern constraints.}
\label{fig:basic}
\end{figure}

\begin{figure}
\begin{code}
data Req a = a :< Constraint

notin :: CtorName -> Constraint
(|>) :: Selector -> Constraint -> Constraint
(<|) :: CtorName -> Prop (Req Int)
\end{code}
\caption{Constraint operations.}
\label{fig:constraint}
\end{figure}

For simplicity, our analysis framework will be introduced using basic pattern constraints (BP-constraints). In  reality more powerful constraints are used, see \S\ref{sec:regexp} and \S\ref{sec:multipattern}.

A BP-constraint directly corresponds to Haskell pattern matching. A constraint is a finite set of patterns represented as in Figure \ref{fig:basic}. A given data structure |d| satisfies a constraint |ps| if |d| matches at least one pattern in |ps|. True and false constraints can be represented as |[Any]| and |[]| respectively.

The BP-constraint language is limited in expressivity. For example it is impossible to state that all the elements of a boolean list are True. However, even with this limited constraint language the Risers example can be proven safe.

Some operations must be provided in every constraint implementation. Signatures are given in Figure \ref{fig:constraint}. \C{Req} is the type of the |(:<)| condition, introduced in \ref{sec:constraints}. The lifting and splitting operators |(||>)| and |(<||)| are discussed in \S\ref{sec:backward}. The \C{notin} function generates a constraint which does \textit{not} match the constructor given. For example, for the BP-constraint language:

\begin{code}
notin c = map f (delete c (ctors c))
   where f x = Pattern x (replicate (arity c) Any)
\end{code}

The BP-constraint language does \textit{not} meet our requirement that any type has only a finite number of constraints. Consider the type |[Bool]|: an infinite number of type-correct patterns are possible, following the pattern |Any|, |Pattern ":" [Any, Any]|, |Pattern ":" [Any, Pattern ":" [Any, Any]]|. This issue could be resolved by limiting the depth of the patterns, for the presentation of the algorithms we do not do this. If these constraints were to be used in the real tool, depth limiting would be necessary.

\subsection{Safety Preconditions}
\label{sec:precond}

\begin{figure}
\begin{code}
pre :: Expr -> Prop (Req Expr)
pre (Var   x         ) = True
pre (Make  _   xs    ) = and (map pre xs)
pre (Call  fn  xs    ) = preFunc fn xs && and (map pre xs)
pre (Case  on  alts  ) = pre on && and [f c e | Alt c vs e <- alts]
    where f c e = (on :< notin c) || pre e

preFunc :: FuncName -> [Expr] -> Prop (Req Expr)
\end{code}
\caption{Precondition of an expression, \C{pre}.}
\label{fig:precondition}
\end{figure}

Our intention is that for every function, constraints on the arguments form a precondition for pattern safety. Given a function $f$ and a precondition $p$, $p$ is a valid condition if when satisfied, all function applications made by $f$ have their preconditions respected. The precondition for \C{error} is False. A program is safe if the precondition on \C{main} is True. The process of analysis can be seen as deriving these preconditions.

Given a function \C{preFunc} that returns the precondition on a function's arguments, a function to determine the precondition for safe evaluation of an expression can be specified as \C{pre} in Figure \ref{fig:precondition}.

\subsubsection{Stable Preconditions}

\begin{discuss}
The preconditions given by |preFunc| should have the following property:

\begin{code}
forall fn, xs `o` preFunc fn xs => pre (instantiate fn xs)
\end{code}

That is, the preconditions assigned to each function by \C{preFunc} must imply the preconditions calculated on the instantiation of that function with its arguments. One conservative implementation is that all calls to \C{preFunc} return False. A perfect precondition would be such that |preFunc f xs == pre (instantiate f xs)|.
\end{discuss}

\begin{figure}
\begin{code}
for fn `elem` funcs do conds(fn) <- (fn /= "error")
loop
    for fn `elem` funcs do
        conds'(fn) <- conds(fn) && pre (instantiate fn vs)
    if conds' == conds then break
    conds <- conds'
end loop
    where
        preFunc fn xs = cond(fn)[vs1/xs1 .. vs_n/xs_n]
        vs = ... -- free variables
\end{code}
\caption{Precondition calculation.}
\label{fig:precond_fixp}
\end{figure}

The algorithm for calculating preconditions is given in Figure \ref{fig:precond_fixp}. Initially all preconditions are assumed to be True, apart from \C{error}, which is False. New preconditions are calculated for every function, until the conditions do not change. In each iteration the preconditions become more restrictive, so given a constraint model with a finite number of terms, termination is guaranteed. A more efficient algorithm can be derived by tracking dependencies between preconditions, and only performing the minimum amount of recalculation. Finding strongly connected components in the static call graph of a program would also allow parts of the program to be checked separately.

\subsubsection{Preconditions and Laziness}

The \C{pre} function defined in Figures \ref{fig:precondition} does not respect laziness. The \C{Call} equation demands that preconditions hold on \textit{all} arguments -- only correct if a function is strict in all arguments. For example, the precondition on |False {-"\hbox{\textsf{\&\&} }"-} error "here"| is False, when it should be True. In general, preconditions may be more restrictive than necessary. However, investigation of a range of examples suggests that inlining |({-"\hbox{\textsf{\&\&}}"-})| and |({-"||||"-})| captures many of the common cases where laziness would be required.


\subsection{Manipulating constraints}
\label{sec:backward}

\begin{figure}
\begin{code}
back :: Req Expr -> Prop (Req Expr)
back (Var   x         :< k) = on :< (c |> k)
    where Just (on, c) = var x
back (Make  c   xs    :< k) = replaceVars xs (c <| k)
back (Case  on  alts  :< k) = and [f c e | Alt c vs e <- alts]
    where f c e = (on :< notin c) || (e :< k)
back (Call  fn  xs    :< k) = replaceVars xs (property fn k)

backs :: Prop Req -> Prop (Req VarName)
property :: FuncName -> Constraint -> Prop (Req Int)

replaceVars :: [Expr] -> Prop (Req Int) -> Prop (Req Expr)
replaceVars xs p = mapProp (\(i:<k) -> (xs!!i) :< k) p
\end{code}
\caption{Specification of backward analysis, \C{back}}
\label{fig:backward}
\end{figure}

Constraints on arguments to a function can be considered as preconditions to that function. Constraints on other expressions can be rewritten using the \C{back} function, detailed in Figure \ref{fig:backward}. The \C{backs} function repeatedly applies \C{back} until all constraints are on arguments. The \C{replaceVars} function takes a propositional constraint over a set of index variables, |v1..v_n|, and replaces each variable with a corresponding expression from an indexed list.

\para{The \C{Var} rule} applies only to variables bound by case expressions. This rule moves the condition from the bound variable to the subject of the |case| expression which introduced it.

The | ||>| operator lifts a constraint on one part of a data structure to a constraint on the entire data structure. For BP-constraints | ||>| can be defined:

\begin{code}
(c,i) |> k = map extend k
    where
    extend x = Pattern c (anys i ++ [x] ++ anys (n-i))
    anys j = replicate j Any
    n = arity c
\end{code}

\para{The \C{Make} rule} deals with an application of a constructor. The |<||| operator splits a constraint on an entire structure into a combination of constraints on each part.

\begin{code}
c <| k = propOr (map f k)
    where
    f Any = Any
    f (Pattern c2 xs) = c2 == c &&
        and (zipWith (:<) [0..] (map (:[]) xs))
\end{code}

\begin{discuss}
\para{The \C{Case} rule} generates a conjunct for each alternative. The generated condition says either the subject of the case analysis has a different constructor (so this particular alternative is not executed in this circumstance), or the right hand side of the alternative is safe given the conditions for this expression.
\end{discuss}

\para{The \C{Call} rule} relies on the key property:

\begin{code}
forall fn, xs, k `o`  backs (Call fn xs :< k) =>
                      backs (instantiate fn xs :< k)
\end{code}

That is, the result is at least as restrictive as if the function was inlined at this point. Two instantiation strategies can be used:

\begin{enumerate}
\item Use $\beta$ substitution (\C{instantiate}) replacing the call with the body of |fn|.
\item Create fresh variables for each element in |xs|, solve this new problem, and then instantiate the results.
\end{enumerate}

\catch05 used method 1. \catch07 uses method 2, as it is more amenable to finding a fixed point, and allows a cache of results to be built from which future questions can be answered.

\begin{figure}
\begin{code}
ps <- [(fn,k) | fn <- funcs, k <- constraints]
for (fn,k) `elem` ps do prop(fn,k) := True
loop
    for (fn,k) `elem` p do
        prop'(fn,k) := prop(fn,k) && backs (instantiate fn vs :< k)
    if prop' == prop then break
    prop <- prop'
end loop
    where
        property fn k = prop(fn,k)
\end{code}
\caption{Fixed point calculation for \C{property}.}
\label{fig:property_fixp}
\end{figure}

Answers can be calculated by obtaining a fixed point as detailed in Figure \ref{fig:property_fixp}. The function \C{property} generates fresh variables for each of the arguments, and solves this new problem. As in the precondition calculation in \S\ref{sec:precond}, each result is anded with its existing value. As there are finitely many constraint/function pairs termination is guaranteed. Here again, a speed up can be obtained by tracking the dependencies between constraints.

\subsection{Semantics of Constraints}

A key function in the semantics of constraints tests whether a value satisfies a constraint. The |(||>)| operator already provides all the necessary information:

\begin{code}
data Value = Value CtorName [Value]

satisfies :: Value -> Constraint -> Bool
satisfies (Value c xs) = mapProp f (c <| k)
    where f (i :< k2) = satisfies (xs !! i) k2
\end{code}

Now we have the \C{satisfies} function, we can expression properties that the other constraint operations must have:

\begin{code}
forall v, k, i  `o` satisfies v ((c,i) |> k)
                => (c2 /= c || satisfies (x !! i) k)
    where Value c2 xs = v

forall v, c `o` satisfies v (notin c) => (valueCtor v /= c)
    where valueCtor (Value c2 xs) = c2
\end{code}

\noindent Note that both properties are implications, not equivalences, as constraints may be more restrictive than necessary.

\section{Richer Constraint Systems}
\label{sec:constraint}

There are various ways of defining a richer constraint system. Here we outline two -- one adapted from \catch05, the other used in \catch07. Neither is strictly more powerful than the other; each is capable of expressing constraints that the other cannot express.

\subsection{Regular Expression (RE) Constraints}
\label{sec:regexp}

\begin{figure}
\begin{code}
data Constraint = RegExp :- [CtorName]

type RegExp = [Atom]

data Atom  =  Atom  Selector
           |  Star  [Selector]

notin :: CtorName -> Constraint
notin c = [] :- delete c (ctors c)

(|>) :: Selector -> Constraint -> Constraint
p |> (r :- cs) = integrate p r :- cs

(<|) :: CtorName -> Constraint -> Prop (Req Int)
c <| (r :- cs) = (ewp r => c `elem` cs) &&
    and (zipWith f (paths c) [0..])
    where
    f p x = case  differentiate p r of
                  Nothing -> True
                  Just r2 -> x :< (r2 :- cs)

ewp :: RegExp -> Bool
ewp x = all isStar x
   where  isStar (Star  _) = True
          isStar (Atom  _) = False

integrate :: Selector -> RegExp -> RegExp
integrate p r | not (isRec p) = Atom p : r
integrate p (Star ps:r) = Star (nub (p:ps) : r)
integrate p r = Star [p] : r

differentiate :: Selector -> Path -> Maybe Path
differentiate p [] = Nothing
differentiate p (Atom  r:rs)  | p == r     = Just rs
                              | otherwise  = Nothing
differentiate p (Star  r:rs)  | p `elem` r  = Just (Star r:rs)
                              | otherwise   = differentiate p rs
\end{code}
\caption{RE-constraints.}
\label{fig:regexp}
\end{figure}

\catch05 used regular expressions in constraints. A data type for regular expression based constraints (RE-constraints), along with the essential operations upon it is given in Figure \ref{fig:regexp}. In a constraint of the form |(r :- cs)|, |r| is a regular expression, and |cs| is a set of constructors. Such a constraint is satisfied by a data structure |d| if every well-defined application to |d| of a sequence of selectors described by |r| reaches a constructor in the set |cs|.

That is:

\begin{code}
d :< (r :- c) <=> (forall l `elem` lang r `o` defined d l => ctor d l `elem` c)
\end{code}

Here |lang r| is the language represented by the regular expression |r|; |defined d l| returns true if the sequence of selectors |l| is well-defined on |d|; and |ctor d l| follows the selectors |l| from |d|, then returns the constructor used to construct this data. If no sequence of selectors has a well-defined result then the constraint is vacuously true.

Concerning the helper functions needed to define | ||>| and |<||| in Figure \ref{fig:regexp}, the differentiation operation is defined by \citet{conway:regexp}, and called quotient in some text books; \C{ewp} is the empty word property; integration is the inverse of differentiation.

\catch05 regular expressions were unrestricted. Unfortunately regular expressions quickly grew to an unmanageable size, thwarting analysis of larger programs. The original solution to this problem was to bound the number of successive selectors to some depth, typically 3, and use kleene star for anything larger.

In general, a regular expression takes one of six forms:\\ \\
\begin{tabular}{ll}
|s+t|  & union of regular expressions |s| and |t| \\
|s^.t| & concatenation of regular expressions |s| then |t| \\
|s^*|  & any number (possibly zero) occurrences of |s| \\
\C{x}  & a selector, i.e. |(:,0)| for the head of a list \\
       & We use \C{hd} to abbreviate |(:,0)| and \C{tl} to abbreviate |(:,1)|. \\
0      & the language is the empty set \\
1      & the language is the set containing the empty string
\end{tabular} \\

\catch07 restricts REs to concatenations of atoms, or stars of unions:

\[\begin{array}{lllll}
r & = & 1     & || & |a^.r| \\
a & = & \C{x} & || & |u^*| \\
u & = & \C{x} & || & \C{x} + u
\end{array}\]

In addition, all selectors for which \C{isRec} returns true must be under \C{Star}, all which are not must be \C{Atom}, and \C{Star}s cannot be concatenated. These restrictions are motivated by three observations:

\begin{itemize}
\item \begin{discuss} The constructors, because of static typing and the restricted form of regular expression, must all be of the same type. In \catch05 expressions such as |hd^*| could arise, which is no longer type safe.\end{discuss}

\item There are finitely many regular expressions for any type. Combined with the finite number of constructors, this property is enough to guarantee termination when computing a fixed-point iteration on constraints.

\item The restricted REs with 0 are closed under integration and differentiation. The 0 alternative is catered for by the \C{Maybe} return type in the differentiation. As |0 :- c| always evaluates to True, |(<||)| replaces \C{Nothing} by True.
\end{itemize}

\begin{example}
|(head xs)| is safe if |xs| evaluates to a non-empty list. The RE-constraint is:

\smallskip\noindent |xs :< (1 :- {:})|
\end{example}

\begin{example}
|(map head xs)| is safe if |xs| evaluates to a list of non-empty lists. The RE-constraint is:

\begin{code}
xs :< (tl^* ^. hd :- {:})
\end{code}

If the list |xs| is empty, it still satisfies the constraint. If |xs| is infinite then the condition applies to all elements, constraining an infinite number. This constraint was not possible with the simple constraint framework introduced earlier.
\end{example}

\begin{example}
|(map head (reverse xs))| is safe if either every item in the list |xs| is |(:)|-constructed, or if |xs| is infinite -- so \C{reverse} does not terminate.

\smallskip\noindent |xs :< (tl^* ^. hd :- {:}) || x :< (tl^* :- {:})|
\end{example}

\subsubsection{Finite Number of RE-Constraints}
\label{sec:finite_re}

We require that for any type, there are finitely many constraints (see \S\ref{sec:bounded}). We can model types as:

\begin{code}
type Type  = [Ctor]
type Ctor  = [Maybe Type]
\end{code}

Under this modelling, each \C{Type} consists of a number of constructors. For each constructor \C{Ctor}, every component has either a recursive type (represented as \C{Nothing}) or a non-recursive type |t| (represented as |Just t|). Each non-recursive type is structurally smaller than the original, so if a function recurses on the type, it will terminate.

We can now define a function \C{count} which takes a type and returns the number of possible RE-constraints.

\begin{code}
count :: Type -> Integer
count t = 2 ^ rec * (2 ^ ctor + sum (map count nonrec))
    where
    rec = length (filter isNothing (concat t))
    nonrec = [x | Just x <- concat t]
    ctor = length t
\end{code}

The |2^rec| term corresponds to the number of possible constraints under \C{Star}. The |2^ctor| term accounts for the case where the path is empty, and a constructor of this type is given.


\subsection{RE-Constraint Propositions}
\label{sec:re-propositions}

\catch{} computes over propositional formulae with constraints as atomic propositions. These propositions are compared for equality to obtain a fixed point, and are variously manipulated by the system. Since the complexity of performing an operation is often proportional to the number of atomic constraints in a proposition, we apply simplification rules to reduce this number. There are nineteen rules implemented in the current checker. Three particularly useful ones are:

\para{Exhaustion:} In the constraint |x :< (r :- {:,[]})| the condition lists all the possible constructors. Because of static typing |x| must be one of these constructors, so this constraint simplifies to True.

\para{And merging:} The conjunction |e :< (r :- c1) && e :< (r :- c2)| can be replaced by |e :< (r :- (c1 `union` c2))|.

\para{Or merging:} The disjunction |e :< (r1 :- c1) |||| e :< (r2 :- c2)| can be replaced by |e :< (r2 :- c2)| if |r1 `subseteq` r2 && c1 `subseteq` r2|.

All the fixed-point algorithms given in this paper stop once equal constraints are found. We use Binary Decision Diagrams (BDD) \citep{lee:bdd} to make these equality tests fast.

\subsection{Multipattern (MP) Constraints}
\label{sec:multipattern}

\begin{figure}
\begin{code}
type Constraint = [Val]

data Val      =  [Pattern] :* [Pattern]
              |  Any
data Pattern  =  Pattern CtorName [Val]

-- useful auxiliaries, non recursive selectors
nonRecs :: CtorName -> [Int]
nonRecs c = [i | i <- [0..arity c-1], not (isRec (c,i))]

-- a complete Pattern on |c|
complete :: CtorName -> Pattern
complete c = Pattern c (map (const Any) (nonRecs c))

notin :: CtorName -> Constraint
notin c = [map complete (delete c cs) :* map complete cs]
    where cs = ctors c

(|>) :: Selector -> Constraint -> Constraint
(c,i) |> k = notin c ++ map f k
    where
    f Any = Any
    f (ms1 :* ms2) | isRec (c,i) = [complete c] :* merge ms1 ms2
    f v =  [Pattern c [if i == j then v else Any | j <- nonRecs c]]
           :* map complete (ctors c)

(<|) :: CtorName -> Constraint -> Prop (Req Int)
c <| vs = or (map f vs)
    where
    (rec,non) = partition (isRec . (,) c) [0..arity c-1]

    f Any = propTrue
    f (ms1 :* ms2) = or [g vs1 | Pattern c1 vs1 <- ms1, c1 == c]
        where g vs =  and (zipWith (:<) non (map (:[]) vs)) &&
                      and (map (:< [ms2 :* ms2]) rec)

(`mergeVal`) :: Val -> Val -> Val
(a1 :* b1) `mergeVal` (a2 :* b2)  = merge a1 a2 :* merge b1 b2
Any        `mergeVal` x           = x
x          `mergeVal` Any         = x

merge :: [Pattern] -> [Pattern] -> [Pattern]
merge  ms1 ms2 = [Pattern c1 (zipWith (`mergeVal`) vs1 vs2) |
       Pattern c1 vs1 <- ms1, Pattern c2 vs2 <- ms2, c1 == c2]
\end{code}
\caption{MP-constraints.}
\label{fig:enumeration}
\end{figure}

Although RE-constraints are capable of solving many problems, they suffer from a problem of scale. As the system becomes more complex the size of the propositions grows quickly, slowing down the system to an unacceptable level. Multipattern constraints (MP-constraints) are an alternative which scales better.

MP-constraints are defined in Figure \ref{fig:enumeration}. They are similar to BP-constraints, but can constrain an infinite number of items. A value |v| satisfies a constraint |p1 :* p2| if |v| itself satisfies |p1| and all its recursive components satisfy |p2|. Each of |p1| and |p2| is given as a set of matches -- much like the simple pattern language in \S\ref{sec:basic}. The difference is that each \C{Pattern} only specifies the values for the non-recursive selectors, all recursive selectors are handled by |p2|.

\begin{revisit}{1} safe evaluation of |(head xs)| requires |xs| to be a non-empty list. The MP-constraint on |xs| is:

\begin{code}
{(:) Any} :* {[], (:) Any}
\end{code}

This constraint is longer than the corresponding RE-constraint because it is explicitly stated that the \C{hd} field is unrestricted.
\end{revisit}

\begin{revisit}{2} safe evaluation of |(map head xs)| requires |xs| to be a list of non-empty lists. The MP-constraint on |xs| is

\smallskip
\par\noindent |{[], (:) ({(:) Any} :* {[], (:) Any})}|
\par\noindent |:*|
\par\noindent |{[], (:) ({(:) Any} :* {[], (:) Any})}|
\end{revisit}

\begin{revisit}{3} |(map head (reverse x))| requires |xs| to be a list of non-empty lists \textit{or} infinite. The MP-constraint for an infinite list is:

\smallskip\noindent |{(:) Any} :* {(:) Any}|
\end{revisit}

As we saw in \S\ref{sec:re-propositions}, RE-constraints have simplification rules.  MP-constraints have a \textit{normalisation procedure}. Normalisation takes a single MP-constraint and returns one which is equivalent, but preferably simpler. There are eight rules in the normalisation procedure. Two examples:

\para{|[Val]| normalisation:} Given a list of |Val|, if the value |Any| is in this list, the list is equal to |[Any]|. If a value occurs more than once in the list, one copy can be removed.

\para{|Val| normalisation:} In a constraint |p1 :* p2|, if both |p1| and |p2| cover all constructors and all their selectors have |Any| as their constraint, this constraint can be replaced with |Any|.


\subsubsection{Finitely Many MP-Constraints per Type}

As in \S\ref{sec:finite_re}, we define \C{count} to determine the number of different constraints for a type, showing that there are finitely many constraints.

\begin{code}
count :: Type -> Integer
count t = 2 ^ val t

val t = 1 + 2 * 2 ^ (pattern t)

pattern t = sum (map f t)
    where f c = product [count t2 | Just t2 <- c]
\end{code}

The \C{val} function counts the number of possible \C{Val} constructions. The \C{pattern} function performs a similar role for \C{Pattern} constructions.


\subsection{MP-Constraint Propositions and Uncurrying}

A big advantage of MP-constraints is that if two constraints on the same expression are combined at the proposition level, they can be reduced into one constraint:

\begin{code}
(e :< v1) || (e :< v2) = e :< (v1 ++ v2)
(e :< v1) && (e :< v2) = e :< [a `mergeVal` b | a <- v1, b <- v2]
\end{code}

\noindent This ability to combine constraints on equal expressions can be exploited further by translating the program to be analysed. After applying \C{backs}, all constraints will be in terms of the arguments to a function. So if all functions took exactly one argument then \textit{all} the constraints associated with a function body could be collapsed into one. We therefore \textit{uncurry} all functions.

\begin{example}
\begin{code}
(||) x y = case  x of
                 True   -> True
                 False  -> y
\end{code}

\noindent can be translated into:

\begin{code}
(||) a = case  a of
               (x,y) -> case  x of
                              True    -> True
                              False   -> y
\end{code}
\end{example}

Combining MP-constraint reduction rules with the uncurrying transformation, a \C{Prop (Req VarName)} is now equivalent in power to \C{Req VarName}. This simplification reduces the number of different propositional constraints, making fixed-point computations faster.

In the RE-constraint system uncurrying would do no harm, but it would be of no use. None of the RE simplification rules is able to reduce distinct components in a tuple.

\subsection{Comparison of RE and MP Constraints}

Both RE-constraints and MP-constraints are capable of expressing a wide range of value-sets, but neither subsumes the other. We give examples where one constraint language can differentiate between a pair of values, and the other cannot.

\begin{example}
Consider the values |v1 = (T:[])| and |v2 = (T:T:[])| and the MP-constraint |{(:) Any} :* {[]}|. This constraint is satisfied by |v1| but not by |v2|. No proposition over RE-constraints can separate these two values.
\end{example}

\begin{example}
Consider a data type |Tree alpha|:

\begin{code}
data Tree alpha  =  Branch {left :: Tree alpha, right :: Tree alpha}
                 |  Leaf alpha
\end{code}

\noindent and two values of the type |Tree Bool|

\begin{code}
v1 = Branch (Leaf True   ) (Leaf False  )
v2 = Branch (Leaf False  ) (Leaf True   )
\end{code}

\noindent The RE-constraint |(left^* :- True)| is satisfied by |v1| but not |v2|. No MP-constraint separates the two values.
\end{example}

\begin{discuss}
\catch07 uses MP-constraints, as they scale to much larger examples.
\end{discuss}

\section{Converting Haskell to Core}
\label{sec:transform}

The full Haskell language is unwieldy for analysis. As noted in \S\ref{sec:core}, the checker works instead on a simplified language, a core to which other Haskell programs can be reduced.

\subsection{Yhc Core}

\begin{comment}
-- a simple variant of lambda calculus without types, but with source position information. Yhc works by applying basic desugaring transformations, without optimisation. This simplicity ensures the generated PosLambda is close to the original Haskell in its structure. Each top-level function in a source file maps to a top-level function in the generated PosLambda, retaining the same name.

However, PosLambda has constructs that have no direct representation in Haskell. For example, there is a FatBar construct \cite{spj:implementation}, used for compiling pattern matches which require fall through behaviour. The PosLambda language
\end{comment}

To generate core representations of programs, it is natural to start with a full Haskell compiler, and we chose Yhc \citep{Yhc}, a fork of nhc \citep{nhc}. The internal language of Yhc is called PosLambda. It was intended only as an internal representation, and exposes certain details that are specific to the compiler. We have therefore introduced a new Core language to Yhc to which PosLambda can easily be translated. This Core language is not explicitly typed, and has few constructs:

\begin{itemize}
\item No syntactic sugar such as list comprehensions, |do| notation etc.
\item Only simple |case| expressions, matching only the top level constructor
\item All |case| expressions are complete, including an \C{error} call if necessary
\item All names are fully qualified
\item Haskell's type classes have been removed (see \S\ref{sec:dict})
\item Only top-level functions remain, all local functions have been lambda lifted
\item All constructor applications are fully saturated
\end{itemize}

\subsection{The Dictionary Transformation}
\label{sec:dict}

Most of the transformations in Yhc operate at a local level, within a single function definition. The only phases which require information about more than one function are type checking and the dictionary transformation, used to implement type classes \citep{wadler:type_classes}.

The dictionary transformation generates a tuple, containing the functions to use for the type class, and passes this structure into all functions requiring this type class. The additional argument is the dictionary, with functions being looked up in the dictionary to obtain the correct behaviour at runtime. Haskell also allows subclassing. For example, \C{Ord} requires \C{Eq} for the same type. In such cases the dictionary transformation generates a nested tuple, the \C{Eq} dictionary is a component of the \C{Ord} dictionary.

\begin{example}
\begin{code}
f :: Eq alpha => alpha -> alpha -> Bool
f x y = x == y || x /= y
\end{code}

\noindent is translated by Yhc into

\begin{code}
f :: (alpha -> alpha -> Bool, alpha -> alpha -> Bool) -> alpha -> alpha -> Bool
f dict x y = (||) (((==) dict) x y) (((/=) dict) x y)

(==) (a,b) = a
(/=) (a,b) = b
\end{code}

The \C{Eq} class is implemented as two selector functions, |(==)| and |(/=)|, acting on a method table. For different types of |alpha|, different method tables are provided.
\end{example}

The dictionary transformation is a global transformation. In Example \lastexample{} the \C{Eq} context in \C{f} not only requires a dictionary to be accepted by \C{f}; it requires all the callers of \C{f} to pass a dictionary as first argument.

An alternative approach to implementing type classes is given in \cite{jones:dictionary_free}, which does not rely on higher order functions. Although this approach might suit \catch{} better, it is not the method implemented in Yhc, which already does the dictionary transformation.

\subsection{First-Order Haskell}

The analysis presented in \S\ref{sec:manipulate} operates on a first-order language. In order to analyse full Haskell, we transform Haskell to a first-order language. We briefly consider three alternative methods.

\subsubsection{Reynold's style defunctionalization}

Reynold's style defunctionalization \citep{reynolds:defunc} is the seminal method for generating a first-order equivalent of a higher-order program.

\begin{example}
\begin{code}
map fn x = case  x of
                 []      -> []
                 (a:as)  -> fn a : map fn as
\end{code}

Defunctionalization works by creating a data type to represent all values that |fn| may take anywhere in the whole program. For instance, it might be:

\begin{code}
data Functions = Head | Tail

apply Head  x = head  x
apply Tail  x = tail  x

map fn x = case  x of
                 []      -> []
                 (a:as)  -> apply fn a : map fn as
\end{code}

Now all calls to |map head| are replaced by |map Head|.
\end{example}

\begin{example}
The scheme easily extends to currying. Given a function \C{add} of arity 2, we can partially apply either zero or one arguments. Defunctionalization is still suitable:

\begin{code}
data Functions = Head | Tail | Add0 | Add1 Int

apply Add0      n  = Add1 n
apply (Add1 a)  b  = add a b
\end{code}
\end{example}

Defunctionalized code is still type safe, but type checking would require a dependently typed language. This presents no problem for \catch{}, which does not use type information. The unacceptable aspect is the creation of an \C{apply} function, whose meaning is excessively general. The effect of this kind of transformation is to introduce a bottleneck through which various properties must be proven. Asking questions such as \textit{``Is the result of \C{apply} an empty-list?''}, requires a lot of computation.

\catch{} only uses Reynold's style defunctionalization if all other methods fail.


\subsubsection{Specialisation}

\catch05 used a different technique to remove higher-order functions: specialisation. A mutually recursive group of functions can be specialised in their $n$th argument if in all recursive calls this argument is invariant. Examples of common functions whose applications can be specialised include \C{map}, \C{filter}, \C{foldr} and \C{foldl}. When a function can be specialised, the expression passed as the $n$th argument has all its free variables passed as extra arguments, and is expanded in the specialised version. All recursive calls within the new function are then renamed.

\begin{example}
\begin{code}
map fn xs = case  xs of
                  []      -> []
                  (a:as)  -> fn a : map fn as

adds x n = map (add n) x
\end{code}

\noindent is transformed into:

\begin{code}
map_adds n xs = case  xs of
                      []      -> []
                      (a:as)  -> add n a : map_adds n as

adds x n = map_adds n x
\end{code}
\end{example}

Specialisation is sufficient for many examples, but it cannot cope with point-free code, and does not deal with many forms of dictionaries.

\subsubsection{Specialisation with Inlining}

The power of specialisation is greatly increased if it is combined with inlining, and applied selectively to higher-order functions. Although this technique is not complete, we have yet to encounter an example where it fails, apart from artificial tests which we created specifically to break the approach!

\begin{figure}
\begin{code}
data Expr  =  ... -- as in Figure {\ref{fig:core}}
           |  Part   Int FuncName [Expr]
           |  Apply  Expr [Expr]

Part 0 fn xs == Call fn xs
Apply (Part n fn xs) ys == Part (n - length ys) fn (xs ++ ys)
\end{code}
\caption{Augmented Core syntax.}
\label{fig:core_ho}
\end{figure}

In order to permit a higher-order program to be represented, the Core language is augmented with additional constructs, as shown in Figure \ref{fig:core_ho}. The \C{Apply} constructor represents an unsaturated function call, or a variable to be used as the function. The \C{Part} constructor is used to represent unsaturated function calls, leaving the normal \C{Call} constructor to represent saturated calls. A \C{Part} construction records how many arguments are needed.

\begin{figure}
\begin{code}
isHO :: Expr -> Bool
isHO (Part n _ _)    = n > 0
isHO (Make _ xs)     = any isHO xs
isHO (Case on alts)  = any (isHO . snd) alts
isHO _               = False
\end{code}
\caption{Tests for the firstifier.}
\label{fig:isHO}
\end{figure}

The algorithm for removing higher-order functions has two components, specialisation and inlining. We apply the specialise rule until a fixed point is reached, then apply the inline rule once. We repeat these two steps until a fixed point is reached. Given an appropriate \C{fix} function, \C{firstify} can be implemented as:

\begin{code}
firstify :: Program -> Program
firstify = fix (inline . fix specialise)
\end{code}

\para{The \C{inline}} stage is simple. Each function call is inlined if the body of the called function passes the \C{isHO} test, defined in Figure \ref{fig:isHO}. If this process causes a function to no longer be called from the root of the program, then the function is removed after inlining.

\para{The \C{specialise}} stage takes every expression of the form |Call fn xs| where |any isHO xs|, and generates a specialised version of the function |fn| with all functional arguments in |xs| frozen in, and all others passed normally. Assuming that a function |fn| results in the specialised version |fn2|, then the translation would be:

\begin{code}
transform (Call fn xs) | any isHO xs =
    Call fn2 (concatMap f xs)
    where
    fn2 = generate fn xs

    f x = if isHO x then freeVars x else [x]
\end{code}

The combination of specialisation and inlining is power, and removes most functional arguments. Either the entire body is an application of the functional argument (in which case it will be inlined), or it must occur as the argument to a function (in which case it is specialised). There are only two places left for functional arguments to be used:

\begin{enumerate}
\item As the subject of a |case| expression. But this situation is impossible as all |case| expressions must choose over a data value.

\item Inside an \C{Apply} with another functional argument variable as the function. This situation is very rare in practice.
\end{enumerate}

If functions are placed in a recursive data structure, and an accumulating function operates over them a program may have an infinite number of specialisations. For example:

\begin{code}
data Wrap alpha = Wrap (Wrap alpha) | Value alpha

main = f head

f x = f (Wrap x)
\end{code}

In reality, functions are rarely placed in any data structure other than a tuple. But if necessary we revert to Reynold's style defunctionalization.

\section{Results and Evaluation}
\label{sec:results}

The best way to see the power of Catch is by example. \S\ref{sec:safety} discusses some of the general issues when modifying programs to obtain provable safety. \S\ref{sec:imaginary} investigates all the examples from the Imaginary section of Nofib, \S\ref{sec:hscolour} investigates one larger real-world program (HsColour), and \S\ref{sec:filepath} investigates a library (System.FilePath).

\subsection{The Price of Safety}
\label{sec:safety}

Take the following example:

\begin{code}
average xs = sum xs / length xs
\end{code}

If |xs| is |[]| then a division by zero occurs. In \catch{}, division by zero is modelled as a pattern-match error. One small local change could be made which would remove this pattern match error:

\begin{code}
average xs = if null xs then 0 else sum xs / length xs
\end{code}

In this modified version, if |x| is |[]|, then the program will simply return 0, and no pattern match error will occur. While this program is now safe, this safety has come at a cost -- the code no longer behaves as it did before -- replacing $\bot{}$ with the non-sensical value 0.

In general pattern-match errors can be avoided in two ways:

\begin{description}
\item [Widden the domain of definition:] The above example shows a local fix: in the same function that calls the division, add a special test. This fix is fairly simple, and requires only small code changes in the definition.

\item [Narrow the domain of application:] Investigate the code that calls \C{average}, and ensuring that |[]| is not passed as the first argument. This fix requires a deeper understanding of the flow of the program, and depending on the overall design, may require substantial work at each application. The end result is likely to be a cleaner body of code.
\end{description}

For programs which were not design with pattern match safety in mind (see Gen-Regexps in \S\ref{sec:imaginary}), the global changes may constitute an entire reimplementation. In the following sections, where modifications were required, we have attempted to make the minimum number of changes. Consequently, we have widened the domain of application.

\subsection{The Imaginary Nofib Benchmark}
\label{sec:imaginary}

The whole Nofib benchmark \citep{nofib} is too large to present results for, so instead the `Imaginary' section has been chosen as our primary focus. These programs are all under a page of text (excluding any Prelude or library definitions used), and particularly stress list operations and numeric computations. All the benchmark code is available online\footnote{\texttt{http://darcs.haskell.org/nofib/imaginary/}, as of March 2007}.

\begin{table}
\begin{tabular}{lrrrrrrlll}
Program               & Src & Core & First & \C{error} & Pre & sec & Mb \\
\vspace{-2.2ex} \\
\hline
\vspace{-1.8ex} \\
\textbf{Bernoulli}     & 35 & 1616 &  652 & 5 & 11 & 4.1 & 0.8 \\
\textbf{Digits of E1}  & 44 &  957 &  377 & 3 &  8 & 0.3 & 0.6 \\
Digits of E2           & 54 & 1179 &  455 & 5 & 19 & 0.5 & 0.8 \\
Exp3-8                 & 29 &  220 &  163 & 0 &  0 & 0.1 & 0.1 \\
\textbf{Gen-Regexps}   & 41 & 1006 &  776 & 1 &  1 & 0.3 & 0.4 \\
Integrate              & 39 & 2466 &  364 & 3 &  3 & 0.3 & 1.9 \\
\textbf{Paraffins}     & 91 & 2627 & 1153 & 2 &  2 & 0.8 & 1.9 \\
Primes                 & 16 &  302 &  241 & 6 & 13 & 0.2 & 0.1 \\
Queens                 & 16 &  648 &  283 & 0 &  0 & 0.2 & 0.2 \\
Rfib                   &  9 & 1918 &  100 & 0 &  0 & 0.1 & 1.7 \\
Tak                    & 12 &  209 &  155 & 0 &  0 & 0.1 & 0.1 \\
\textbf{Wheel Sieve 1} & 37 & 1221 &  570 & 7 & 10 & 7.5 & 0.9 \\
\textbf{Wheel Sieve 2} & 45 & 1397 &  636 & 2 &  2 & 0.3 & 0.6 \\
X2n1                   & 10 & 2637 &  331 & 2 &  5 & 1.8 & 1.9 \\
\end{tabular}

\begin{description}
\item[Src] is the number of lines in the original source code.
\item[Core] is the total number of lines of Yhc Core, including all functions used from libraries, when pretty printed.
\item[First] is the number of lines after firstification, just before analysis.
\item[\C{error}] is the number of calls to error.
\item[Pre] is the number of functions which have a precondition which is not True.
\item[Sec] is the time in seconds taken for transformations and analysis, excluding the standard Yhc compilation time. No great effort has been put into the optimisation of the analysis code.
\item[Mb] is the maximum residency of the program at garbage collection time, in Mb. Typically a program will consume twice this size in system memory.
\end{description}
\caption{Table of results}
\label{tab:results}
\end{table}

Results are given in Table \ref{tab:results}. Where changes were required, the name of the program is in bold, and will be discussed separately. \catch{} produces a log containing the entire program in Core form before analysis begins, including the effect of abstraction and firstification, a list of all the properties calculated and the preconditions on every function -- this information gives the other statistics.

To take a typical benchmark, Primes, the \C{main} function is:

\begin{code}
main = do  [arg] <- getArgs
           print $ primes !! (read arg)
\end{code} % $

The first unsafe pattern here is |[arg] <- getArgs|, as \C{getArgs} is a primitive which may return any value. Given this expression, \catch{} will return False as the precondition to \C{main}.

The next step that may fail is when \C{read} is applied to an argument extracted from getArgs. This argument is entirely unknown, and \C{read} is a sufficiently complicated function that although it can be modelled by Catch, there is no possibility for getting an appropriate abstraction which models the failure case. As a result, any program which calls \C{read} is unsafe, according to \catch{}! Using \C{reads}, which indicates failure properly, a program may still be checked  successfully.

As a result the programs have been rewritten, and \catch{} is directed to check from the \C{compute} function:

\begin{code}
compute x = print $ primes !! x
\end{code}

Four programs use the list-indexing operator |(!!)|, which requires the index to be non-negative and less than the length of the list. \catch{} can only prove this condition if the list being indexed on is infinite. Eight programs include applications of either \C{head} or \C{tail}, most of which can be proven safe. Seven programs have incomplete patterns, often in a |where| binding and \catch{} performs well on these. Nine programs use division, with the precondition that the divisor must not be zero. Most of these can be proven safe. Only four programs contain no calls to \C{error}.

Three programs have preconditions on the \C{main} function, all of which state that the argument must be a natural number. In all cases the generated precondition is a necessary one -- if the input violates the precondition then pattern-match failure will occur.

We now discuss the four programs which required changes, along with the Digits of E2 program -- a program with complex pattern matching that \catch{} is able to prove safe.

\paragraph{Bernoulli}

This program has one instance of |tail (tail x)|, which is beyond the capabilities of \catch{}. Consider the example program and generated condition:

\begin{code}
main x = tail (tail x)
> main, x :< {(:) Any} :* {(:) Any}
\end{code}

The MP-constraints are unable to express that a list must be of at least length two, so conservatively strengthens this to the condition that the list must be infinite -- a condition that Bernoulli does not satisfy.

Our remedy is to replace |tail (tail x)| with |drop 2 x|. Even with this change, the program still has a high level of non-exhaustive pattern matches. But all are proven safe.


\paragraph{Digits of E1}

This program contains the following equation:

\begin{code}
ratTrans (a,b,c,d) xs |
  ((signum c == signum d) || (abs c < abs d)) &&
  (c+d)*q <= a+b && (c+d)*q + (c+d) > a+b
     = q:ratTrans (c,d,a-q*c,b-q*d) xs
  where q = b `div` d
\end{code}

Catch is able to prove that the division by |d| is only unsafe if both |c| and |d| are zero, but it is not able to prove that this invariant is maintained. Using the domain widening tactic, a slight modification is possible:

\begin{code}
  where q = if d == 0 then 0 else b `div` d
\end{code}

\paragraph{Gen-Regexps}

This program expects valid regular expressions as input. Ways of crashing the program include entering |""|, |"["|, |"<"| and lots of other inputs.

One potential error comes from |head . lines|, which can be replaced by |takeWhile (/= '\n')| to have the same effect, apart from on the empty list, where it returns |""|.

One pattern match error is can be fixed by adding a default clause.

Two errors of the form |(a,_:b) = span f x| can be fixed by changing them to |(a,b) = safeSpan f x|, where \C{safeSpan} is defined as:

\begin{code}
safeSpan p x = (a, drop 1 b)
    where (a,b) = span p x
\end{code}

While at first glance this pattern looks like a simple pattern match against |(:)|, much in the same way as \C{head}, the reality is much more complex. The pattern is only safe if for one of the elements in the list |x|, |f| returns True. In the particular example of Gen-Regexps, the test |f| is actually |(/= '-')|. Using the abstraction for |Char| as detailed in \S\ref{sec:abstraction}, |(/=)| over |Char| always has an unknown result, making this pattern impossible to prove safe.

This example shows that a program written with no regard for pattern-match safety can be made safe using local fixes, without excessive code alterations.

\paragraph{Wheel Sieve 1}

The program Wheel Sieve 1 defines a data structure, and a function \C{sieve}:

\begin{code}
data Wheel = Wheel Int [Int]

sieve :: [Wheel] -> [Int] -> [Int] -> [Int]
\end{code}

The invariant on all these types is that all the lists are infinite, and all the Int's are strictly positive numbers. Unfortunately the program is written in a manner that makes these invariants too complex for \catch{} to infer them all, but \catch{} can manage some.

Two modifications are required to prove safety. A safe variant of \C{mod} is required which does not raise division by zero -- to allow for \catch{} failing to infer that all numbers are positive. A pattern in \C{notDivBy} has to be completed -- to allow for \catch{} failing to infer two particular lists are both infinite.

Even with these two modifications, \catch{} has to perform a lot of computation to check the other possible pattern matches. This significant amount of computation results in a runtime just over 10 seconds, more than double the second most expensive computation.


\paragraph{Wheel Sieve 2}

The program Wheel Sieve 2 is based on Wheel Sieve 1, with the same data and invariants, but has much greater complexity. In this particular case \catch{} is able to prove very few of the necessary invariants -- so we resort to completing the patterns.

By widening the domain of application to several functions -- replacing |tail| with |drop 1|, |head| with a version returning a default on the empty list, |mod| with a safe variant -- \catch{} is able to prove safety.


\paragraph{Paraffins}

The Paraffins program can only be validated by Catch after significant modification. There are two reasons:

\begin{description}
\item[Laziness] The first error, which can be fixed easily, is the use of \C{undefined} to prevent a space leak:

    \begin{code}
    radical_generator n = radicals undefined
      where radicals unused = big_memory_computation
    \end{code}

    In Haskell if a function is defined with no arguments it is considered a constant applicative form (CAF), and has its results computed only once in the program. To prevent something which allocates a lot of memory from having this behaviour, a dummy argument can be passed. The \C{undefined} is merely serving as a dummy argument, but if evaluated would result in a pattern match error. If the analysis was lazy (as discussed in \S\ref{sec:precond}) then this example would succeed using \catch{}. As it is, simply changing \C{undefined} to |()| removes the potential error.

\item[Arrays] The Paraffins program uses arrays. The function |array :: Ix a => (a, a) -> [(a, b)] -> Array a b| takes a list of index/value pairs and builds an array. The precondition on this function is that all indexes must be in the range specified and all arguments must be valid. This precondition is too complex for Catch. But simply using \C{listArray}, which takes a list of elements one after another, the program can be validated. (Also, the use of \C{listArray} makes the program shorter and more readable.)

    Use of the array indexing operator |(!)| is also troublesome. The precondition requires that the index is in the bounds given when the array was constructed, something \catch{} does not currently model.
\end{description}

The Paraffins program does highlight that one weak area of \catch{} is code that uses arrays.

\paragraph{Digits of E2}

This program is quite complex, featuring a number of possible pattern match errors. To illustrate, consider the following fragment:

\begin{code}
  where  carryguess = d `div` base
         remainder = d `mod` base
         nextcarry:fraction = carryPropagate (base+1) ds
\end{code}

Here there are three potential pattern match errors in as many lines. Two of these are the calls to |div| and |mod|, both requiring |base| to be non-zero. A possibly more subtle pattern match error is the |nextcarry:fraction| left-hand side of the third line. \catch{} is able to prove that no pattern match fails.

\begin{code}
e =  ("2."++) $
     tail . concat $
     map (show.head) $
     iterate (carryPropagate 2 . map (10*) . tail) $
     2:[1,1..]
\end{code}

There are two calls to \C{tail}, and one to \C{head}. They are mixed in a relatively complex manner, including as a higher order functional pipeline in the case of \C{iterate} and \C{map}. \catch{} is able to prove that no pattern match fails, and that \C{head} and \C{tail} have their preconditions respected.

\subsection{The HsColour Program}
\label{sec:hscolour}

The disadvantage of working with artificial benchmarks is that they are not designed to be total. When it comes to a real program, with real users, a crash is a much more serious issue -- and eliminating them a desirable goal. We have taken the HsColour program\footnote{http://www.cs.york.ac.uk/fp/darcs/hscolour/} and analysed it using \catch07. We have contributed patches back to the author of HsColour, with the result that the development version can be proved free from pattern match errors.

HsColour is written in Haskell, being both a library and a program -- for the purposes of the checker we have focused on the program. HsColour comprises of 12 modules, 823 lines, is 4 years old and has had patches from 6 different people.

Catch required 4 small patches to successfully check the entire program. Of these 4 patches, 3 were genuine pattern match errors which could be tripped by constructing unexpected input. The three issues were:

\begin{enumerate}
\item \C{read} was called on a preferences file from the user, this could crash by specifying a malformed preferences file.
\item By giving the document consisting of the a single double quote character \T{"}, and passing the ``-latex'' flag, a crash occurred.
\item By giving the document \T{(`)}, namely open bracket, backtick, close bracket, and passing ``-html -anchor'' a crash occurred.
\end{enumerate}

The one patch which did not (as far as we are able to ascertain) fix a real bug could still be considered an improvement in the code base, and was minor in nature (a single line).

The entire process requires 2.1 seconds to run, and uses 2.7Mb of memory.

\subsection{The System.FilePath library}
\label{sec:filepath}

The System.FilePath library\footnote{http://www-users.cs.york.ac.uk/$\sim$ndm/projects/libraries.php} is a relatively new library which has been proposed for inclusion into the Haskell base libraries. It comprises 810 lines, of which 352 are non-blank and non-comment lines, and exports 51 functions in its interface.

The initial challenge to checking a library is that there is no \C{main} function. \catch{} provides the |(||||||)| operator to remove this issue. A \C{main} function can be constructed as follows:

\begin{code}
main = function1 ||| function2 ||| function3
\end{code}

The |(||||||)| operator takes two functions, and using Haskell type classes saturates them with enough arguments, then checks that a crash does not result. This operator is provided in an additional library by \catch{}.

Only one change was required to the library:

\begin{code}
f x  | isPosix    = dropTrailSlash $ normalise x
     | isWindows  = dropTrailSlash $ map toLower $ normalise x
\end{code}

Here \catch{} is unable to prove that this function is safe. By changing \C{isWindows} to \C{otherwise}, this code can be proven safe. The function is safe, but relies on the CAF \C{System.Info.os} being a constant string through the run of the program -- something \catch{} does not take account of.

With this single modification \catch{} requires 1.6 seconds and 1.6Mb of memory to prove all library functions are free from pattern match errors.

\section{Related Work}
\label{sec:related}

This paper follows very much the same spirit as \cite{me:catch_tfp} -- the goals are the same and some of the approach has similarities. The comparison to the previous paper has been made throughout the this paper, but the two main improvements are support for full Haskell, and scalability.

\subsection{Proving Incomplete Patterns Safe}

Despite the seriousness of the problem of pattern matching, the topic of pattern-match error detection and elimination has not been studied extensively. The other paper that covers this issue most directly is \citep{esc_haskell}, where ESC/Haskell is introduced. The ESC/Haskell approach requires the programmer to give explicit preconditions and contracts which the program obeys. This approach is intended to result in a checker with more expressive power -- one of the examples involves an invariant on an ordered list, something beyond \catch{} -- but it requires the programmer to spend more time doing the validation. The preconditions and properties derived by \catch{} could be turned into ESC/Haskell annotations, allowing both tools to be used together.

The version of ESC/Haskell presented in the paper is only a prototype, there is currently no publicly available version. ESC/Haskell does not yet support full Haskell, lacks essential features such as type classes, and is has only examples of limited size -- the largest example given is 9 lines long, and none require additional library functions. We hope that a future version of ESC/Haskell will address these limitations, and allow a fuller comparison to \catch{} to be made.


\subsection{Eliminating Incomplete Patterns}

One simple way to guarantee that a program does not crash with an incomplete pattern is to ensure that all pattern matching is exhaustive. The GHC compiler \citep{ghc} has a warning flag to detect incomplete patterns, named \T{-fwarn-incomplete-patterns}. Adding this flag when compiling risers results in a message that ``Pattern matches are non-exhaustive''. Unfortunately the Bugs (12.2.1) section of the manual notes that the checks are sometimes wrong, particularly with string patterns or guards, and that this part of the compiler ``needs an overhaul really'' \citep{ghc_manual}. But the GHC checks are only local. Using an incomplete function from a library, such as \C{head}, gives no warning. If the function \C{head} is defined, then it raises a warning.

There have been attempts at defining what it means for a program to be complete with regard to pattern matches \citep{maranget:pattern_warnings}, along with associated implementations.

A more radical approach is to design a \textit{total} functional language without pattern match errors (or non-termination) \citep{turner:total}. In this approach incomplete pattern matches are banned, suggesting this restriction will ``force you to pay attention to exactly those corner cases which are likely to cause trouble''. The results in \S\ref{sec:imaginary} show that in sample Haskell programs there are a reasonably high number of incomplete pattern matches, which are still safe.

The \catch{} tool could perhaps allow the exhaustive pattern matching restriction to be lifted somewhat. The \catch{} tool will always report success if there are no inexhaustive patterns, so a more generous requirement would be for the \catch{} tool to succeed on the program.

\subsection{A Mistake Detector}

There has been a long history of writing tools to analyse programs to detect potential bugs, going back to the classic C Lint tool \citep{lint}. In the functional arena the Dialyzer tool \citep{dialyzer} for Erlang \citep{erlang} performs a similar function. The aim is to have a static checker that works on unmodified code, with no additional annotations. However, a key difference is that in Dialyzer all warnings indicate a genuine problem that needs to be fixed. Because Erlang is a dynamically typed language, a large proportion of Dialyzer's warnings relate to mistakes a type checker would have detected.

\subsection{Type System Safety}

One way of checking pattern matching is to define a slightly more precise type system. In this light the checker can be compared to the tree automata work done on XML and XSL \citep{static_xslt}, which can be seen as an algebraic data type and a functional language. Another soft typing system with similarities is by \citet{aiken:type_infer}, on the functional language FL. This system tries to assign a type to each function using a set of constructors, for example \T{head} is given just \T{Cons} and not \T{Nil}.

\begin{figure}
\begin{code}
data Cons; data Unknown
newtype List a t = List [a]

nil :: List a Unknown
nil = List []

cons :: a -> [a] -> List a Cons
cons a as = List (a:as)

fromList :: [a] -> List a Unknown
fromList xs = List xs

safeTail :: List a Cons -> a
safeTail (List (a:as)) = as
\end{code}
\caption{A safe \C{head} function with Phantom types.}
\label{fig:phantom}
\end{figure}

Types can also be used to explicitly encode invariants on data in functional languages is the type system. One approach is the use of Phantom types \citep{fluet:phantom}, for example a safe variant of \C{head} can be written as in Figure \ref{fig:phantom}. In this example the \C{List} data structure would not be exported, ensuring that all lists with a \C{Cons} tag are indeed non-empty. The values \C{Cons} and \C{Unknown} are phantom types -- they exist only at the type level, not at the value level.

\begin{figure}
\begin{code}
data ConsT a; data NilT

data List a t where
    Cons  :: a -> List a b -> List a (ConsT b)
    Nil   :: List a NilT

safeTail :: List a (ConsT t) -> List a t
safeTail (Cons a b) = b

fromList :: [a] -> (forall t. List a t -> r) -> r
fromList []      fn = fn Nil
fromList (x:xs)  fn = fromList xs (\sl -> fn (Cons x sl))
\end{code}
\caption{A safe \C{tail} function using GADT's.}
\label{fig:gadt}
\end{figure}

Another method of encoding addition type information which is becoming increasingly popular in Haskell is the use of GADT's \citep{spj:gadt}. Using this technique, sometimes referred to as first class Phantom Types, an encoding of lists can be written as in Figure \ref{fig:gadt}. Unlike the first method, the \C{fromList} method requires the complexity of existential types.

The type directed method can be pushed much further with dependant types, which allow types to depend on values. This approach allows safe versions of \C{head} and \C{tail} to be written. There has been much work on type systems, using undecidable type systems \citep{cayenne, epigram}, using extensible kinds \citep{omega} and using type systems restricted to a decidable fragment \citep{xi:dependent_practical}.

The downside to all these type systems is that they require the programmer to make explicit annotations, and require the user to learn new techniques for computation.


\section{Conclusions and Future Work}
\label{sec:conclusion}

A static checker for potential pattern-match errors in Haskell has been specified and implemented. This checker is capable of determining preconditions under which a program with non-exhaustive patterns executes without failing due to a pattern-match error. A section of the Nofib suite has been tested, with encouraging results -- all but one program can be proved safe, many without modification. Two larger real-world examples have been checked, and new bugs have been discovered in one existing program.

There are two main avenues of future work. First is to extend the power of the checker. Possible directions to increase the power include the addition of explicit annotations of properties, or a more powerful constraint language. The constraint languages introduced could be augmented with special purpose constraints, designed to tackle particular problems. One particular constraint system that may prove fruitful is a linear inequality constraint system \todo{Colin wants citation}.

The next direction for future work would be using the \catch{} tool to solve other problems. The direct result of the absence of pattern match failure could be used to feed information to an optimising compiler. The properties generated could also be used to generate more efficient code. If the programmer was able to specify more restrictive preconditions, or properties, or invariants, then these may be able checked with Catch. If the power of the constraint system was increased then richer program annotations could be used.

As the Catch tool stands, it is already capable of detecting and proving the absence of pattern match failures in sample programs. We hope this tool will become a part of the standard arsenal of Haskell programmers.


% \appendix
% \section{Appendix Title}
%
% Here is the text of the appendix, if you need one.

\acks

The first author is a PhD student supported by a studentship from the Engineering and Physical Sciences Research Council of the UK.

\bibliographystyle{plainnat}
\bibliography{catch}



\end{document}
