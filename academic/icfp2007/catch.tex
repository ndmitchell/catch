\documentclass[preprint]{sigplanconf}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{alltt}
\usepackage{url}
\usepackage{natbib}
\usepackage{datetime}
\usepackage{comment}

%include polycode.fmt
%include catch.fmt

% general stuff
\newcommand{\T}[1]{\texttt{#1}}
\newcommand{\C}[1]{\textsf{#1}}
\newcommand{\tup}[1]{\ensuremath{\langle #1 \rangle}}
\newcommand{\mtxt}[1]{\textsf{#1}}

\newcommand{\catch}{\textsc{Catch}}

% examples
\newcounter{exmp}
\setcounter{exmp}{1}
\newcommand{\yesexample}{\subsubsection*{Example \arabic{exmp}}\addtocounter{exmp}{1}}
\newcommand{\noexample}{\hfill$\Box$}
\newcommand{\lastexample}{\arabic{exmp}}

\newcommand{\todo}[1]{\textbf{\textsc{Todo:} #1}}

% code blocks
\newenvironment{code}{\begin{alltt}\small}{\end{alltt}}
\newenvironment{codepage}
    {\begin{minipage}[h]{\textwidth}\begin{code}}
    {\end{code}\end{minipage}}

\newenvironment{example}{\yesexample}{\noexample}

\newcommand{\K}{\ensuremath{^\ast}} % kleene star
\newcommand{\D}{\ensuremath{\cdot}} % central dot

\renewcommand{\c}[3]{\tup{\T{#1},\T{#2},\T{\{#3\}}}}
\newcommand{\cc}[2]{\c{#1}{$\lambda$}{#2}}

\newcommand{\s}[1]{\ensuremath{_{\tt #1}}} % subscript, in tt font
\newcommand{\g}[1]{\{#1\}} % group, put { } round it
\newcommand{\U}{\textunderscore}
\newcommand{\vecto}[1]{\overrightarrow{#1\;}}
\newcommand{\gap}{\;\;}
\newcommand{\dom}{\text{dom}}


\begin{document}

\conferenceinfo{ICFP '07}{date, City.} %
\copyrightyear{2007} %
\copyrightdata{[to be supplied]}

\titlebanner{\today{} - \currenttime{}}        % These are ignored unless
\preprintfooter{}   % 'preprint' option specified.

\title{Safe Pattern Matching}
\subtitle{A Static Checker for Haskell}

\authorinfo{Neil Mitchell}
           {York}
           {ndm}
\authorinfo{Colin Runciman}
           {York}
           {colin}

\maketitle

\begin{abstract}
A Haskell program may fail at runtime with a pattern-match error if the program has any incomplete (non-exhaustive) patterns in definitions or case alternatives. This paper describes a static checker that allows non-exhaustive patterns to exist, yet ensures that a pattern-match error does not occur. It describes a constraint language that can be used to reason about pattern matches, along with mechanisms to propagate these constraints between program components.

The checker obtains minimal preconditions for 10 out of 14 programs in the Imaginary section of the Nofib suite\citep{nofib}. With minor modifications a further 3 are proven safe.
\end{abstract}

% \category{CR-number}{subcategory}{third-level}

% \terms
% term1, term2

% \keywords
% keyword1, keyword2

\section{Introduction}
\label{sec:introduction}

Often it is useful to define pattern matches which are incomplete, for example \C{head} fails on the empty list. Unfortunately programs with incomplete pattern matches may fail at runtime.

The problem of pattern match failures in Haskell is a serious one. The darcs \citep{darcs} project is one of the most successful large scale programs written in Haskell. Taking a look at their bug tracker, 13 problems are errors related to \C{fromJust} and 19 are direct pattern match failures.

Consider the following example:

\begin{code}
risers :: Ord alpha => [alpha] -> [[alpha]]
risers [] = []
risers [x] = [[x]]
risers (x:y:etc) = if x <= y then (x:s):ss else [x]:(s:ss)
    where (s:ss) = risers (y:etc)
\end{code}

A sample execution of this function would be:

\begin{code}
> risers [1,2,3,1,2]
[[1,2,3],[1,2]]
\end{code}

In the last line of the definition, |(s:ss)| is matched against the output of \C{risers}. If |risers (y:etc)| returns an empty list a pattern match error will be raised. It takes a few moments to check this program manually -- and a few more to be sure one has not made a mistake!

Turning the \C{risers} function over to the checker developed in this paper, the output is:

\begin{code}
> Incomplete pattern on line 5, proven safe
> Program is Safe
\end{code}

In addition the checker produces a set of properties it has proved about Risers, along with an outline of the proof. Fuller details of how the checking is performed follow in \S\ref{sec:walkthrough}.

\subsection{Contributions}

The contributions of this paper include:

\begin{itemize}
\item Two constraint languages to reason about pattern match failures.
\item A mechanism for generating and solving appropriate constraints.
\item Details of an implementation which handles the complete Haskell 98 language \citep{haskell}.
\item An implementation of the checker, referred to as \catch07.
\item Results, showing success on a number of unmodified examples.
\item Some larger examples, investigating the scalability of the checker.
\end{itemize}

In \citet{me:catch_tfp} a pattern match checker was described with similar aims, which we will refer to as \catch05. All the issues listed in the future work section of that paper have been addressed in this paper. \catch07 is robust enough to handle real Haskell programs, with no restriction on the recursion patterns, higher order function use or type classes. Many scalability problems have been addressed -- \catch07 is able to check much larger programs.

The underlying mechanism of \catch07 is radically different from \catch05 -- particularly in relation to the constraints and fixed points. Perhaps the introductory risers example is most clearly illustrates the differences -- for \catch05 risers was towards the boundary of what was possible, for \catch07 it is trivial.

\subsection{Road map}

All data structures and equations are presented in Haskell, although should be accessible to all readers.

\S\ref{sec:walkthrough} gives an overview of the checking process for the \C{risers} function. \S\ref{sec:manipulate} introduces a small core functional language and a mechanism for reasoning about this language, \S\ref{sec:constraint} covers a constraint language. \S\ref{sec:transform} discusses how to transform Haskell to this core language.

\S\ref{sec:results} evaluates Catch on various sample programs -- including the Nofib benchmark suite. \S\ref{sec:related} compares our work to other work before \S\ref{sec:conclusion} presents concluding remarks.

\section{Walkthrough of Risers}
\label{sec:walkthrough}

This section details the process of checking that the \C{risers} function in the Introduction does not crash with a pattern match error.

\subsection{Modifying Risers for analysis}

In order to check \C{risers}, a \C{main} function must be provided, where \catch{} will begin checking:

\begin{code}
main :: [Int] -> [[Int]]
main x = risers x
\end{code}

\catch imposes a restriction on \C{main}, it must not have any argument of functional type, or any type classes. The reason for this restriction is to reduce the complexity of analysis. In \C{main} the type signature is more restrictive than \C{risers}, removing the type class.

In Haskell, type classes are typically implemented as dictionaries \citep{wadler:type_classes}. All functions with type classes have one or more dictionaries of class methods as auxiliary arguments. This implementation results in type classes being converted to arguments of functional type, hence the restriction on type classes.

\subsection{Conversion to a Core Language}

Rather than analyse full Haskell, \catch{} analyses a first-order Core language, without lambda expressions, partial application or let bindings. A convertor is provided from the full Haskell 98 language to this restricted language. Full details are provided in \S\ref{sec:transform}.

After converting the \C{risers} program to Core Haskell, and renaming the identifiers for ease of human reading, the resulting Core is as shown in Figure \ref{fig:risers_core}. The case expressions in the functions \C{risers4} and \C{risers2} correspond to the pattern match in the |where|. The function |<=| in this example has been specialised to the Int data type, unlike the standard |(<=)| operator which is a member of the \C{Num} class.

\begin{figure}
\begin{code}
risers3 x y = risers4 (risers (x : y))

risers4 x = case x of
    (y:ys) -> (ys, y)
    [] -> error "Pattern Match Failure, 11:12."

risers x = case x of
    [] -> []
    (y:ys) ->  case ys of
         [] -> (y : []) : []
         (z:zs) -> risers2 (risers3 z zs) (y <= z) y

risers2 x y z =  case y of
    True -> (z : snd x) : (fst x)
    False -> (z : []) : (snd x : fst x)
\end{code}
\caption{\C{risers} in the Core language.}
\label{fig:risers_core}
\end{figure}

\subsection{Analysis}

In the Core language every pattern match covers all possible constructors of the appropriate type. The alternatives for constructor cases not originally given are calls to \C{error}. The analysis first starts by finding calls to \C{error}, then tries to prove that these calls will not be reached. In the \C{risers} example there is only one \C{error} call, corresponding to the |where| pattern match.

Only one precondition is required for this example (see \S\ref{sec:precond}):

\begin{code}
risers4, x :< (:)
\end{code}

This condition states that all callers of \C{risers4} must supply an argument which is a |(:)| constructed value. To construct a proof, two properties are required (see \S\ref{sec:template}):

\begin{code}
x :< (:)  => (risers x       ) :< (:)
True      => (risers2 x y z  ) :< (:)
\end{code}

The first property says that provided the input to \C{risers} is a |(:)| constructed value, the output will be. The second states that the output from \C{risers2} is always a |(:)|. Given the precondition, and the properties, a proof \C{risers} does not reach \C{error} can be constructed.

\section{Pattern Match Analysis}
\label{sec:manipulate}

This section explains the underlying constraint system in Catch, focusing on how the constraints are put together to express properties of expressions and the data structures they evaluate to.

\subsection{Reduced expression language}
\label{sec:core}

\begin{figure}
\begin{code}
type CtorName  =  String
type FuncName  =  String
type VarName   =  String
type Field     =  (CtorName, Int)

data Func  =  Func FuncName [VarName] Expr

data Expr  =  Var   VarName
           |  Make  CtorName  [Expr]
           |  Call  FuncName  [Expr]
           |  Case  Expr      [Alt]

data Alt   =  Alt CtorName [VarName] Expr
\end{code}
\caption{Core Data Type.}
\label{fig:core}
\end{figure}

Syntax for the core language is given in Figure \ref{fig:core}. Our Core language is a little more restrictive than the core languages used in other compilers \citep{ghc_core}. The language is first order, has only simple case statements, and only algebraic data types. All case statements are syntactically complete, with error being introduced where a pattern match error would occur. An \C{Alt} data type has the constructor to match, a list of variables to bind to and an expression to execute.

The evaluation strategy is lazy. \todo{Colin wants semantics}

\subsubsection{Operations on Core}

\begin{figure}
\begin{code}
ctors        :: CtorName  -> [CtorName]
arity        :: CtorName  -> Int
var          :: VarName   -> Maybe (Expr, Field)
instantiate  :: FuncName  -> [Expr] -> Expr
isRec        :: Field     -> Bool
\end{code}
\caption{Operations on Core.}
\label{fig:core_operations}
\end{figure}

Figure \ref{fig:core_operations} gives functions which the checker uses to operate over the core data type. Every constructor has an arity, which can be obtained with the \C{arity} function. To determine all constructors in a set the \C{ctors} function can be used, for example |ctors True = {False, True}| and |ctors [] = {[], :}|. The \C{var} function returns \C{Nothing} for variables which are arguments, and \C{Just} with the expression on the \C{Case} expression and the field. The function \C{instantiate} corresponds to $\beta$ reduction.

The |isRec (c,n)| function returns true if the constructor |c| has a recursive element at position |n|. This function is used to bound the size of constraints. Taking the example of a \C{List}:

\begin{code}
data List alpha = Nil | Cons alpha (List alpha)
\end{code}

The type of the definition is |List alpha|, all fields of this type are said to be recursive. \C{Cons} has two fields, the first is of type |alpha| and is not recursive, the second is of type |List alpha| and is considered recursive.

This approach works provided no fields have a type that is structurally greater than the defined type, and no higher kinded type variables are used. If the presence of definitions involving these features, either a translation can be made to a permitted form, or \C{isRec} can conservatively return True.

More formally, there exists a function |bounded t| such that the number of non-recursive fields in any path of any value of type |t| is less than this value.

\begin{code}
forall x,t,p `o`  x :: t && p `elem` paths x =>
                  length (filter (not . isRec) p) < bounded t
\end{code}

A path is a list of fields, the \C{paths} function returns all possible paths in a given data structure -- a potentially infinite number.

\subsubsection{Abstraction}
\label{sec:abstraction}

The Core language only has algebraic data types. So what about primitive types such as text characters, integers (bounded and unbounded) and floating point numbers? \catch{} allows for these types by abstracting them into algebraic data types.

Natural numbers are often encoded by Peano numerals, and this idea can easily be extended to integers:

\begin{code}
data Pos  = One | Succ Pos
data Int  = Minus Pos | Zero | Plus Pos
\end{code}

Although this abstraction of \C{Int} captures all the underlying detail of the number system, the underlying constraint systems discussed in \S\ref{sec:constraint} would be unable to distinguishing between any pair of numbers both greater than 2, or both less than -2. The actual abstraction used in \catch{} is:

\begin{code}
data Int = Neg | Zero | One | Pos
\end{code}

In our experience, most constraints on numeric systems require a number to be a natural, or to be non-zero. Similarly, the addition or subtraction of one is the most common operation. These trends stem from the common treatment of integers as inductive natural numbers in a functional language \citep{runciman:naturals}. This abstraction models the common properties and operations well, while remaining simple.

The abstraction of characters is often more practically interesting. The Haskell language standard calls for Unicode character literals. In practice, depending on the compiler, the number of distinct characters varies between 256 and 1114111. There are several possible abstractions of characters:

\begin{code}
data Char = Char
data Char = Alpha | Digit | White | Other
data Char = 'A' .. 'Z' | 'a' .. 'z' | '0' .. '9' | Other
data Char = '\0' | '\1' | '\2' ..
\end{code}

The simplest abstraction is that all characters are the same. A slightly more refined abstraction is to partition the characters based on the character testing functions provided in the \C{Char} module of Haskell. Refining the model further can give special status to characters that commonly occur, giving all uncommon characters the \C{Other} value. Finally, given that \C{Char} is a finite enumeration, the entire range of characters could each be represented distinctly.

\catch{} employs the first abstraction by default, but provides a flag to allow alternative abstractions to be selected. This choice is motivated by simplicity -- few additional programs are proven safe with a more specific character abstraction.

The final issue of abstraction relates to primitive functions in the \C{IO} monad, for example the \C{getArgs} function which returns the command line arguments, or the \C{readFile} which reads from the underlying filesystem. In most cases the function is modelled as returning any value of the correct type.


\subsection{Constraints}

An expression in our first-order core language either evaluates to a (potentially infinite) data structure, or to $\bot{}$ caused by either non-termination of pattern match error. If an expression does evaluate to a data structure, then a constraint states the possible forms of data value it may take. A constraint describes a set of data structures that a value may match.

If |x| is an expression and |c| is a constraint we write |x :< c| to assert that expression |x| must, when evaluated, be a member of the set defined by |c|. Since the Core language is typed, the constraint |c| will only refer to a set of well typed terms. These atomic constraints can be built up into a proposition of constraints, each possibly on different variables.

Free variables are bound by function calls. It is therefore convenient to have a notation for constraints upon named arguments of named functions: |f, x :< c| states that the argument |x| of |f| must be in the set |c|.

We require that for any type, there are only a finite number of constraints. This property is used for termination proofs. Several underlying models are possible, as discussed in \S\ref{sec:basic}, \S\ref{sec:regexp} and \S\ref{sec:multipattern}.


\subsection{Basic Constraints and Pattern Matching}
\label{sec:basic}

\begin{figure}
\begin{code}
type Constraint = [Match]

data Match  =  Match CtorName [Match]
            |  Any
\end{code}
\caption{Basic pattern constraints.}
\label{fig:basic}
\end{figure}

\begin{figure}
\begin{code}
data Req a = a :< Constraint

notin :: CtorName -> Constraint
(|>) :: Field -> Constraint -> Constraint
(<|) :: CtorName -> Prop (Req Int)
\end{code}
\caption{Constraint operations.}
\label{fig:constraint}
\end{figure}

\todo{COLIN QUESTION: Propositions are never introduced, and things like |(||||)| and |(&&)| etc are assumed to work over propositions and booleans equally. Is this wrong? Should this be fixed? What about \C{propLit}?}

A basic constraint directly corresponds to Haskell pattern matching. A constraint is a finite set of patterns represented in Figure \ref{fig:basic}. A given data structure |d| satisfies a constraint |ms| if |d| matches at least one pattern in |ms|. True and false can be represented as |[Any]| and |[]| respectively.

This constraint language is limited in expressivity -- for example it is impossible to state that all the elements of a list are True. However, even with this limited constraint language the Risers example can be proven safe. For simplicity, the analysis framework will be introduced using these basic constraints. In  reality more powerful constraints are used, see \S\ref{sec:regexp} and \S\ref{sec:multipattern}.

There are 3 operations which must be provided on every constraint implementation, whose signatures are given in figure \ref{fig:constraint}. \C{Req} is the type of the |(:<)| condition, introduced previously. The operators |(||>)| and |(<||)| are discussed in \S\ref{sec:backward}. The \C{notin} function generates a constraint which does \textit{not} match the constructor given. For example, an implementation for the simple constraint language would be:

\begin{code}
notin c = map f (delete c (ctors c))
   where f x = Match x (replicate (arity c) Any)
\end{code}

This constraint language violates the property of having a finite number of constraints a type. Consider a list: an infinite number of type-correct patterns are possible. This issue could be resolved by limiting the depth of the patterns, but would obscure the simplicity of the constraints.

\subsection{Safety Preconditions}
\label{sec:precond}

\begin{figure}
\begin{code}
pre :: Expr -> Prop (Req Expr)
pre (Var   x         ) = True
pre (Make  _   xs    ) = and (map pre xs)
pre (Call  fn  xs    ) = preFunc fn xs && and (map pre xs)
pre (Case  on  alts  ) = pre on && and [f c e | Alt c vs e <- alts]
    where f c e = (on :< notin c) || pre e

preFunc :: FuncName -> [Expr] -> Prop (Req Expr)
\end{code}
\caption{Precondition of an expression, \C{pre}.}
\label{fig:precondition}
\end{figure}

For every function, constraints on the arguments form a precondition for pattern safety. This precondition implies that all function calls made will have their preconditions met. The precondition on \C{error} is False. The process of analysis can be seen as deriving these preconditions.

A program is safe if the precondition on \C{main} is True. Given a function that returns the precondition on a function's arguments \C{preFunc}, a function to determine the precondition on an expression can be specified as \C{pre} in figure \ref{fig:precondition}.

\subsubsection{Precondition Fixed Point}

The preconditions given by |preFunc| are sound if a fixed point can be found, satisfying the following property:

\begin{code}
forall fn, xs `o` preFunc fn xs => pre (instantiate fn xs)
\end{code}

This property states that the preconditions assigned to each function by \C{preFunc} must imply the preconditions calculated on the instantiation of that function with its arguments. One such fixed point is that all calls to \C{preFunc} return False. A perfect precondition would be such that |preFunc f xs == pre (instantiate f xs)|.

\begin{figure}
\begin{code}
for fn `elem` funcs do conds(fn) <- (fn /= "erorr")
loop
    for fn `elem` funcs do
        conds'(fn) <- conds(fn) && pre (instantiate fn vs)
    if conds' == conds then break
    conds <- conds'
end loop
    where
        preFunc fn xs = cond(fn)[vs1/xs1 .. vs_n/xs_n]
        vs = ... -- free variables
\end{code}
\caption{Precondition fixed point calculation.}
\label{fig:precond_fixp}
\end{figure}

The algorithm for finding a fixed point is given in Figure \ref{fig:precond_fixp}. Initially all preconditions are assumed to be True, apart from \C{error}, which is False. New preconditions are calculated for every function, until the conditions do not change. The reason for anding with the previous precondition is to ensure termination. In each iteration the preconditions become more restrictive, and therefore given a constraint model with a finite number of terms, termination is guaranteed.

A more efficient algorithm can be derived by tracking which preconditions depend on which others, and only performing the minimum amount of recalculation. Computing the preconditions for functions based on the dependencies can also speed up the computation. Constructing a set of strongly connected components would also allow the computation for separate parts of the program to be performed in isolation.

\subsubsection{Preconditions and Laziness}

This method for computing preconditions does not respect laziness. The \C{Call} equation demands that preconditions to hold on all arguments -- only correct if a function is strict in all arguments. For example, the precondition on |False && error "here"| is False, when it should be True. In general, preconditions may be more restrictive than necessary, so remains sound. The analysis relies on laziness for transformations.

To introduce laziness an evaluation method could be added, which would require an additional fixed point computation, and complicate both the analysis and the generated constraints. Investigation of a range of examples suggests inlining |(&&)| and |(||||)| captures many of the common cases where laziness would be required.


\subsection{Manipulating constraints}
\label{sec:backward}

\begin{figure}
\begin{code}
back :: Req Expr -> Prop (Req Expr)
back (Var   x         :< k) = on :< (c |> k)
    where Just (on, c) = var x
back (Make  c   xs    :< k) = replaceVars xs (c <| k)
back (Case  on  alts  :< k) = and [f c e | Alt c vs e <- alts]
    where f c e = (on :< notin c) || back (e :< k)
back (Call  fn  xs    :< k) = replaceVars xs (property fn k)

backs :: Prop Req -> Prop (Req VarName)
property :: FuncName -> Constraint -> Prop (Req Int)

replaceVars :: [Expr] -> Prop (Req Int) -> Prop (Req Expr)
replaceVars xs p = mapProp (\(i:<k) -> (xs!!i) :< k) p
\end{code}
\caption{Specification of backward analysis, \C{back}}
\label{fig:backward}
\end{figure}

Constraints on arguments to a function can be considered as preconditions to that function. Constraints on other expressions can be rewritten using the \C{back} function, detailed in Figure \ref{fig:backward}. The \C{backs} function repeatedly applies the \C{back} function rules until all remaining constraints are on arguments.

The \C{replaceVars} function takes a proposition of constraints over a set of variables, |v1..v_n|, and replaces each variable with a corresponding expression. This implementation ensures that the function \C{property} and the operator |(<||)| are not effected by the expressions within the \C{Call} or \C{Make}.

\begin{description}

\item[The \C{Var} rule] applies only to variables which are bound by case expressions. This rule moves the condition from the bound variable to the subject of the |case| expression which introduced it.

    This equation requires the |(||>)| operation, which lifts a constraint on one part of a data structure, to a constraint on the entire data structure. The implementation of this operation on the simple constraint type is now given:

\begin{code}
(c,i) |> k = map extend k
    where
    extend x = Match c (anys i ++ [x] ++ anys (n-i))
    anys j = replicate j Any
    n = arity c
\end{code}

\item[The \C{Make} rule] deals with an application of a constructor. The |(<||)| operator splits a basic constraint on an entire structure into a combination of constraints on each part.

\begin{code}
c <| k = propOr (map f k)
    where
    f Any = Any
    f (Match c2 xs) = c2 == c &&
        and (zipWith (:<) [0..] (map (:[]) xs))
\end{code}

\item[The \C{Case} rule] generates a conjunct for each alternative. The generated condition says either the subject of the case analysis has a different constructor (so this particular alternative is not executed in this circumstance), or the right hand side of the alternative is safe given the conditions for this expression.

\item[The \C{Call} rule] relies on properties of the applied function. The key property of this equation is:

    \begin{code}
    forall fn, xs, k `o`  backs (Call fn xs :< k) =>
                          backs (instantiate fn xs :< k)
    \end{code}

    Two strategies that can be used to implement this:

    \begin{enumerate}
    \item Use $\beta$ substitution (\C{instantiate}) replacing the call with the body of |fn|.
    \item Create fresh variables for each element in |xs|, solve this new problem, and then instantiate the results.
    \end{enumerate}

    An earlier version of the Catch tool \citep{me:catch_tfp} used method 1. But the current version uses method 2, which is more amenable to finding a fixed point, and allows a cache of results to be built from which future questions can be answered.

    The implementation uses a function \C{property} which fresh variables and solves the problem on this problem. The \C{replaceVars} changes the fresh variables into the arguments give to the |Call|.

    \begin{figure}
    \begin{code}
    ps <- [(fn,k) | fn <- funcs, k <- constraints]
    for (fn,k) `elem` ps do prop(fn,k) := True
    loop
        for (fn,k) `elem` p do
            prop'(fn,k) := prop(fn,k) && backs (instantiate fn vs :< k)
        if prop' == prop then break
        prop <- prop'
    end loop
        where
            property fn k = prop(fn,k)
    \end{code}
    \caption{Fixed point calculation for \C{property}.}
    \label{fig:property_fixp}
    \end{figure}

    The properties can be calculated by obtaining a fixed point as detailed in Figure \ref{fig:property_fixp}. In a similar strategy to the fixed point for preconditions in \S\ref{sec:precond}, each result is anded with its existing value. Given that there are a finite number of constraints, there are only a finite number of constraint/function pairs, which guarantees that this function terminates.

    In a similar way to the precondition fixed point, a speed up can be obtained by tracking the dependencies between constraints.
\end{description}

\subsection{Semantics of Constraints}

It is useful to have a way of expressing which values a constraint accepts. It turns out that the |(||>)| operator already provides all the necessary information:

\begin{code}
data Value = Value CtorName [Value]

accept :: Value -> Constraint -> Bool
accept (Value c xs) = mapProp f (c <| k)
    where f (i :< k2) = accept (xs !! i) k2
\end{code}

With this representation of the semantics of a constraint, it is now possible to state more concretely the properties that were previously stated about the other operations:

\begin{code}
forall v, k, i `o` accept v ((c,i) |> k) => (c2 /= c || accept (x !! i) k)
    where Value c2 xs = v

forall v, c `o` accept v (notin c) => (valueCtor v /= c)
    where valueCtor (Value c2 xs) = c2
\end{code}

Note that in both properties the |=>| operator is used -- the constraints may become more restrictive than necessary to capture the semantics.

\section{Constraint Systems}
\label{sec:constraint}

There are various interpretations of constraints, here we outline two -- one adapted from \catch05, the other used in \catch07. Neither is strictly more powerful than the other; each is capable of expressing constraints that the other cannot expression.

\subsection{Regular Expression Constraints}
\label{sec:regexp}

\begin{figure}
\begin{code}
data Constraint = RegExp :- [CtorName]

type RegExp = [Atom]

data Atom  =  Atom  Field
           |  Star  [Field]

notin :: CtorName -> Constraint
notin c = [] :- delete c (ctors c)

(|>) :: Field -> Constraint -> Constraint
p |> (r :- cs) = integrate p r :- cs

(<|) :: CtorName -> Constraint -> Prop (Req Int)
c <| (r :- cs) = (ewp r => c `elem` cs) &&
    and (zipWith f (paths c) [0..])
    where
    f p x = case  differentiate p r of
                  Nothing -> True
                  Just r2 -> x :< (r2 :- cs)

ewp :: RegExp -> Bool
ewp x = all isStar x
   where  isStar (Star  _) = True
          isStar (Atom  _) = False

integrate :: Field -> RegExp -> RegExp
integrate p r | not (isRec p) = Atom p : r
integrate p (Star ps:r) = Star (nub (p:ps) : r)
integrate p r = Star [p] : r

differentiate :: Field -> Path -> Maybe Path
differentiate p [] = Nothing
differentiate p (Atom  r:rs)  | p == r     = Just rs
                              | otherwise  = Nothing
differentiate p (Star  r:rs)  | p `elem` r  = Just (Star r:rs)
                              | otherwise   = differentiate p rs
\end{code}
\caption{RE-constraints.}
\label{fig:regexp}
\end{figure}

\catch05 used regular expressions in constraints. A data type for regular expression based constraints (RE-constraints), along with the essential operations upon it is given in Figure \ref{fig:regexp}. In a constraint of the form |(r :- cs)|, |r| is a regular expression, and |cs| is a set of constructors. Such a constraint is satisfied by a data structure if every well-defined application to a path of fields described by |r| reaches a constructor in the set |cs|.

That is:

\begin{code}
e :< (r :- c) <=> (forall l `elem` lang r `o` defined e l => ctor e l `elem` c)
\end{code}

Here |lang r| is the language represented by the regular expression |r|; \C{defined} returns true if a path selection is well-defined; and \C{ctor} gives the constructor used to create the data, after following the path. Since |lang r| is potentially infinite, this property cannot be checked by enumeration. If no path selection is well-defined then the constraint is vacuously true.

As usual, a regular expression takes on of six forms:\\ \\
\begin{tabular}{ll}
|s+t|  & union of regular expressions |s| and |t| \\
|s^.t| & concatenation of regular expressions |s| then |t| \\
|s^*|  & any number (possibly zero) occurrences of |s| \\
\C{x}  & a field, i.e. |(:,0)| for the head of a list \\
       & We use \C{hd} to abbreviate |(:,0)| and \C{tl} to abbreviate |(:,1)|. \\
0      & the language is the empty set \\
1      & the language is the set containing the empty string
\end{tabular} \\

The differentiation operation is defined by \citet{conway:regexp}, and called quotient in some text books. The empty word property (\C{ewp}) is equivalent to |lang 1 `subseteq` lang r|, and can be calculated simply from a regular expression. Integration is the inverse of differentiation.

\catch05 regular expressions were unrestricted. In contrast \catch07 is limited to concatenation of atoms, or stars of unions. Formally the context free grammar is:

\[\begin{array}{lllll}
r & = & 1     & || & |a^.r| \\
a & = & \C{x} & || & |u^*| \\
u & = & \C{x} & || & \C{x} + u
\end{array}\]

There are additional restrictions on regular expressions: all fields for which \C{isRec} returns true must be under \C{Star} and all which are not must be \C{Atom}. Also two \C{Star}s cannot be concatenated.

In \catch05 regular expressions quickly grew to an unmanageable size, thwarting analysis of larger programs. The original solution to this problem was to bound the number of successive fields to some depth, typically 3, and use kleene star for anything larger. The restricted language was chosen for several reasons:

\begin{itemize}
\item The constructors, because of static typing and the restricted form of regular expression, must all be of the same type. In \catch05 expressions such as |hd^*| could arise, which is no longer type safe.

\item There are only a finite number of regular expressions for any type. Combined with the finite number of constructors, this property is enough to guarantee finiteness, which satisfies the termination properties assumed on constraints.

\item This restricted regular expression language, combined with 0, is closed under integration and differentiation. The 0 alternative is catered for by the \C{Maybe} return type in the differentiation. The constraint |0 :- c| always evaluates to True, so in |(<||)|, \C{Nothing}\ is replaced by True.
\end{itemize}

\subsection{Example Constraints}
\label{sec:small_examples}

In order to introduce the constraints more concretely, we shall express the preconditions to a set of sample expressions:

\begin{description}
\item[|(head x)|] is safe if |x| evaluates to a non-empty list. The constraint for this is:

    \begin{code}
    x :< (1 :- {:})
    \end{code}

\item[|(map head x)|] is safe if |x| evaluates to a list of non-empty lists. This is expressed as:

    \begin{code}
    x :< (tl^* ^. hd :- {:})
    \end{code}

    If the list is empty, it still satisfies the constraint. If the list is infinite then the condition applies to all elements, constraining an infinite number. This constraint was not possible with the simple constraint framework introduced earlier.

\item[|(map head (reverse x))|] is safe if either every item in the list is |(:)|-constructed, or if the list is infinite -- as in this case \C{reverse} will not terminate.

    \begin{code}
    x :< (tl^* ^. hd :- {:}) || x :< (tl^* :- {:})
    \end{code}
\end{description}

\subsection{Constraint Propositions}

The underlying type manipulated by Catch is not a constraint, but propositional formulae with constraints as atomic propositions. These propositions are compared for equality to obtain a fixed point, and are variously manipulated by the system. Since the complexity of performing an operation is often proportional to the number of atomic constraints in the proposition, simplification rules can decrease the running time.

From the definition of the constraints it is possible to construct a number of simplification rules. There are nineteen rules implemented in the current checker, three particularly useful ones are:

\begin{description}
\item[Exhaustive conditions:] in the constraint |x :< (r :- {:,[]})| the condition lists all the possible constructors, if |x| reaches weak head normal form then because of static typing |x| must be one of these constructors, therefore this constraint simplifies to True.

\item[And merging:] given |e :< (r :- c1) && e :< (r :- c2)|, this constraint can be replaced by |e :< (r :- (c1 `union` c2))|.

\item[Or merging:] given |e :< (r1 :- c1) || e :< (r2 :- c2)|, where |r1 `subseteq` r2 && c1 `subseteq` r2|, this can be replaced by |e :< (r2 :- c2)|.
\end{description}

All the fixed point algorithms given in this paper stop once equal values are found. One method for testing a proposition for equality is to transform it to disjunctive normal form -- unfortunately this can often cause the size of the proposition to increase substantially. We use a Binary Decision Diagram (BDD) \citep{lee:bdd} to test for equality faster.

\subsection{Multipattern Constraints}
\label{sec:multipattern}

\begin{figure}
\begin{code}
type Constraint = [Val]

data Val    =  [Match] :* [Match]
            |  Any
data Match  =  Match CtorName [Val]

-- useful auxiliary, a complete pattern
complete :: CtorName -> Match
complete c = Match c [Any  |  i <- [0..arity n - 1]
                           ,  not (isRec(c,i))]

notin :: CtorName -> Constraint
notin c = map complete (delete c cs) :* map complete cs
    where cs = ctors c

(|>) :: Field -> Constraint -> Constraint
(c,i) |> k = notin c ++ map f k
    where
    f Any = Any
    f (p1 :* p2) | isRec (c,i) = complete c :* (p1 `mergePart` p2)
    f v = Match c fs :* map complete (ctors c)
        where fs =  [  if i == j then v else Any
                    |  j <- [0..arity c - 1], not (isRec (c,j))]

-- \todo{Fixups: Use the new data type below here}
(<|) :: CtorName -> Constraint -> Prop (Req Int)
c <| vs = or (map f vs)
    where
    f Any = Any
    -- mashed and manged below here
    f (p1 :* p2(c1 :* v1) :| r) = c `elem` c1 && and (zipWith g (paths c) xs)
        where g p x = if isRec p then r :| r else lookup p v1

(`mergeVal`) :: Val -> Val -> Val
(a1 :* b1) `mergeVal` (a2 :* b2) = (mergePart a1 a2) :* (mergePart b1 b2)

mergePart :: Part -> Part -> Part
mergePart (c1 :| v1) (c2 :| v2) = (c1 `intersect` c2) :| zipWith f v1 v2
    where f (p1,v1) (p2,v2) = assert (p1 == p2) (v1 `mergeVal` v2)
\end{code}
\caption{MP-constraints.}
\label{fig:enumeration}
\end{figure}

While the RE-constraints are capable of solving many problems, they suffer from a problem of scale. As the system becomes more complex the size of the propositions grows quickly, slowing down the system to an unacceptable level. Multipattern constraints (MP-constraints) are an alternative which scales better.

MP-constraints are implemented as in Figure \ref{fig:enumeration}. These constraints are similar to basic pattern constraints, but can constrain an infinite number of items, and can be represented in a finite space.

Given a constraint |p1 :* p2|, |p1| is the restricts the current value, while |p2| restricts all the recursive fields. Each part is given as a set of matches -- much like the simple pattern language in \S\ref{sec:basic}. The difference is that each \C{Match} only specifies the values for the non-recursive fields, all recursive fields are handled by |p2|. The proof that there are finitely many multipattern constraint for a given type is relatively easy.

Revisiting the examples from the RE-constraints in \S\ref{sec:small_examples}:

\begin{description}
\item[|(head x)|] requires |x| to be a non-empty list.

    \begin{code}
    {(:) Any} :* {[], (:) Any}
    \end{code}

    This constraint is longer than the corresponding RE-constraint, partly because it is more explicit. Although no restriction is given on the \C{hd} field, it is still stated.

\item[|(map head x)|] requires |x| to be a list of non-empty lists.

    \begin{code}
    {[], (:) ({(:) Any} :* {[], (:) Any})}
    :*
    {[], (:) ({(:) Any} :* {[], (:) Any})}
    \end{code}

\item[|(map head (reverse x))|] requires |x| to be a list of non-empty lists, or infinite. The restriction to an infinite list is expressed as:

    \begin{code}
    {(:) Any} :* {(:) Any}
    \end{code}
\end{description}

While RE-constraints have simplification rules, MP-constraints have a normalisation procedure. Normalisation takes an MP-constraint and returns one which is equivalent, but in a more standard form -- for example in some cases multiple matches can be collapsed into one. In a similar manner to the RE-constraints, these allow a fixed point to be found faster.

\subsection{Multipattern Propositions}

The MP-constraints have different power to the RE-constraints, but their primary advantage comes when they are combined into propositions. The advantage is that two constraints on the same expression are combined at the proposition level, they can be reduced into one constraint:

\begin{code}
(e :< v1) || (e :< v2) = e :< (v1 ++ v2)
(e :< v1) && (e :< v2) = e :< [a `mergeVal` b | a <- v1, b <- v2]
\end{code}

This ability to combine constraints on equal expressions can be exploited further by translating the program to be analysed. After the \C{backs} function has operated, all constraints will be in terms of the arguments to a function -- if all functions took exactly one argument then all expressions could be collapsed. By placing all the arguments to a function into a tuple, a function can be reduced to one with only a single argument. Taking an example:

\begin{code}
(||) x y = case  x of
                 True   -> True
                 False  -> y
\end{code}

\noindent can be translated into:

\begin{code}
(||) a = case  a of
               (x,y) -> case  x of
                              True    -> True
                              False   -> y
\end{code}

Combining the MP-constraint reduction rules, along with the tupling transformation, a \C{Prop (Req VarName)} is now equivalent in power to \C{Req VarName}. This simplification reduces the number of different propositional values, and obtains a fixed point faster.

In the RE-constraint system the tupling transformation would do no harm, but would be of no benefit either. None of the simplification rules are able to optimise between discrete components in a tuple.

\subsection{Comparison of Constraints}

Both RE-constraints and MP-constraints are capable of expressing a wide range of values, but neither subsumes the other. We give examples where one constraint language can differentiate between a pair of values, and the other cannot.

\begin{example}
Take the following two values: |(T:[])| and |(T:T:[])|, using the enumeration based constraint of |{(:) Any} :* {[]}| the first matches but the second does not. Using propositions over RE-constraints there is no way to differentiate between these two values.
\end{example}

\begin{example}
Taking a tree data structure:

\begin{code}
data Tree alpha  =  Branch {left :: Tree alpha, right :: Tree alpha}
                 |  Leaf alpha

v1 = Branch (Leaf True   ) (Leaf False  )
v2 = Branch (Leaf False  ) (Leaf True   )
\end{code}

The RE-constraint |(left^* :- True)| matches |v1| but not |v2|. Using enumeration based constraints, there is no way to differentiate between these two values.
\end{example}

\catch07 uses MP-constraints, as they scale to much larger examples.

\section{Converting Haskell to Core}
\label{sec:transform}

The full Haskell language is unwieldy for analysis. As noted in \S\ref{sec:core}, the checker works instead on a simplified language, a core to which other Haskell programs can be reduced.

\subsection{Yhc Core}

To generate core representations of programs, it is natural to start with a full Haskell compiler, and we chose Yhc \citep{Yhc}, a fork of nhc \citep{nhc}. The internal language of Yhc is called PosLambda -- a simple variant of lambda calculus without types, but with source position information. Yhc works by applying basic desugaring transformations, without optimisation. This simplicity ensures the generated PosLambda is close to the original Haskell in its structure. Each top-level function in a source file maps to a top-level function in the generated PosLambda, retaining the same name.

However, PosLambda has constructs that have no direct representation in Haskell. For example, there is a FatBar construct \cite{spj:implementation}, used for compiling pattern matches which require fall through behaviour. The PosLambda language was intended only as an internal representation, and exposes certain details that are specific to the compiler. We have therefore introduced a new Core language to Yhc on to which PosLambda can easily be mapped. This Core language is not explicitly typed, and has few constructs. We have also written a library, Yhc.Core, which is used by Yhc to generate these Core files, and by external programs to load and manipulate the generated Core.

The importance of this Core language is not what remains, but what has been removed.

\begin{itemize}
\item No syntactic sugar such as list comprehensions, |do| notation etc.
\item Only simple |case| expressions, matching only the top level constructor
\item All |case| expressions are complete, including an \C{error} call if necessary
\item All names are fully qualified
\item Haskell's type classes have been removed (see \ref{sec:dict})
\item Only top-level functions remain, all local functions have been lambda lifted
\item All constructor applications are fully saturated
\end{itemize}

\subsection{The Dictionary Transformation}
\label{sec:dict}

Most of the transformations in Yhc operate at a local level, within a single function definition. The only analysis/transformation phases which require information about more than one function are type checking and the dictionary transformation, used to implement type classes \citep{wadler:type_classes}.

\begin{example}
The definition

\begin{code}
f :: Eq alpha => alpha -> alpha -> Bool
f x y = x == y || x /= y
\end{code}

\noindent is translated by Yhc into

\begin{code}
f :: (alpha -> alpha -> Bool, alpha -> alpha -> Bool) -> alpha -> alpha -> Bool
f dict x y = (||) (((==) dict) x y) (((/=) dict) x y)

(==) (a,b) = a
(/=) (a,b) = b
\end{code}

The \C{Eq} class consists of two functions, |(==)| and |(/=)|. Depending on the type of the |alpha| variable in the function \C{f}, different code will be executed for the two functions.
\end{example}

The dictionary transformation generates a tuple, containing the functions to use for the type class, and passes this structure into all functions requiring this type class. The additional argument is the dictionary, with functions being looked up in the dictionary to obtain the correct behaviour at runtime.

The dictionary transformation is a global transformation. In Example \lastexample{} the \C{Eq} context in \C{f} not only requires a dictionary to be accepted by \C{f}; it requires all the callers of \C{f} to pass a dictionary as well.

The type class mechanism in Haskell allows certain classes to require other classes. For example, \C{Ord} requires \C{Eq} for the same type. In such cases the dictionary transformation generates a nested tuple, the \C{Eq} dictionary is a component of the \C{Ord} dictionary.

An alternative approach to implementing type classes is given in \cite{jones:dictionary_free}, which does not rely on higher order functions. While this approach would probably suit \catch{} better, it is not the method implemented in Yhc, which already does the dictionary transformation. The dictionary transformation along with the higher order removal performed by \catch{} is similar to this approach.

\subsection{First Order Haskell}

Two further features of Haskell may complicate analysis: laziness and higher-order functions. It is possible to convert Haskell programs to strict equivalents by using continuation passing, and to first-order equivalents by defunctionalization, both are detailed in \citet{reynolds:defunc}. For the purposes of this checker, laziness is a nice property, as it allows $\beta$-reduction to be performed. Higher order functions are not handled by \catch{} -- their removal is required.

\subsection{Reynold's style defunctionalization}

Reynold's style defunctionalization \citep{reynolds:defunc} is a method for generating a first-order equivalent of a higher-order program.

\begin{example}
\begin{code}
map fn x = case  x of
                 []      -> []
                 (a:as)  -> fn a : map fn as
\end{code}

Defunctionalization works by creating a data type to represent all values that |fn| may take.

\begin{code}
data Functions = Head | Tail

apply Head  x = head  x
apply Tail  x = tail  x

map fn x = case  x of
                 []      -> []
                 (a:as)  -> apply fn a : map fn as
\end{code}

Now all calls to |map head| are replaced by |map Head|, and \C{Head} is a first order value.
\end{example}

\begin{example}
The scheme easily extendeds to currying. Take the \C{add} function, which adds two \C{Int}'s:

\begin{code}
data Functions = Head | Tail | Add0 | Add1 Int

apply Add0      n  = Add1 n
apply (Add1 a)  b  = add a b
\end{code}
\end{example}

Although defunctionalized code is still type safe, to do type checking would require a dependently typed language. This presents no problem for \catch{}, which does not use type information. The unacceptable aspect is the creation of an \C{apply} function, whose meaning is excessively general. This transformation introduces a bottleneck through which various properties must be proven. Asking questions, such as is the result of \C{apply} a |[]| value, require a lot of computation.

Reynold's style defunctionalization is suitable for use by \catch{}, but the introduction of \C{apply} means that defunctionalization is not the preferred method.

\subsection{Specialisation}

\catch05 used a different technique to remove higher-order functions: specialisation. A mutually recursive group of functions can be specialised in their $n$th argument if in all recursive calls this argument is invariant. Examples of common functions whose applications can be specialised include \C{map}, \C{filter}, \C{foldr} and \C{foldl}.

When a function can be specialised, the expression passed as the $n$th argument has all its free variables passed as extra arguments, and is expanded in the specialised version. All recursive calls within the new function are then renamed.

\begin{example}
\begin{code}
map fn xs = case  xs of
                  []      -> []
                  (a:as)  -> fn a : map fn as

adds x n = map (add n) x
\end{code}

\noindent is transformed into:

\begin{code}
map_adds n xs = case  xs of
                      []      -> []
                      (a:as)  -> add n a : map_adds n as

adds x n = map_adds n x
\end{code}
\end{example}

Although specialisation is not complete by any means, it is sufficient for a large range of examples. This approach has the unfortunate effect of specialising more than just higher order functions, which can lead to obscured code. It also cannot cope with point-free code, and does not deal with many forms of dictionaries.

\subsection{Specialisation with Inlining}

The power of specialisation is greatly increased if it is combined with inlining, and restricted to only target higher order functions. Although this technique is not complete, in practice we have yet to encounter an example where it fails, excluding nonsense examples created specifically to break this approach.

\begin{figure}
\begin{code}
data Expr  =  ... -- as in Figure {\ref{fig:core}}
           |  Part   Int FuncName [Expr]
           |  Apply  Expr [Expr]

Part 0 fn xs == Call fn xs
Apply (Part n fn xs) ys == Part (n - length ys) fn (xs ++ ys)
\end{code}
\caption{Augmented Core syntax.}
\label{fig:core_ho}
\end{figure}

In order to permit a higher order program to be represented, the Core language is augmented with additional constructs, as shown in Figure \ref{fig:core_ho}, along with two identities. The \C{Apply} allows an unsaturated function call, or a variable to be used as the function. The \C{Part} constructor is used to represent unsaturated function calls, leaving the normal \C{Call} constructor to represent saturated calls. A \C{Part} construction records how many arguments are needed.

\begin{figure}
\begin{code}
isHO :: Expr -> Bool
isHO (Part n _ _)    = n > 0
isHO (Make _ xs)     = any isHO xs
isHO (Case on alts)  = any (isHO . snd) alts
isHO _               = False
\end{code}
\caption{Tests for the firstifier.}
\label{fig:isHO}
\end{figure}

The algorithm has two components, specialisation and inlining. The algorithm applies the specialise rule until a fixed point is reached, then applies the inline rule once. The algorithm repeats these two steps until a fixed point is reached. Given an appropriate \C{fix} function, \C{firstify} can be implemented as:

\begin{code}
firstify :: Program -> Program
firstify = fix (inline . fix specialise)
\end{code}

\begin{description}
\item[The \C{inline}] stage of the algorithm is simple. A function call is inlined if the body of the called function passes the \C{isHO} test, defined in Figure \ref{fig:isHO}. If this process causes a function to no longer be called from the root of the program, then the function is removed after inlining.

\item[The \C{specialise}] stage of the algorithm takes expressions of the form |Call fn xs| where |any isHO xs|, and generates a specialised version of the function |fn| where all the function arguments in |xs| are frozen in, and all the others are passed normally.

    Assuming that a function |fn| results in the specialised version |fn2|, then the translation would be:

    \begin{code}
    transform (Call fn xs) | any isHO xs =
        Call fn' (concatMap f xs)
        where
        fn' = generate fn xs

        f x = if isHO x then freeVars x else [x]
    \end{code}
\end{description}

This technique is powerful. The fact that functional arguments have ``nowhere to hide'' can be seen easily. Eiether the entire body is an application of the functional argument (in which case it will be inlined), or it must occur as the argument to a function (in which case it is specialised). There are only two places left for functional arguments to be used:

\begin{enumerate}
\item As the subject of a |case| expression. This situation is not possible due to static typing -- all |case| expressions must choose over a data value.

\item Inside an \C{Apply} with another argument variable as the function. This situation does not happen if all other functional arguments have been removed. \todo{Mutual application, circular reasoning, non-terminating proof!}
\end{enumerate}

One tricky issue is \C{undefined}, consider the program:

\begin{code}
main = f undefined
f x = x 1
\end{code}

This program passes \C{undefined} as a function, which is allowable as \C{undefined} has a fully polymorphic result. This problem can be worked around by modifying \C{isHO} to treat \C{undefined} (and by extension, \C{error}) as higher order functions, ensuring they are specialised and inlined.

Finally, it is possible to construct a program with an infinite number of specialisations:

\begin{code}
data Wrap alpha = Wrap (Wrap alpha) | Value alpha

main = f head

f x = f (Wrap x)
\end{code}

This pattern requires functions to be placed in a recursive data structure, and an accumulating function to operate over them. In reality, functions are rarely placed in any data structure other than a tuple. This situation can be detected, and a fallback to Reynold's style defunctionalization is possible.

\section{Results and Evaluation}
\label{sec:results}

\todo{Mention the price of safety bit}

The best way to see the power of Catch is by example. While introducing the constraints in \S\ref{sec:small_examples} I gave a few micro-examples. \S\ref{sec:safety} discusses some of the general issues when modifying programs to obtain provable safety. \S\ref{sec:imaginary} investigates all the examples from the Imaginary section of Nofib, \S\ref{sec:spectral} gives results from a few larger examples chosen from the Spectral section of Nofib.

\subsection{The Price of Safety}
\label{sec:safety}

Take the following example:

\begin{code}
average xs = sum xs / length xs
\end{code}

In \C{average}, if |xs| is |[]| then a division by zero would occur. In \catch{}, division by zero is modelled as a pattern match error. One small local change could be made which would remove this pattern match error:

\begin{code}
average xs = if null xs then 0 else sum xs / length xs
\end{code}

In this modified version, if |x| is |[]|, then the program will simply return 0, and no pattern match error will occur. While this program is now safe, this safety has come at a cost -- the code no longer behaves as it did before -- replacing $\bot{}$ with the non-sensical value 0.

In general pattern match errors can be handled in two manners:

\begin{description}
\item [Locally:] The above example shows a local fix, in the same function that calls the division, add a special test. This fix is fairly simple, and requires only small code changes.

\item [Globally:] By investigating the code that calls \C{average}, and ensuring that |[]| is not passed as the first argument. This fix requires a deeper understanding of the flow of the program, and depending on the overall design, may require substantial work. The end result is likely to be a cleaner body of code.
\end{description}

For programs which were not design with pattern match safety in mind (see Gen-Regexps in \S\ref{sec:imaginary}), the global changes may constitute an entire reimplementation. In the following sections, where modifications were required, we have attempted to make the minimum number of changes. Consequently, we have used local fixes.

\subsection{The Imaginary Nofib Benchmark}

The whole Nofib benchmark \citep{nofib} is too large for the authors to take the time to investigate all the programs, so instead the `Imaginary' section has been chosen as our primary focus. These programs are all under a page of text, and particularly stress list operations and numeric computations. All the benchmark code is available online\footnote{\texttt{http://darcs.haskell.org/nofib/imaginary/}, as of February 2007}.

To take a typical benchmark, Primes, the \C{main} function is:

\begin{code}
main = do  [arg] <- getArgs
           print $ primes !! (read arg)
\end{code} % $

The first unsafe pattern here is |[arg] <- getArgs|, as \C{getArgs} is a primitive whose implementation is opaque to the system. With this expression, \catch{} will always return False as the precondition to \C{main}.

The next step that may fail is when \C{read} is applied to an argument extracted from getArgs. This argument is entirely unknown, and \C{read} is a sufficiently complicated function that although it can be modelled by Catch, there is no possibility for getting an appropriate abstraction which models the failure case. As a result, any program which calls \C{read} is unsafe, according to \catch{}!. Using \C{reads}, which indicates failure properly, can allow a program to still be checked.

As a result the program have been rewritten, and \catch{} is directed to check from the \C{compute} function:

\begin{code}
compute x = print $ primes !! x
\end{code}

\begin{table}
\begin{tabular}{lrrrrrrlll}
Program               & Src & Core & First & \C{error} & Pre & sec & Mb \\
\vspace{-2.2ex} \\
\hline
\vspace{-1.8ex} \\
\textbf{Bernoulli}    & 35 & 1616 &  652 & 5 & 11 & 1.3 & 0.8 \\
\textbf{Digits of E1} & 44 &  957 &  377 & 3 &  8 & 1.2 & 0.6 \\
Digits of E2          & 54 & 1179 &  455 & 5 & 19 & 0.9 & 0.8 \\
Exp3-8                & 29 &  220 &  163 & 0 &  0 & 0.1 & 0.1 \\
\textbf{Gen-Regexps}  & 41 & 1006 &  776 & 1 &  1 & 0.5 & 0.4 \\
Integrate             & 39 & 2466 &  364 & 3 &  3 & 0.3 & 1.9 \\
\textbf{Paraffins}    & 91 & 2627 & 1153 & 2 &  2 & 1.6 & 1.9 \\
Primes                & 16 &  302 &  241 & 6 & 13 & 1.9 & 0.1 \\
Queens                & 16 &  648 &  283 & 0 &  0 & 0.2 & 0.2 \\
Rfib                  &  9 & 1918 &  100 & 0 &  0 & 0.1 & 1.7 \\
Tak                   & 12 &  209 &  155 & 0 &  0 & 0.1 & 0.1 \\
Wheel Sieve 1         & 37 & 1221 &  570 \\
Wheel Sieve 2         & 45 & 1397 &  636 \\
X2n1                  & 10 & 2637 &  331 & 2 &  5 & 2.5 & 1.9 \\
\end{tabular}
\caption{Table of results}
\label{tab:results}
\end{table}

Catch additionally produces a log containing the entire program in Core form before analysis begins, including the effect of abstraction and firstification, a list of all the template axioms calculated and the preconditions on every function.

Results are given in Table \ref{tab:results}. Where changes were required to obtain the correct precondition, the name of the program is in bold, and will be discussed separately. \catch{} produces a log containing the entire program in Core form before analysis begins, including the effect of abstraction and firstification, a list of all the properties calculated and the preconditions on every function -- this information gives the other statistics.

\begin{description}
\item[Source] is the number of lines in the original source code.
\item[Core] is the total number of lines of Yhc Core, including all functions used from libraries, when pretty printed.
\item[First-order] is the number of lines after firstification, just before analysis.
\item[\C{error}] is the number of calls to error.
\item[Pre] is the number of functions which have a precondition which is not True.
\item[Sec] is the time in seconds taken for transformations and analysis, excluding the standard Yhc compilation time. No great effort has been put into the optimisation of the analysis code.
\item[Mb] is the maximum residency of the program at garbage collection time, in Mb. Typically a program will consume twice this size in system memory.
\end{description}

Four programs use the |(!!)| function, which requires the index to be non-negative and less than the length of the list. \catch{} can only prove this condition if the list being indexed on is infinite. Eight programs include applications of either \C{head} or \C{tail}, most of which can be proven safe. Seven programs have incomplete patterns, often in a |where| binding -- Catch performs well on these. Nine programs use division, with the precondition that the divisor must not be zero. Most of these can be proven safe. Only four programs contain no calls to \C{error}.

Three programs have preconditions on the \C{main} function, all of which state that the argument must be a natural number. In all cases the generated precondition is minimal -- if the input violates the precondition then pattern match failure will occur.

We will now discuss the four programs which required changes, along with the Digits of E2 program -- a program with complex pattern matching that \catch{} is able to prove safe.

\paragraph{Bernoulli}

This program has one instance of |tail (tail x)|, which is beyond the capabilities of \catch{}. Consider the example program and generated condition:

\begin{code}
main x = tail (tail x)
> main, x :< {(:) Any} :* {(:) Any}
\end{code}

The MP-constraints are unable to express that a list must be of at least length two, so conservatively strengthens this to the condition that the list must be infinite -- a condition that Bernoulli does not satisfy.

Replacing |tail (tail x)| with |drop 2 x| allows all other pattern matches to be proven. Even with this change, the program still has a high level of non-exhaustive pattern matches -- all of which can be proven safe.


\paragraph{Digits of E1}

This program contains the following equation:

\begin{code}
ratTrans (a,b,c,d) xs |
  ((signum c == signum d) || (abs c < abs d)) &&
  (c+d)*q <= a+b && (c+d)*q + (c+d) > a+b
     = q:ratTrans (c,d,a-q*c,b-q*d) xs
  where q = b `div` d
\end{code}

Catch is able to prove that the division by |d| is only unsafe if both |c| and |d| are zero. Proving that this invariant is maintained is beyond the capabilities of either Catch, or the authors of this paper. A slight modification is possible:

\begin{code}
  where q = if d == 0 then 0 else b `div` d
\end{code}

This expression gives the same behavior where the previous program would have, but $\bot{}$ is now replaced by a defined (although potentially nonsensical) value.

\paragraph{Gen-Regexps}

This program expects valid regular expressions as input. Ways of crashing the program include entering |""|, |"["|, |"<"| and lots of other inputs.

One potential error comes from |head . lines|, which can be replaced by |takeWhile (/= '\n')| to have the same effect, apart from on the empty list, where it returns |""|.

One pattern match error is can be fixed by adding a default clause.

Two errors of the form |(a,_:b) = span f x| can be fixed by changing them to |(a,b) = safeSpan f x|, where \C{safeSpan} is defined as:

\begin{code}
safeSpan p x = (a, drop 1 b)
    where (a,b) = span p x
\end{code}

While at first glance this pattern looks like a simple pattern match against |(:)|, much in the same way as \C{head}, the reality is much more complex. The pattern is only safe if for one of the elements in the list |x|, |f| returns True. In the particular example of Gen-Regexps, the test |f| is actually |(/= '-')|. Using the abstraction for |Char| as detailed in \S\ref{sec:abstraction}, |(/=)| over |Char| always has an unknown result, making this pattern impossible to prove safe.

This example shows that a program written with no regard for pattern match safety can be made safe using local fixes, without excessive code alterations.

\paragraph{Paraffins}

The Paraffins program cannot be validated by Catch, without significant modification. There are two reasons for this:

\begin{description}
\item[Laziness] The first error, which can be fixed easily, is the use of \C{undefined} to prevent a space leak:

    \begin{code}
    radical_generator n = radicals undefined
      where radicals unused = big_memory_computation
    \end{code}

    In Haskell if a function is defined with no arguments it is considered a constant applicative form (CAF), and has its results computed only once in the program. To prevent something which allocates a lot of memory from having this behaviour, a dummy argument can be passed. The \C{undefined} is merely serving as a dummy argument, but if evaluated would result in a pattern match error. If the analysis was lazy (as discussed in \S\ref{sec:precond}) then this example would succeed using \catch{}. As it is, simply changing \C{undefined} to |()| removes the potential error.

\item[Arrays] The Paraffins program uses arrays. The function |array :: Ix a => (a, a) -> [(a, b)] -> Array a b| takes a list of index/value pairs and builds an array. The precondition on this function is that all indexes must be in the range specified and all arguments must be valid. This precondition is too complex for Catch. But simply using \C{listArray}, which takes a list of elements one after another, the program can be validated. (Also, the use of \C{listArray} makes the program shorter and more readable.)

    Use of the array indexing operator |(!)| is also troublesome. The precondition requires that the index is in the bounds given when the array was constructed, something \catch{} does not currently model.
\end{description}

The Paraffins program does highlight that one weak area of \catch{} is code that uses arrays.

\paragraph{Digits of E2}

This program is quite complex, featuring a number of possible pattern match errors. To illustrate, consider the following fragment:

\begin{code}
  where  carryguess = d `div` base
         remainder = d `mod` base
         nextcarry:fraction = carryPropagate (base+1) ds
\end{code}

Here there are three potential pattern match errors in as many lines. Two of these are the calls to |div| and |mod|, both requiring |base| to be non-zero. A possibly more subtle pattern match error is the |nextcarry:fraction| left-hand side of the third line. \catch{} is able to prove that no pattern match fails.

\begin{code}
e =  ("2."++) $
     tail . concat $
     map (show.head) $
     iterate (carryPropagate 2 . map (10*) . tail) $
     2:[1,1..]
\end{code}

There are two calls to \C{tail}, and one to \C{head}. They are mixed in a relatively complex manner, including as a higher order functional pipeline in the case of \C{iterate} and \C{map}. \catch{} is able to prove that no pattern match fails, and that \C{head} and \C{tail} have their preconditions respected.

\subsection{The Spectral Nofib Benchmark}
\label{sec:spectral}

\todo{Write this section, get some results}

The `Spectral' section of the Nofib benchmark contains the computational kernels from real program.

The Mandel2 program has 5 divisions, some of which are by the constant 2, others are by a global variable.

2 other programs are also safe.

\section{Related Work}
\label{sec:related}

\todo{Got up to here!}

\subsection{Proving Incomplete Patterns Safe}

Despite the seriousness of the problem of pattern matching, the topic of pattern match error detection and elimination has not been studied extensively. The two papers which offer the most promising start in this area are the approach by \cite{me:catch_tfp} and ESC/Haskell \citep{esc_haskell}. The first paper attempts to define an automatic inference that pattern matches are safe, similar to the analysis presented in this paper. Unfortunately the restrictions in that paper make it an interesting starting point, but nothing more. The analysis in this paper differs radically in many design decisions -- particularly in relation to fixed points and constraints. Perhaps the introductory risers example is most clearly illustrates the differences -- for the original paper risers was towards the boundary of what was possible, for this paper it is trivial.

The ESC/Haskell approach requires the programmer to give explicit preconditions and contracts which the program obeys. This approach is likely to result in a checker with more expressive power in the end (one of the examples involves an invariant on an ordered list, something beyond Catch), but requires the programmer to spend more time doing the validation. It should be noted that the preconditions derived by Catch, along with the properties, could easily be turned into ESC/Haskell annotations, allowing both tools to be used together.

\subsection{Eliminating Incomplete Patterns}

One simple way to guarantee that a program does not crash with an incomplete pattern is to ensure that there are no pattern match errors in the code. The GHC compiler  \citep{ghc} has a warning flag to detect incomplete patterns, named \T{-fwarn-incomplete-patterns}. Adding this flag when compiling risers results in a message that ``Pattern matches are non-exhaustive''. Unfortunately the Bugs (12.2.1) section of the manual notes that the checks are sometimes wrong, particularly with string patterns or guards, and that this part of the compiler ``needs an overhaul really'' \citep{ghc_manual}. But the GHC checks are only local. Using an incomplete function from a library, such as \C{head}, gives no warning. If the function \C{head} is defined, then it raises a warning.

There have been attempts at defining what it means for a program to be complete with regard to pattern matches \citep{maranget:pattern_warnings}, along with associated implementations.

One approach for programming is to design a functional language without pattern match errors or non-termination \cite{turner:total}. In this approach incomplete pattern matches are banned, suggesting this restriction will ``force you to pay attention to exactly those corner cases which are likely to cause trouble''. The results in \S\ref{sec:nofib} show that in sample Haskell programs there are a reasonably high number of incomplete pattern matches, which are still safe. Using the Catch tool this restriction could be reduced.

\subsection{A Mistake Detector}

One tool in most Haskell programmers arsenals is the QuickCheck tool \citep{quickcheck}. This tool allows properties to be specified which are randomly tests. In many cases these properties may be similar to those derived by the templating mechanism -- if the Catch tool had a way of directly entering these conditions then perhaps the simpler properties could be proved automatically.

There has been a long history of writing tools to analyse programs to detect potential bugs, going back to the classic C Lint tool \citep{lint}. In the functional arena the Dialyzer \citep{dialyzer} for Erlang \citep{erlang} performs a similar function. The aim is to have a static checker that works on unmodified code, with no additional annotations. However, a key difference is that in Dialyzer all warnings indicate a genuine problem that needs to be fixed. Because Erlang is a dynamically typed language, a large proportion of Dialyzer's warnings relate to mistakes a type checker would have detected.

Part of the mistake detection aspect can be seen as defining a slightly more restrictive type system. In this light the checker can be compared to the tree automata work done on XML \citep{xml} and XSL \citep{static_xslt}, which can be seen as an algebraic data type and a functional language. Another soft typing system with similarities is by Aiken \cite{aiken:type_infer}, on the functional language FL. This system tries to assign a type to each function using a set of constructors, for example \T{head} is given just \T{Cons} and not \T{Nil}.

\subsection{Type System Safety}

\begin{figure}
\begin{code}
data Cons; data Unknown
newtype List a t = List [a]

nil :: List a Unknown
nil = List []

cons :: a -> [a] -> List a Cons
cons a as = List (a:as)

fromList :: [a] -> List a Unknown
fromList xs = List xs

safeTail :: List a Cons -> a
safeTail (List (a:as)) = as
\end{code}
\caption{A safe \C{head} function with Phantom types.}
\label{fig:phantom}
\end{figure}

One method which is frequently used to encode invariants on data in functional languages is the type system. One approach is the use of Phantom types \citep{fluet:phantom}, for example a safe variant of \C{head} can be written as in Figure \ref{fig:phantom}. In this example the \C{List} data structure would not be exported, ensuring that all lists with a \C{Cons} tag are indeed non-empty. The values \C{Cons} and \C{Unknown} are phantom types -- they exist only in the type level, not the value level.

\begin{figure}
\begin{code}
data ConsT a; data NilT

data List a t where
    Cons  :: a -> List a b -> List a (ConsT b)
    Nil   :: List a NilT

safeTail :: List a (ConsT t) -> List a t
safeTail (Cons a b) = b

fromList :: [a] -> (forall t. List a t -> r) -> r
fromList []      fn = fn Nil
fromList (x:xs)  fn = fromList xs (\sl -> fn (Cons x sl))
\end{code}
\caption{A safe \C{tail} function using GADT's.}
\label{fig:gadt}
\end{figure}

Another method of encoding addition type information which is becoming increasingly popular in Haskell is the use of GADT's \citep{gadt}. Using this technique, sometimes referred to as first class Phantom Types, an encoding of lists can be written as in Figure \ref{fig:gadt}. Unlike the first method, the \C{fromList} method requires the complexity of existential types.

The type directed method can be pushed much further with dependant types, which allow types to depend on values. This approach allows safe versions of \C{head} and \C{tail} to be written. There has been much work on type systems, using undecidable type systems \citep{cayenne, epigram}, using extensible kinds \citep{omega} and using type systems resctricted to a decidable fragment \citep{xi:dependent_practical}.

The downside to all these type systems is that they require the programmer to make explicit annotations, and require the user to learn new techniques for computation.


\section{Conclusions and Future Work}
\label{sec:conclusion}

A static checker for potential pattern-match errors in Haskell has been specified and implemented. This checker is capable of determining preconditions under which a program with non-exhaustive patterns executes without failing due to a pattern-match error. A section of the Nofib suite has been tested, with encouraging results -- all but one program can be proved safe, most without any modification at all.

There are two main avenues of future work to extend to the mechanisms introduced in this paper. There first is to extend the power of the checker. Possible directions to increase the power include the addition of explicit annotations of properties, or a more powerful constraint language. The constraint languages introduced could be augmented with special purpose constraints, designed to tackle particular problems. One particular constraint system that may prove fruitful is a linear inequality constraint system.

The next direction for future work would be using the Catch tool to solve other problems. The direct result of the absence of pattern match failure could be used to feed information to an optimising compiler. The properties generated could also be used to generate more efficient code. If the programmer was able to specify more restrictive preconditions, or properties, or invariants, then these may be able checked with Catch. If the power of the constraint system was increased then richer program annotations could be used.

As the Catch tool stands, it is already capable of detecting and proving the absence of pattern match failures in sample programs. We hope this tool will become a part of the standard arsenal of Haskell programmers.


% \appendix
% \section{Appendix Title}
%
% Here is the text of the appendix, if you need one.

\acks

The first author is a PhD student supported by a studentship from the Engineering and Physical Sciences Research Council of the UK.

\bibliographystyle{plainnat}
\bibliography{catch}



\end{document}
