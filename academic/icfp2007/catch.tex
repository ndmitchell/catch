\documentclass[preprint]{sigplanconf}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{alltt}
\usepackage{url}
\usepackage{natbib}
\usepackage{datetime}
\usepackage{comment}

%include polycode.fmt
%include catch.fmt

% general stuff
\newcommand{\T}[1]{\texttt{#1}}
\newcommand{\C}[1]{\textsf{#1}}
\newcommand{\tup}[1]{\ensuremath{\langle #1 \rangle}}
\newcommand{\mtxt}[1]{\textsf{#1}}

% examples
\newcounter{exmp}
\setcounter{exmp}{1}
\newcommand{\yesexample}{\subsubsection*{Example \arabic{exmp}}\addtocounter{exmp}{1}}
\newcommand{\noexample}{\hfill$\Box$}

\newcommand{\todo}[1]{\textbf{\textsc{Todo:} #1}}

% code blocks
\newenvironment{code}{\begin{alltt}\small}{\end{alltt}}
\newenvironment{codepage}
    {\begin{minipage}[h]{\textwidth}\begin{code}}
    {\end{code}\end{minipage}}

\newcommand{\K}{\ensuremath{^\ast}} % kleene star
\newcommand{\D}{\ensuremath{\cdot}} % central dot

\renewcommand{\c}[3]{\tup{\T{#1},\T{#2},\T{\{#3\}}}}
\newcommand{\cc}[2]{\c{#1}{$\lambda$}{#2}}

\newcommand{\s}[1]{\ensuremath{_{\tt #1}}} % subscript, in tt font
\newcommand{\g}[1]{\{#1\}} % group, put { } round it
\newcommand{\U}{\textunderscore}
\newcommand{\vecto}[1]{\overrightarrow{#1\;}}
\newcommand{\gap}{\;\;}
\newcommand{\dom}{\text{dom}}


\begin{document}

\conferenceinfo{ICFP '07}{date, City.} %
\copyrightyear{2007} %
\copyrightdata{[to be supplied]}

\titlebanner{\today{} - \currenttime{}}        % These are ignored unless
\preprintfooter{Catch: A Technical Overview}   % 'preprint' option specified.

\title{Catch}
\subtitle{A Technical Overview}

\authorinfo{Neil Mitchell}
           {York}
           {ndm}
\authorinfo{Colin Runciman}
           {York}
           {colin}

\maketitle

\begin{abstract}
A Haskell program may fail at runtime with a pattern-match error if the program has any incomplete (non-exhaustive) patterns in definitions or case alternatives. This paper describes a static checker that allows non-exhaustive patterns to exist, yet ensures that a pattern-match error does not occur. It describes a constraint language that can be used to reason about pattern matches, along with mechanisms to propagate these constraints between program components.

I claim an achievement, I can successfully check 94\% of the Nofib suite, and the reasons for the rest failing are discussed.
\end{abstract}

% \category{CR-number}{subcategory}{third-level}

% \terms
% term1, term2

% \keywords
% keyword1, keyword2

\section{Introduction}
\label{sec:introduction}

Often it is useful to define pattern matches which are incomplete, for example \T{head} fails on the empty list. Unfortunately programs with incomplete pattern matches may fail at runtime.

Consider the following example:

\begin{code}
risers :: Ord alpha => [alpha] -> [[alpha]]
risers [] = []
risers [x] = [[x]]
risers (x:y:etc) = if x <= y then (x:s):ss else [x]:(s:ss)
    where (s:ss) = risers (y:etc)
\end{code}

A sample execution of this function would be:

\begin{code}
> risers [1,2,3,1,2]
[[1,2,3],[1,2]]
\end{code}

In the last line of the definition, |(s:ss)| is matched against the output of \C{risers}. If |risers (y:etc)| returns an empty list this would cause a pattern match error. It takes a few moments to check this program manually -- and a few more to be sure one has not made a mistake!

GHC \citep{ghc_manual} 6.6 has a warning flag to detect incomplete patterns, which is named \T{-fwarn-incomplete-patterns}. Adding this flag at compile time reports:

\begin{code}
Warning: Pattern match(es) are non-exhaustive
\end{code}

The Bugs (12.2.1) section of the manual notes that the checks are sometimes wrong, particularly with string patterns or guards, and that this part of the compiler ``needs an overhaul really'' \citep{ghc_manual}.

But the GHC checks are only local. If the function \C{head} is defined, then it raises a warning. No effort is made to check the \textit{callers} of \C{head} -- this is an obligation left to the programmer.

Turning the \C{risers} function over to the checker developed in this paper, the output is:

\begin{code}
> Safe
\end{code}

In addition the checker produces a set of axioms it has proved about Risers, along with an outline of the proof. Fuller details of how the checking is performed follow in Section \ref{sec:walkthrough}.

\subsection{Contributions}

This is not the first paper on pattern match checking, indeed this system owes a lot of its heritage to \citet{catch_tfp}. A quick glance at the future work section of that original paper shows that all the issues listed are addressed in this paper. The original version of Catch followed a similar motivation, but was unable to handle full Haskell, and had a particularly unsatisfactory story when it came to type classes and higher order functions -- both things now dealt with convincingly. In terms of the underlying analysis, the constraint language has been changed, the fixed pointing method is entirely different and the success on moderately complex examples is massively increased.

While the original version of Catch shared motivation with this paper, the underlying mechanism have changed entirely, leading to a much more powerful tool.

\subsection{Road map}

This paper first takes the reader on an overview walkthrough of checking the \C{risers} function \S\ref{sec:walkthrough}. Next we introduce a small core functional language, along with a mechanism for reasoning about this language \S\ref{sec:manipulate}, and a constraint language \S\ref{sec:constraint}. Next we discuss how to transform Haskell to this core language \S\ref{sec:transform}.

Having described the underlying system, we move on to an evaluation on various sample programs -- including the \textsf{nofib} benchmark suite \S\ref{sec:results}. We compare our work to other work \S\ref{sec:related} before presenting some concluding remarks \S\ref{sec:conclusion}.

\section{Walkthrough of Risers}
\label{sec:walkthrough}

This section details all the stages that are executed in order to generate a proof that the \C{risers} function in the Introduction does not crash with a pattern match error. No one area is examined in detail, but an overall flavor is given.

\subsection{Modifying Risers for analysis}

There is one restriction on the functions that Catch can analyse -- they must not take any explicitly higher order parameters. The reason for this is to reduce the complexity in the analysis engine. Removing the ability to pass functions as input removes the need to state conditions on functions.

In Haskell \citep{haskell},type classes are typically implemented \citep{type_classes} as dictionaries, meaning that all functions with a type class have a dictionary of higher order functions passed. This means that the function to analyse must not have any outstanding context.

The type of the \C{risers} function is |risers :: Ord alpha => [alpha] -> [[alpha]]| -- this can easily be made first-order by making a new function \C{main} the root of the analysis, and providing a concrete type:

\begin{code}
main :: [Int] -> [[Int]]
main x = risers x
\end{code}

\subsection{Conversion to a Core Language}

Rather than analyse full Haskell, the Catch tool analyses a first-order Core language, without lambda's (other than at the top level), partial application or let bindings. While this at first seems like a substantial restriction, a convertor is provided from the full Haskell 98 language to this very restricted language. Full details are provided in section \S\ref{sec:transform}.

After converting to Core Haskell, and renaming the identifiers for ease of human reading, the resulting Core is as shown in Figure \ref{fig:risers_core}. The function \C{risers4} and \C{risers2} correspond to the pattern match in the |where|. The function |<=| in this example has been specialised to the Int data type, unlike the standard |(<=)| operator which is a member of the \C{Num} class.

\begin{figure}
\begin{code}
risers3 x y = risers4 (risers (x : y))

risers4 x = case x of
    (y:ys) -> (ys, y)
    [] -> error "Pattern Match Failure, 11:12."

risers x = case x of
    [] -> []
    (y:ys) ->  case ys of
         [] -> (y : []) : []
         (z:zs) -> risers2 (risers3 z zs) (y <= z) y

risers2 x y z =  case y of
    True -> (z : snd x) : (fst x)
    False -> (z : []) : (snd x : fst x)
\end{code}
\caption{\C{risers} in the Core language}
\label{fig:risers_core}
\end{figure}

\subsection{Analysis}

Within the Core language every pattern match covers all possible constructors of the appropriate type -- any patterns which were not originally complete have calls to \C{error} inserted. The analysis first starts by finding calls to \C{error}, then trying to prove that these calls will not be reached. In the above example it is easy to see there is only one \C{error} call, which corresponds to a pattern match error which has been desugared.

The analysis determines that if \C{risers4} is called, the argument must always be a |(:)|-constructed value for the code to be safe. Following the call chain backwards shows that \C{risers3} calls \C{risers4}, passing |(risers (x:y))| as the argument. In this particular case it is possible to see that risers of a |(:)| always results in a |(:)| - requiring in part the knowledge that \C{risers2} always evaluates to a |(:)|. With these axioms it can be shown that the entire program is safe.

Unfortunately the analysis is not blessed with the foresight to know which axioms are required and the direction in which the proof must proceed. The analysis manipulates constraints on expressions until a fixed point is found -- speculatively attempting to discharge constraints as they arise. The exact transformation rules, along with the constraints that can be expressed by the system are the main substance of the analysis.


\section{The Constraints}
\label{sec:manipulate}

This section explains the underlying constraint system in Catch, focusing on how the constraints are put together to express properties of expressions and the data structures they evaluate to. All data structures and equations are presented in Haskell, although should be accessible to all readers.

\subsection{Reduced expression language}
\label{sec:core}

\begin{figure}
\begin{code}
type CtorName  =  String
type FuncName  =  String
type PathName  =  String

data Expr  =  Var String
           |  Path Expr PathName
           |  Make CtorName [Expr]
           |  Call FuncName [Expr]
           |  Case Expr [Alt]

type Alt = (CtorName, Expr)]
\end{code}
\caption{Core Data Type}
\label{fig:core}
\end{figure}

\begin{figure}
\begin{code}
ctors     :: CtorName  -> [CtorName]
arity     :: CtorName  -> Int
paths     :: CtorName  -> [PathName]
pathCtor  :: PathName  -> CtorName
pathPosn  :: PathName  -> Int
isRec     :: PathName  -> Bool
\end{code}
\caption{Miscellaneous functions to manipulate Core}
\end{figure}

The abstract syntax tree for the reduced expression language is given in figure \ref{fig:core}. The Core language chosen for this purpose is like most others, with the exception of paths. The language is first order, has only simple case statements, and only algebraic data types. The evaluation strategy is lazy. All case statements are defined to be complete, with error being introduced where a pattern match error would occur. Every constructor has an arity, which can be obtained with the \C{arity} function. To determine all constructors in a set the \C{ctors} function can be used, for example |ctors "True" = ["False", "True"]| and |ctors "[]" = ["[]", ":"]|.

For analysis purposes we can ignore the default construction for alternatives - in practice this can always be expressed by a set of alternatives with bound constructors. In practice the default alternative is present in Catch, which results in a performance gain.

\subsubsection{Paths}

The one thing present in our Core language, but not in other languages, is the \textit{path}. Some examples of paths include \C{hd} and \C{tl}. A path asserts that a data value has a given constructor, and then follows its appropriate field. The concrete syntax for paths is |x-*tl|. Some examples of paths:

\begin{code}
(1:2:[])-*hd == 1
(1:2:[])-*tl == (2:[])
[]-*hd -- this is statically impossible
\end{code}

A path has a given constructor, and returns a given field member from that constructor -- this information is available using the functions \C{pathCtor} and \C{pathPosn}.

Constructions such as |[]-*hd| are statically disallowed. The original input language to Catch is Haskell, which does not have a path expression, meaning that all path's are introduced by the transformation process. It is the responsibility of the transformations to ensure that invalid paths are not introduced.

To take a simple example, here is the function \C{risers4} from the introductory example, using path's instead of constructors. In general functions are given without the use of paths -- they are an underlying implementation concern which does not change the meaning or understanding of the algorithms.

\begin{code}
risers4 x = case x of
    (:) -> (x-*tl, x-*hd)
    [] -> error "Pattern Match Failure, 11:12."
\end{code}

Our use of paths contrasts with other functional Core language's (for example \citet{ghc_core}). Our choice has a number of merits, in particular they simplify a number of issues. By using path statements, the alternatives in a \C{case} expression do not need to bind variables. This then means that all free variables are declared at the top of a function, and have a scope consisting of the entire function -- since let's expressions are not permitted. The simplification of free variable handling allows a corresponding simplification of the entire analysis. The rules for variables are also simplified -- there are no special cases for variables bound in a case expression.

The name of paths can be automatically generated from a constructor and an index, however if a Haskell program defines a data type with uniquely named fields, then these field names are used. The definition for lists in Catch is:

\begin{code}
data [alpha] = [] | {hd :: alpha, tl :: [alpha]}
\end{code}

\subsubsection{\C{isRec} function}

Detail the purpose of the \C{isRec} function, what it does, and why. Also mention that co-recursive types are disallowed.

Invariant, given a finite typed structure, there must be no chain of fields which are not \C{isRec} one after another.

\subsubsection{Abstraction}

\begin{figure}
\begin{code}
data Int = Neg | Zero | Pos

any0 = primitive
any2 x y = if any0 then x else y
any3 x y z = any2 x (any2 y z)

x     + Zero  = x
Zero  + x     = x
x     + y     | x == y = x
_     + _     = any3 Neg Zero Pos

x     - Zero  = x
Zero  - Zero  = Zero
Zero  - Neg   = Pos
Zero  - Pos   = Neg
Neg   - Pos   = Neg
Pos   - Neg   = Pos
_     - _     = any3 Neg Zero Pos
\end{code}
\caption{Abstract implementation of integers}
\label{fig:abstract_int}
\end{figure}

The Core language only has algebraic data types, specifically the types missing from a general purpose functional programming language are text characters, integers (bounded and unbounded) and floating point numbers. Catch allows for these programs by abstracting them into algebraic data constructors. Different programs may require different abstractions for the various pattern matches.

Natural numbers are often encoded by Peano numerals, and this idea can easily be extended to integers:

\begin{code}
data Nat  = Zero | Succ Nat
data Int  = Minus Nat | Zero | Plus Nat
\end{code}

While this abstraction captures all the underlying detail of the number system, it is not detail that can be used during the analysis -- due to the underlying constraint systems discussed in \S\ref{sec:constraints}. A more practically motivated example is given in Figure \ref{fig:abstract_int}, and this is the abstraction that is used in practice. The \T{any0} method is an internally implemented method of Catch, assumed to be any demonic value.

The abstraction of characters is often more practically interesting. The Haskell language standard calls for Unicode character literals -- and depending on the compiler the number of distinct characters varies between 256 and 1114111. There are several possible abstractions of characters:

\begin{code}
data Char = Char
data Char = Alpha | Digit | White | Other
data Char = A .. Z | a .. z | 0 .. 9 | ! .. # | Other
data Char = 00 | 01 | 02 ..
\end{code}

The simplest abstraction is that all characters are the same. A slightly more refined abstraction is to partition the characters based on the character testing functions provided in the \C{Char} module of Haskell. Refining the model further can give special status to characters that commonly occur, giving all uncommon characters the \C{Other} value. Finally, given that \C{Char} is a finite enumeration, the entire range of characters could each be represented distinctly.

In practice, programs often require different levels of abstraction for characters. Many will accept the most basic abstraction, some require certain literals to be represented precisely - for example a noughts and crosses\footnote{Tic-tac-toe, for US readers.} program matches on 'X' and 'O'.

The final issue of abstraction relates to primitive functions, for example the \C{getArgs} function which returns the command line arguments, or the \C{readFile} which reads from the underlying filesystem. In most cases the most appropriate model is to return \C{any0}. In some special cases, for example the CPU tick count, a slightly more precise answer can be given -- namely a positive integer.


\subsection{Constraints}

An expression evaluates to a (potentially infinite) data structure, or to $\bot{}$ caused by either non-termination of pattern match error. If an expression does evaluate to a data structure, then a constraint states the possible forms of data value it may take. A constraint is a set of data structures that a value may match. In order to practically represent a constraint, there are several different concrete representations -- these will be discussed in \S\ref{sec:constraints}.

Given a mechanism of expressing some set of constructors, $E(x) \in C$ states that the expression $x$ must, when evaluated be a member of the set defined by $C$. Since the Core language is typed, the constraint $C$ will only refer to a well typed term. These atomic constraints can be built up into a proposition of constraints, each possibly on different variables.

Since free variables are bound by function calls, it is possible to scope a constraint by making it a proposition. These can then be placed into a proposition. For example, $\forall f, i \in C$ states that for the function $f$, the argument $i$ must be in the set $C$. To take a concrete example, one particular example is $\forall \C{risers4}, x \in \g{\_ : \_}$.

These constraints are sufficient to express many useful properties. Several underlying models are possible, and will be discussed later.

\subsection{Constraints - Pattern Matching}

\begin{figure}
\begin{code}
type Constraint = [Match]

data Match  =  Match CtorName [Match]
            |  Any

data Req = Expr :< Constraint

notin :: CtorName -> Constraint
(|>) :: FieldName -> Constraint -> Constraint
(<|) :: Expr -> Constraint -> Prop Req
\end{code}
\caption{Constraint language}
\label{fig:constraint}
\end{figure}

The simplest constraint language can be seen as that which corresponds to Haskell pattern matching. A constraint is a set of pattern matches, following the data structure given in figure \ref{fig:constraint}. A given data structure can be said to be a member of this constraint if there exists a match which would allow the data structure.

This given constraint language is not very powerful -- for example it is impossible to state in a finite space that all the elements of a list are the literal True. However, even with this limited constraint language the Risers example can be proven safe.

The analysis framework will be introduced using these constraints as a suitable example - but in  reality these constraints are not used. A Req is an expression and a constraint pair, stating that the expression must evaluate to something in the constraint. There are 3 operations which must be provided on every constraint implementation, whose signatures are given in figure n. Of these |(||>)| and |(<||)| are discussed in \S\ref{sec:backward}. The \C{notin} constraint is simpler, this generates a constructor which does \textit{not} match the constructor given. For example, an implementation for the above constraint language would be:

\begin{code}
notin c = map f (delete c (ctors c))
   where f x = Match x (replicate (arity c) Any)
\end{code}

An assumption made about the constraint system is that for any given finite type, there exist only a finite number of constraints. This simple constraint system violates property -- consider a recursive data structure such as a list, an infinite number of type-correct patterns are possible.

\subsection{Safety Preconditions}

\begin{figure}
\begin{code}
pre :: Expr -> Prop Req
pre (Var   x         ) = True
pre (Path  x   _     ) = pre x
pre (Make  _   xs    ) = and (map pre xs)
pre (Call  fn  xs    ) = preFunc fn xs && and (map pre xs)
pre (Case  on  alts  ) = and [f c e | (c,e) <- alts]
    where f c e = on :< notin c || pre e

preFunc :: FuncName -> [Expr] -> Prop Req
\end{code}
\caption{Precondition of an expression, \C{pre}}
\label{fig:precondition}
\end{figure}

There exists a precondition for every function such that if this precondition holds, then all function calls made will have their preconditions respected. The precondition on the \C{error} function is False. The process of analysis can be seen as deriving these preconditions.

A function is safe if the precondition on \T{main} is True. Given a function that returns the precondition on a functions arguments \T{preFunc}, a function to determine the precondition on an expression can be specified as \T{pre} in figure \ref{fig:precondition}. A fixed point is found iff:

\begin{code}
forall f, xs `dot` preFunc f xs => pre (instantiate f xs)
\end{code}

This states that the preconditions assigned to each function by \T{preFunc} must imply the preconditions calculated on the instantiation of a function with its arguments. One such fixed point is that all calls to \T{preFunc} return False. A perfect precondition would be such that \T{preFunc f xs == pre (instantiate f xs)}.

The way preconditions are found for functions is that initially all functions are considered to have the precondition True, apart from error, which has the precondition False. For each precondition that has changed (initially error) all those functions which call this function have their bodies checked, assuming their current precondition. If the precondition has changed, then the new precondition is anded with the current one, and marked as changed. Once all preconditions are stable, a fixed point has been found.

The reason for anding with the previous value is to ensure termination -- the and ensures that the fixed point only ever becomes more restrictive, and therefore given a constraint model with a finite number of terms, is guaranteed to terminate.

A fixed point can be obtained more quickly by first propagating the functions at the leaf of the call tree. Once these have found a fixed point, it is only possible for the functions they call to require further computation with this expression. This is achieved by constructing a call graph, and taking a flattening to get an effective order for propagation. This can substantially reduce the amount of work required.


\subsection{Manipulating constraints}
\label{sec:backward}

Constraints on free variables can be considered as preconditions to the function they are scoped over. Constraints on function calls have to be treated specially, in a manner detailed in \S\ref{sec:template}, using a process called templating. All other constraints can be reduced using the process denoted by a function \T{back}, which takes a constraint and returns a predicate over constraints. This function is detailed in Figure~\ref{fig:backward}.

\begin{figure}
\begin{code}
back :: Req -> Prop Req
back (Var   x         :< k) = Var x :< k
back (Path  x   p     :< k) = x :< (p |> k)
back (Make  c   xs    :< k) = Make ct args <| k
back (Call  fn  xs    :< k) = template k fn args
back (Case  on  alts  :< k) = and [f c e | (c,e) <- alts]
    where f c e = (on :< notin c) || back (e :< k)

backs :: Req -> Prop Req
backs x = mapProp f (back x)
    where  f (Var x :< k) = Var x :< k
           f x = backs x
\end{code}
\caption{Specification of backward analysis, \C{back}}
\label{fig:backward}
\end{figure}

\begin{description}

\item[The \C{Path} rule] moves the condition from the expression to the path. The |(||>)| operation can be seen as extending a constraint from being on one small part of the data structure, to being on the entire data structure. The implementation of this operation on the simple constraint type is now given:

\begin{code}
p |> k = map extend k
    where
    extend x = Match c (anys i ++ [x] ++ anys (n-i))
    anys j = replicate j Any
    i = pathPosn p
    c = pathCtor p
    n = arity c
\end{code}

\item[The \C{Make} rule] deals with an application of a constructor. The |(<||)| operator changes to the constraint from one on the entire structure, to one on each of the parts of the constructor.

\begin{code}
(Make c xs) <| k = or (map f k)
    where
    f Any = Any
    f (Match c2 xs2) = c2 == c &&
        and (zipWith (:<) xs (map (:[]) xs2))
\end{code}


\item[The \C{Case} rule] generates a conjunct for each alternative. The generated condition says either the subject of the case analysis has a different constructor (so this particular alternative is not executed in this circumstance), or the right hand side of the alternative is safe given the conditions for this expression.
\end{description}

The \T{backs} function repeatedly applies these rules until the only remaining constraints will be either on free variables, or function application. Function application is dealt with by templating, to reduce the problem to one on only free variables.

Once a constraint is on free variables, it is treated as a precondition to a function. It is then propagated to all the callers of the given function, and the process is repeated.

\subsection{Templating}
\label{sec:template}

When a constraint of the form \T{Call f xs :< k} is found, this needs to be reduced to a proposition of constraints on the expressions in \T{xs}. There are two strategies that can be used. One method is to use the $\beta$ substitution rule (\T{instantiate}) from $\lambda$-calculus, and replace the call with the body of \T{f}. Another alternative is to create fresh variables for each element in \T{xs}, solve this new problem, and then instantiate the results.

An earlier version of the Catch tool chose the former method, but the current version chooses the latter. The later is more amenable to finding a fixed point, and allows for a cache of the results to be built from which future questions can be answered. This cache of results can be seen as a set of axioms, and indeed this cache is saved to a file for future examination after Catch has finished.

The primary template mechanism, and its essential condition are given as:

\begin{code}
template :: Constraint -> FuncName -> [Expr] -> Prop Req

forall f `dot` p xs => backs (instantiate f xs :< k)
    where p = template k f
\end{code}

The condition can be read as given a function call and a constraint, the resulting condition on the arguments must ensure that the condition holds on the instantiated body.

The mechanism used to calculate a fixed point is to execute the template function for one iteration, on the body of the function instantiated with free variables for each argument. After the result is obtained, it is anded with the existing value. If any other template is invoked within the template call then this must be considered at the same time as the current template. Each template is checked in turn until all templates do not change.

Given that there are a finite number of constraints, there are only a finite number of constraint/function pairs, which guarantees that this function terminates.


\section{Real Constraints}
\label{sec:constraint}

There are various interpretations of constraints, here I outline two that have been used in various versions of Catch. Neither is strictly more powerful than the other, both are capable of expressing constraints that the other system cannot.

\subsection{Semantics of Constraints}

Before discussing various types of constraint, it is useful to have a way of expressing which values a constraint accepts. It turns out that the |(||>)| operator introduced earlier already provides all the necessary information:

\begin{code}
data Value = Value CtorName [Value]

accept :: Value -> Constraint -> Bool
accept v k = backs (f v :< k)
    where f (Value c xs) = Make c (map f xs)
\end{code}

If you assume that the |(||>)| operator reduces a constraint to the arguments to the \C{Make}, then this results in only constraints on \C{Make} being generated -- since all children are guaranteed to be \C{Make}. This means that operator alone is responsible for determining the semantics of the constraint system.

\subsection{Regular Expression Based}

\begin{figure}
\begin{code}
data Constraint = RegExp :! [CtorName]

data RegExp = [Atom]

data Atom = Atom PathName
          | Star [PathName]

notin :: CtorName -> Constraint
notin c = [] :! delete c (ctors c)

(|>) :: PathName -> Constraint -> Constraint
p |> (r :! c) = integrate p r :! c

(<|) :: Expr -> Constraint -> Prop Req
(Make c xs) <| (r :! cs) =
    ewp r => c `elem` cs &&
    and (zipWith f (paths c) xs)
    where
    f p x = case differentiate p r of
                Nothing -> True
                Just r2 -> x :< (r2 :! cs)

ewp :: RegExp -> Bool
ewp x = all isStar x
   where isStar (Star _) = True
         isStar (Atom _) = False

integrate :: PathName -> RegExp -> RegExp
integrate p r | not (isRec p) = Atom p : r
integrate p (Star ps:r)  | p `elem` ps  = Star ps : r
                         | otherwise    = Star (p:ps) : r
integrate p r = Star [p] : r

differentiate :: PathName -> Path -> Maybe Path
differentiate p [] = Nothing
differentiate p (Atom  r:rs)  | p == r     = Just rs
                              | otherwise  = Nothing
differentiate p (Star  r:rs)  | p `elem` r  = Just (Star r:rs)
                              | otherwise   = differentiate p rs
\end{code}
\caption{Regular expression based constraints}
\label{fig:regexp}
\end{figure}

The original Catch tool used regular expression based constraints. A data type for the constraint, along with the essential operations upon it is given in Figure \ref{fig:regexp}. Given a constraint |(r :! c)|, |r| is referred to as the regular expression, and |c| is referred to as the set of constructors. Such a constraint matches those data values such that any well-defined application of a path of selectors described by |r| must reach a constructor in the set |c|.

The meaning of a constraint is defined by:

\begin{code}
e :< (r :! c) <=> (forall l `elem` lang r `dot` defined e l => ctor(e-*l) `elem` c)
\end{code}

Here |lang r| is the language represented by the regular expression |r|; \C{defined} returns true if a path selection is well-defined; and \C{ctor} gives the constructor used to create the data. Of course, since |lang r| is potentially infinite, this cannot be checked by enumeration.

If no path selection is well-defined then the constraint is vacuously true.

A regular expression is defined as:\\ \\
\begin{tabular}{ll}
|s+t| & union of regular expressions $s$ and $t$ \\
$s\D t$ & concatenation of regular expressions $s$ then $t$ \\
$s\K$  & any number (possibly zero) occurrences of $s$ \\
\C{x} & a path, such as \C{hd} or \C{tl} \\
$\lambda$ & the language is the set containing the empty string \\
$\phi$ & the language is the empty set
\end{tabular} \\

The differentiation operation is that as defined by \citet{conway}, and called quotient in some text books. The empty word property (\C{ewp}) is equivalent to |lambda `elem` lang r|, and can be calculated simply from a regular expression. Integration is merely the inverse of differentiation -- not usually treated as a separate operation in most regular expression systems.

In the original version of Catch regular expressions were based on the full regular expression language. However the implementation above is limited to concatenation of atoms, or stars of unions. Formally the context free grammar of this type of regular expression is:

\[\begin{array}{lllll}
r & = & \lambda & || & a\D r \\
a & = & \C{x}   & || & u\K \\
u & = & \C{x}   & || & \C{x} + u
\end{array}\]

There are some additional restrictions on regular expressions: all paths which are recursive must be under \C{Star} and all which are not must be \C{Atom}. Two \C{Star}'s are not allowed to be concatenated to each other.

This restricted regular expression language, combined with $\phi$, is closed under integration and differentiation. The $\phi$ alternative is catered for by the \T{Maybe} return type in the differentiation. The constraint |phi :! c| always evaluates to True, so in |(<||)|, \C{Nothing}\ is replaced by True.

The constructors, because of static typing and the restricted form of regular expression, must all by of the same type.  Given a finite type, these restrictions are enough to ensure that there are only a finite number of regular expressions. Combined with the finite number of constructors, this is enough to guarantee finiteness, which satisfies the termination properties assumed on constraints.

Two examples of constrains are:

\begin{code}
e :< ([] :! [":"])
\end{code}

This constraint says that the expression must be a cons.

\begin{code}
e :< ([Star ["tl"], Atom "hd"] :! [":"])
\end{code}

This constraint says that a list must consist of |(:)| constructed elements. If the list is empty, this still satisfies the constraint. If the list is infinite then the condition applies to all elements, constraining an infinite number. Note that this was not possible with the simple constraint framework introduced earlier.

\subsection{Propositions on Regular Expression Constraints}

The underlying type that is manipulated by Catch is not a constraint, but a proposition of constraints. These propositions are compared for equality to obtain a fixed point, and are variously manipulated by the system. Since the complexity of performing an operation is often proportional to the number of constraints in the proposition, having simplification rules can lead to a big win.

From the definition of the constraints it is possible to construct a number of identities which can be used for simplification.

\begin{description}
\item[Exhaustive conditions:] in the constraint |e :< (r :! [":","[]"])| the condition lists all the possible constructors, if |e| reaches weak head normal form then because of static typing |e| must be one of these constructors, therefore this constraint simplifies to True.

\item[Constraint merging:] given |e :< (r :! c1) `and` e :< (r :! c2)|, this can be replaced by |e :< (r :! (c1 `union` c2))|.

\item[More] more constraints are in the source code, move a few of the better ones to this document.
\end{description}

In addition to having useful simplification rules, by having a more accurate equality test, a fixed point can be reached earlier. A standard algebraic proposition is useful, but testing for the equality of two propositions is a challenge. If you have an ordering over constraints, then you can use a Binary Decision Diagram (BDD) which guarantees a unique normal form for each proposition. The other advantage of BDD's is that (assuming every term is independent) there will be no terms remaining which do not directly effect the truth of a proposition. In the above constraints terms are not independent, but they are sufficiently close to make this a useful property.

The Catch system operates primarily on algebraic propositions, using the simplification rules where possible. Operations like factorisation and expansion of propositions are not performed -- while they can lead to additional simplification, they can also lead to an increase in the proposition size. Before equality is performed, the proposition is first turned into a BDD and normalised -- this helps to obtain a fixed point quicker.

\subsection{Enumeration Based}


\begin{figure}
\begin{code}
type Constraint = [Val]

data Val   =  Part :* Part
           |  Any
data Part  =  [CtorName] :| [(PathName,Val)]

-- useful auxiliary, the non-recursive paths
fields :: [CtorName] -> [PathName]
fields = filter (not . isRec) . concatMap paths

notin :: CtorName -> Constraint
notin c = (delete c cs :| as) :* (cs :| as)
    where
    cs = ctors c
    as = zip (fields cs) (repeat True)

(|>) :: PathName -> Constraint -> Constraint
p |> vs = notin c ++ map f vs
    where
    c = pathCtor p; cs = ctors c; xs = fields cs
    as = zip xs (repeat True)

    f Any = Any
    f (v1 :* v2) | isRec p = ([c] :| as) :* (v1 `mergePart` v2)
    f v = ([c] :| fs) :* (cs :| as)
        where fs = [(x, if x == p then v else Any) |  x <- xs]

(<|) :: Expr -> Constraint -> Prop Req
(Make c xs) <| vs = or (map f vs)
    where
    f Any = Any
    f ((c1 :* v1) :| r) = c `elem` c1 && and (zipWith g (paths c) xs)
        where g p x = if isRec p then r :| r else lookup p v1

(`mergeVal`) :: Val -> Val -> Val
(a1 :* b1) `mergeVal` (a2 :* b2) = (mergePart a1 a2) :* (mergePart b1 b2)

mergePart :: Part -> Part -> Part
mergePart (c1 :| v1) (c2 :| v2) = (c1 `intersect` c2) :| zipWith f v1 v2
    where f (p1,v1) (p2,v2) = assert (p1 == p2) (v1 `mergeVal` v2)
\end{code}
\caption{Enumeration Based Constraints}
\label{fig:enumeration}
\end{figure}

The enumeration based constraints are implemented as in figure \ref{fig:enumeration}. These constraints are similar to those given as the basic example model of constraints, but can constraint an infinite number of items, and can be represented in a finite space.

Each constraint has two parts, the first part (before the |:*|) represents the constructor and the non-recursive fields. The second part represents all recursive fields. Given a constraint |((c1 :* v1) :|| (c2 :* v2))|, each |c| is a set of constructors, and each |v| is a mapping from non-recursive field names to values. In this example, |c1| is the set of constructors allowed in the value, and |c2| is the set allowed after following \textit{any number} of recursive fields. Similarly, |v1| represents the fields allowed following non-recursive field names from the root, while |v2| lists the field names from the recursive values.

To take the examples from the regular expression based constraints:

\begin{code}
cons = ([":"] :| [("hd",Any)]) :* (["[]",":"] :| [("hd",Any)])
\end{code}

This constraint states that the first constructor is a |(:)|. Note that it is a lot more verbose than the regular expression one. Part of this is due to the explicit nature -- even though no conditions are introduced on \C{hd}, the mapping from \C{hd} to \C{Any} is given explicitly.

Taking the next example from regular expressions:

\begin{code}
(["[]",":"] :| [("hd",cons)]) :* (["[]",":"] :| [("hd",cons)])
\end{code}

This example reuses the \C{cons} constraint introduced before, in the interest of brevity. Again this constraint is much larger than the regular expression, partly because it fills out all the details.

The proof of finiteness for these constraints is relatively easy, given a finite type.

One operation on enumeration constraints that is not given here is normalisation. One way to optimise a constraint would be to remove duplicates. Using the relationship between constraints there are additional reductions that are possible. The normalisation operation is complex, but not difficult. The use of normalisation allows for a fixed point to be found more rapidly.

\subsection{Enumeration Based Propositions}

The enumeration constraints have different power to the regular expression constraints, but their primary advantage comes when they are combined into propositions. The massive advantage is that if a given two constraints on the same expression, combined at the proposition level, this can be reduced into one constraint:

\begin{code}
(e :< v1) || (e :< v2) = e :< (v1 ++ v2)
(e :< v1) && (e :< v2) = e :< [a `mergeAnd` b | a <- v1, b <- v2]
\end{code}

Here the list comprehension in the |(&&)| rule is being used to generate the cross-product of the two lists.

This power to reduce constraints on equal expressions can be exploited further by translation of the program at the beginning. If every function takes exactly one argument, this guarantees that all expressions will result in being reduced to the same expression, as they will all be reduced to variables, and only one variable is present in each function. The easy way to perform this translation is to introduce tuples where a function would have more than one argument.

Combining these two things, this allows a \C{Prop Req} to be equivalent to \C{Req} in power. This limits the number of possible terms, and ensures that a fixed point is found faster.

While the introduction of tuples would not harm the regular expression constraint system, due to the simplification rules present, they would not eliminate propositions. None of the simplification rules are unable to optimise between discrete components in a tuple, leading to no benefit at the cost of a slightly obfusticated program.

\subsection{Comparison of Constraints}

Both constraint systems are capable of expressing a wide range of values. There are some areas where one is capable of expressing different things. To give an example of the power, here I give examples of two concrete values where one constraint language can differentiate between them, and the other cannot.

Take the following two values: |(T:[])| and |(T:T:[])|, using the enumeration based constraint of |(([":"] :| [("hd",Any)]) :* (["[]"] :| [("hd",Any)]))| the first matches but the second does not. Using propositions over regular expression constraints there is no way to differentiate between these two values.

Taking a tree like data structure:

\begin{code}
data Tree a  =  Branch {left :: Tree a, right :: Tree a}
             |  Leaf a

v1 = Branch (Leaf True   ) (Leaf False  )
v2 = Branch (Leaf False  ) (Leaf True   )
\end{code}

Taking the values |v1| and |v2|, the regular expression constraint |([Star ["left"]] :! ["True"])| matches |v1| but not |v2|. Using enumeration based constraints, there is no way to differentiate between these two values.

The final Catch system uses enumeration based constraints, for their propositional simplification rules, which is critical in obtaining a fixed point quickly.

\section{Converting Haskell to Core}
\label{sec:transform}

The full Haskell language is a bit unwieldy for analysis. In particular the syntactic sugar complicates analysis by introducing more types of expression to consider. The checker works instead on a simplified language, a core to which other Haskell programs can be reduced. This core language is a functional language, making use of case expressions, function applications and algebraic data types. The abstract syntax tree of expressions is discussed in \S\ref{sec:core}.

\subsection{Yhc Core}

In order to generate a simplified language, it is natural to start with a full Haskell compiler, and we chose Yhc, a fork of nhc. The internal language of Yhc is called PosLambda -- a simple variant of lambda calculus without types, but with source position information. Yhc works by applying basic desugaring transformations, without optimization transformations. This simplicity ensures the generated PosLambda is close to the original Haskell in its structure. Each top level function in a source file maps to a top level function in the generated PosLambda, retaining the same name.

PosLambda does have some constructs that have no direct representation in Haskell, for example there is a FatBar construct, which is used for compiling pattern matches which require fall through behaviour. The PosLambda language was always intended as an internal representation, and exposes certain details that are specific to the compiler. We have introduced a new Core language to Yhc, intended as a simple subset of Haskell where possible, on to which PosLambda can easily be mapped. This Core language is not explicitly typed, and has very few constructs. We have also written a library, Yhc.Core, which is used by Yhc to generate these Core files, and by external programs to load and
manipulate the generated Core.

The importance of this Core language is not what remains, but what has been removed.

\begin{itemize}
\item No syntactic sugar such as list comprehensions, \T{do} notation etc.
\item Only simple \T{case} statements, matching only the top level constructor
\item All \T{case} statements are complete, including an \T{error} call if necessary
\item All names are fully qualified
\item Haskell's type classes have been removed (see \ref{sec:dict})
\item Only top level functions remain, all local functions have been lambda lifted
\item All constructor applications are fully saturated
\end{itemize}

\subsection{The Dictionary Transformation}

Most of the transformations in Yhc operate at a very local level, either on a single function at a time or on a small expression within a function. The only analysis/transformation phases which require information about more than one function are type checking and the dictionary transformation, used to implement type classes.

\yesexample

Take the following code:

\begin{code}
f :: Eq alpha => alpha -> alpha -> Bool
f x y = x == y || x /= y
\end{code}

is translated by Yhc into

\begin{code}
f :: (alpha -> alpha -> Bool, alpha -> alpha -> Bool) -> alpha -> alpha -> Bool
f dict x y = (||) (((==) dict) x y) (((/=) dict) x y)

(==) (a,b) = a
(/=) (a,b) = b
\end{code}

The \C{Eq} class consists of two functions, |(==)| and |(/=)|. Depending on the type of the |alpha| variable in the function \C{f}, different code will be executed for the two functions.

\noexample

The dictionary transformation generates a tuple, containing the functions to use for the type class, and passes this structure into the \C{f} function as an additional argument. The additional argument can be seen as the dictionary, with functions being looked up in the dictionary to obtain the correct behaviour at runtime.

The dictionary transformation is a global transformation. The \C{Eq} context in \C{f} requires a dictionary to be accepted by \C{f}, and requires all the callers of \C{f} to pass a dictionary as well.

The type class mechanism in Haskell allows certain classes to require other classes, for example a definition of \C{Ord} requires a definition of \C{Eq} for the same type. In response the dictionary transformation generates a nested tuple, where the \C{Eq} dictionary is given as a member of the \C{Ord} dictionary.

\subsection{First Order Haskell}

Having a simple Haskell-like language, there are essentially 2 features that Haskell has that can be lived without, laziness and higher-order functions. It is possible to convert Haskell to be a first order language by using defunctionalization, and it is possible to convert Haskell to be a lazy language by continuation passing. For the purposes of this checker, laziness is a nice property, as it allows the code to be treated much like a set of maths equations, without worrying about evaluation order etc. Higher order functions are less helpful, and in fact obscure the flow of control within a program -- their removal is beneficial.

\subsection{Reynold's style defunctionalization}

Reynold's style defunctionalization is a simple method of generating a first order program from a higher order one. Taking the following example:

\begin{code}
map f x = case x of
               [] -> []
               (a:as) -> f a : map f as
\end{code}

Here \C{f} is a higher order input, of type |(a -> b)|. Defunctionalisation works by creating a data type to represent all possible f's, and using that. For example:

\begin{code}
data Functions = Head | Tail

apply Head x = head x
apply Tail x = tail x

map f x = case x of
    [] -> []
    (a:as) -> apply f a : map f as
\end{code}

Now all calls to map head would be replaced with |map Head|, and \C{Head} is a first order value. This scheme can easily be extended to currying, let us take the \C{add} function, which adds two \C{Int}'s:

\begin{code}
data Functions = Head | Tail | Add0 | Add1 Int

apply Add0 n = Add1 n
apply (Add1 a) b = add a b
\end{code}

There are a couple of things to note about this approach. One is that while this is still type safe, to do type checking would require a dependently typed language. This is unfortunate, but for our checker (which does not use type information), this is acceptable. The unacceptable aspect is the creation of an apply function, whose semantics are very general. This essentially introduces a bottleneck through which various properties must be proven. Asking questions, such as is the result of \C{apply} a \C{Cons} or a \C{Nil}, are confusing.

As such, while Reynold's style defunctionalization is acceptable, it is not the ideal method for removing higher order functions.

\subsection{Specialisation}

One technique that can be used to remove higher order functions, and indeed was employed in the original version of Catch, is specialisation. A mutually recursive group of functions can be specialised in their $n$th argument if in all recursive calls this
argument is invariant.

Examples of common functions whose applications can be specialised in this way include \T{map}, \T{filter}, \T{foldr} and \T{foldl}.

When a function can be specialised, the expression passed as the $n$th argument has all its free variables passed as extra arguments, and is expanded in the specialised version. All recursive calls within the new function are then renamed.

\yesexample

\begin{code}
map f xs = case xs of
                []     -> []
                (a:as) -> f a : map f as

adds x n = map (add n) x
\end{code}

\noindent is transformed into:

\begin{code}
map_adds n xs = case xs of
                     []     -> []
                     (a:as) -> add n a : map_adds n as

adds x n = map_adds n x\noexample
\end{code}

Although this firstification approach is not complete by any means, it is sufficient for a large range of examples. This approach has the unfortunate effect of specialising more than just higher order functions, which can lead to obscured code. It also has complications with dictionaries and point-free style code.

\subsection{Higher Order Removal}

A new technique of higher order function removal has been developed. Although this technique is not complete, in practice it is very powerful. The algorithm combines specialisation with inlining, and only removes higher order functions.

The algorithm proceeds as follows. First two additional constructions are introduced into the expression data type:

\begin{code}
data Expr  =  ...
           |  Part Int FuncName [Expr]
           |  Apply Expr [Expr]

Part 0 fn xs == Call fn xs
Apply (Part n fn xs) ys == Part (n - length ys) fn (xs ++ ys)
\end{code}

The \C{Apply} constructor is simply application, which allows an unsaturated function call, or a variable to be used as the function. The \C{Part} constructor is used to represent unsaturated function calls, leaving the normal \C{Call} constructor to represent saturated calls. The \C{Part} keeps track of how many missing arguments it has. To express a higher-order program requires \C{Apply}. Annotating a higher order program with \C{Part} as appropriate is a simple operation. This algorithm assumes that \C{Make} is always fully saturated -- which is easy to achieve with the introduction of new functions as necessary.

\begin{figure}
\begin{code}
isHO :: Expr -> Bool
isHO (Part n _ _)    = n > 0
isHO (Make _ xs)     = any isHO xs
isHO (Case on alts)  = any (isHO . snd) alts
isHO _               = False
\end{code}
\caption{Tests for the firstifier}
\label{fig:firstify}
\end{figure}

The algorithm has two separate stages, specialisation and inlining. The algorithm applies the specialise rule until a fixed point is reached, then applies the inline rule once. The algorithm repeats these two steps until a fixed point is reached. Given an appropriate \C{fix} function, this can be specified as:

\begin{code}
firstify = fix (inline . fix specialise)
\end{code}

\paragraph{Inline:} The \C{inline} stage of the algorithm is much simpler than the specialisation aspect, and hence is treated first. A function is inlined once if the body passes the \C{isHO} test. If this causes a function to no longer be called from the root of the program then the function is removed after inlining.

\paragraph{Specialise:} The \C{specialise} stage of the algorithm takes expressions of the form |Call fn xs| where |any isHO xs|, and generates a specialised version of the function \C{fn} where all the higher order elements are frozen in, and all the other variables are passed normally.

Assuming that a function |fn| results in the specialised version |fn'|, then the translation would be:

\begin{code}
transform (Call fn xs) | any isHO xs = Call fn' (concatMap f xs)
    where
    fn' = generate fn xs

    f x  | isHO x     = freeVars x
         | otherwise  = [x]
\end{code}

This technique is very powerful -- there is no program without a higher order initial input which will not have a rule available for firing. The fact that a higher order function has ``no where to hide'' can be seen easily. A higher order function must either be the first thing in a function (in which case it will be inlined), or it must be the argument to a function (in which case it is specialised). There are only two places left for a higher order function:

\begin{enumerate}
\item As the scrutinee of a |case| expression. This is not possible due to static typing -- all |case| expressions must choose over a data value.

\item Inside an \C{Apply} with a variable as the function. This is does not happen if all other higher order functions have been removed, and hence is safe.
\end{enumerate}

There are some sticky points to this translation, one is the \C{seq} primitive of Haskell, which has no natural translation to Core, and is usually expressed as a primitive function. The \C{seq} command is special in that it is a primitive which is polymorphic in both arguments. If a higher order function is the first argument of \C{seq} then the \C{seq} can be removed, and replaced with the second argument. If the second argument is a higher order function, but the first function is guaranteed not to be, then this can be replaced with a |case| expression. In Haskell |case x of _ -> y| would not evaluate |x|, however in the Core language given this would.

The next tricky point is \C{undefined}, consider the program:

\begin{code}
main = f undefined
f x = x 1
\end{code}

This program passes \C{undefined} as a function, which is allowable as \C{undefined} has a fully polymorphic result. This can be worked around by treating \C{undefined} (and by extension, \C{error}) as higher order functions.

Alternative methods are available for full firstification, such as that detailed by Hughes \cite{hughes:type-spec}.

There are of course disadvantages to specialisation, this is essentially a whole program analysis transformation, and the performance is not stunning. However, in practice it seems that the generated code corresponds much more closely to what the original author had meant, and is significantly shorter -- which is not a massive surprise, Jones has shown this result in the specific context of type classes before.


\section{Results and Evaluation}
\label{sec:results}

\subsection{Small Examples}

Map head, head reverse etc.


\subsection{Case Studies}

adjoxo, soda, clausify

\subsection{Nofib}

The whole nofib benchmark, in detail

Most of the benchmarks use [x] <- getArgs, this will always fail as getArgs is a primitive whose implementation is opaque to the system. As a result the system will always return False as the precondition to main.

The next step where many fail is that read is applied to an argument extracted from getArgs. Firstly this argument is entirely unknown. Secondly read is a sufficiently complicated function that while it can be modeled by Catch, there is no possibility for getting an appropriate abstraction which models the failure case. As a result, any program which calls read is unsafe, according to Catch. Using reads is perfectly sufficient, if the failure condition is appropriately handled.

Imaginery:

bernouilli - blows up going through firstify

digits-of-e1

digits-of-e2

exp3-8

gen-regexps

integrate

paraffins

primes

queens - fails in let elim (need to write abstract before let-elim)

rfib

tak - no pattern match errors, trivially safe

wheel-sieve1

wheel-sieve2

x2n1 - no pattern match failures, but has division and ratio's. Underneath there are calls to the ratio library which do plenty of division, and one explicit division. All are proved safe using neg,zero,pos abstract interpretation. Requires an abstract interpretation for things like gcd etc. Gives preconditions such as the f function must operate only on non-zero numbers. Nice encoding of division as a pattern match failure.

\section{Related Work}
\label{sec:related}

Viewed as a \textbf{proof tool} this work can be seen as following Turner's goal to define a Haskell-like language which is total \cite{tfp:total}. Turner disallows incomplete pattern matches, saying this will ``force you to pay attention to exactly those corner cases which are likely to cause trouble''. Our checker may allow this restriction to be lifted, yet still retain a total programming language.

Viewed as a basic \textbf{pattern match checker}, the work on compiling warnings about incomplete and overlapping patterns is quite relevant \cite{ghc,pattern_match}. As noted in the introduction, these checks are only local.

Viewed as a \textbf{mistake detector} this tool has a similar purpose to the classic C Lint tool \cite{lint}, or Dialyzer \cite{dialyzer} -- a static checker for Erlang. The aim is to have a static checker that works on unmodified code, with no additional annotations. However, a key difference is that in Dialyzer all warnings indicate a genuine problem that needs to be fixed. Because Erlang is a dynamically typed language, a large proportion of Dialyzer's warnings relate to mistakes a type checker would have detected.

Viewed as a \textbf{soft type system} the checker can be compared to the tree automata work done on XML and XSL \cite{static_xslt}, which can be seen as an algebraic data type and a functional language. Another soft typing system with similarities is by Aiken \cite{type:dynamic}, on the functional language FL. This system tries to assign a type to each function using a set of constructors, for example \T{head} is given just \T{Cons} and not \T{Nil}.

My previous paper (not called Catch!) had lots of todo items on it, all have since been done

ESC/Haskell is fully manual.


\section{Conclusions and Future Work}
\label{sec:conclusion}

I conclude that Catch is a useful tool.




% \appendix
% \section{Appendix Title}
%
% This is the text of the appendix, if you need one.

\acks

Thanks to the Plasma people, especially Matt and Tom for fruitful discussions.

References need to be much much longer.

\bibliographystyle{plainnat}
\bibliography{catch}



\end{document}
