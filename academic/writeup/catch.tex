\documentclass[preprint]{sigplanconf}

\usepackage{amsmath}

\begin{document}

\conferenceinfo{ICFP '07}{date, City.} %
\copyrightyear{2007} %
\copyrightdata{[to be supplied]}

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Catch}
\subtitle{A Technical Overview}

\authorinfo{Neil Mitchell}
           {York}
           {ndm}
\authorinfo{Colin Runciman}
           {York}
           {colin}

\maketitle

\begin{abstract}
A Haskell program may fail at runtime with a pattern-match error if the program
has any incomplete (non-exhaustive) patterns in definitions or case
alternatives. This paper describes a static checker that allows non-exhaustive
patterns to exist, yet ensures that a pattern-match error does not occur. It
describes a constraint language that can be used to reason about pattern
matches, along with mechanisms to propagate these constraints between program
components.
\end{abstract}

\category{CR-number}{subcategory}{third-level}

\terms
term1, term2

\keywords
keyword1, keyword2

\section{Introduction}

Introduction, todo.

\subsection{Road map}

The Catch tool can be seen as 3 entirely separate sections. Initially a program
is translated into the reduced Haskell language \S\ref{chap:yhc}. Second, the
program is transformed into a simpler first-order program with the same
semantics \S\ref{chap:defunc}. Third, a constraint language
\S\ref{chap:constraints} is used to analyse the program \S\ref{chap:backward}.
Some results are presented \S\ref{chap:results} along with some concluding
remarks \S\ref{chap:conc}.

\section{Haskell: Sugarless and Typeless}
\label{chap:yhc}

\subsection{Yhc Core}

The full Haskell language is a bit unwieldy for analysis. In particular the
syntactic sugar complicates analysis by introducing more types of expression to
consider. The checker works instead on a simplified language, a core to which
other Haskell programs can be reduced. This core language is a functional
language, making use of case expressions, function applications and algebraic
data types.

In order to generate a simplified language, it is natural to start with a full
Haskell compiler, and we chose Yhc, a fork of nhc. The internal language of Yhc
is called PosLambda -- a simple variant of lambda calculus without types, but
with positional information. Since Yhc is a relatively simple Haskell compiler,
relying on simple desugaring transformations, this means that the generated
PosLambda is reasonably close to the original Haskell in its structure.

PosLambda does have some features that have no natural parallel in Haskell, for
example there is a FatBar construct, that is used for compiling pattern
matches. The PosLambda language is also very much internal to the compiler. We
have introduced a new Core language to Yhc, very similar to both PosLambda, and
a simple subset of Haskell. This Core language is not explicitly typed, and has
very few constructs in it. We have also written a library, Yhc.Core which is
used by Yhc to generate these Core files, and can also be used by external
programs to read and manipulate the generated Core.

Another important simplification is that all case's are now complete, and where
an error can possibly occur an explicit call to error is inserted, with the
string representing the source position of the error.

\subsection{The Dictionary Transformation}

Most of the desugaring transformations performed by Yhc are at a very local
level -- for example \T{f = (+1)} becomes \T{f v1 = flip ((+) v1) (fromInteger
1)}. The biggest exception to this is the typeclass transformation, which is
based on a method called dictionary passing.

For example, take the following code:

\T{f :: Eq a => a -> a -> Bool; f x y = x == y || x /= y}

This is translated to:

f dict x y = (||) (((==) dict) x y) (((/=) dict) x y)

where

(==) (a,b) = a

and

(/=) (a,b) = b

Essentially, a tuple is passed around containing the methods in dictionary.


\subsection{Case and Paths}

The Catch internal language is differs in its representation of Case, and
introduces Paths. First we introduce a motivation for the path construct, its
full use will only be shown later in the constraint section. Let us take the
example of head, in the Core language of Yhc this comes out approximately as:

head x = case x of
            (a:as) -> a
            [] -> error "here"

Now, let us consider the case where we know that x is a (:) constructor before
executing head. Now, we can rewrite this code as:

head x = case x of
            (a:as) -> a

Unfortunately we are now using the case expression for two different purposes,
to select the elements in a known data structure, and to test the type of a
data structure at runtime. For the purposes of analysis this is even worse, and
complicates things considerably.

What if we were to introduce an extra construct, such as x.hd meaning x, taking
the hd component of a (:). Now we can rewrite head of a known (:) constructor
as:

head x = x.hd

It is impossible to have a x.hd where x is [], this is not a runtime error, but
a static guarantee imposed on the program. With this new notation, we can now
rewrite the standard head as:

head x = case x of
            (:) -> x.hd
            [] -> error "here!"

Note that now we have introduced paths, there is no need to specify names for
each of the constructors given in a case statement. There is also no need to
introduce fresh variables within a function, only the variables can be
manipulated.

\section{First Order Haskell}
\label{chap:defunc}

Having a simple Haskell-like language, there are essentially 2 features that
Haskell has that can be lived without, laziness and higher-order functions. It
is possible to convert Haskell to be a first order language by using
defunctionalisation, and it is possible to convert Haskell to be a lazy
language by continuation passing. For the purposes of this checker, laziness is
a nice property, as it allows the code to be treated much like a set of maths
equations, without worrying about evaluation order etc. Higher order functions
are less helpful, and in fact obscure the flow of control within a program --
their removal is beneficial.

\subsection{Reynold's style defunctionalization}

Reynold's style defunctionalisation is a simple method of generating a first
order program from a higher order one. Taking the following example:

map f x = case x of
    [] -> []
    (a:as) -> f a : map f as

Here f is a higher order input, of type (a -> b). Defunctionalisation works by
creating a data type to represent all possible f's, and using that. For
example:

 data Functions = Head | Tail

 apply Head x = head x
 apply Tail x = tail x

 map f x = case x of
     [] -> []
     (a:as) -> apply f a : map f as

Now all calls to map head would be replaced with map Head, and Head is a first
order value. This scheme can easily be extended to currying, let us take the
add function, which adds to Int's:

 data Functions = Head | Tail | Add0 | Add1 Int

 apply Add0 n = Add1 n
 apply (Add1 a) b = add a b

There are a couple of things to note about this approach. One is that while
this is still type safe, to do type checking would require a dependantly typed
language. This is unfortunate, but for our checker (which does not use type
information), this is acceptable. The unacceptable aspect is the creation of an
apply function, whose semantics are very general. This essentially introduces a
bottleneck through which various properties must be proven. Asking questions,
such as is the result of apply a Cons or a Nil, are confusing.

As such, while Reynold's style defunctionalisation is acceptable, it is not the
ideal method for removing higher order functions.

\subsection{Specialisation}

Often when a function is called, there is some information known about the
arguments -- for example one may be a constant. The way that the GHC program
makes use of this information is by inlining heavily, unfortunately sometimes
there is a good amount of information available, but the function is unsuitable
for inlining. Specialisation solves this problem neatly.

Let us consider for a minute the function map, defined as above. Now let us
consider the application map f []. Here it is not a great idea to inline map,
and GHC is unable to simplify this definition \footnote{GHC is able to simplify
the map provided in teh standard Prelude using rewrite rules, however by
defining map' with the above definition, no simplification is made}.

However, using specialisation, it is easy to generate map specialised with []
as:

map f = []

Of course, now even a very conservative inline pass will succeed with this,
resulting in [] as the end result.

The notation used for specialisation is given as follows. The original version
of map takes two arguments, and is given as map<?,?> -- i.e. map with two
arguments, and no additional information. The version specialised on [] is
given as map<?,[]>. Note that here there is no second argument, the specialised
version wraps this up.

Now let us consider map f (x:xs), this specialised in the same manner to use
the function map<?,?:?> f x xs = f x : map<?,?> xs. Note that the recursive
case calls the standard map, as it cannot deduce any information about xs.
Obviously this has the potential for non-termination, say the function
map<?,?:(?:(?: ...> was required. The way to make this system terminating is to
demand that the type must be new, for example it is impossible to specialise
the tail of a (:), since the type of the tail is the same as the initial list.
This simple mechanism ensures termination, and at the same time promotes good
simplification of complex argument structures.

However, coming back to the original point, removing higher order functions.
These can be specialised in exactly the same way, for example map head x can
use the specialised version map<head,?>. Curried functions can be used in the
specialisation as well, map<add ?,?> is perfectly valid.

One particular useage of this specialisation treatment is the removal of the
dictionary transformation. For example, a function requiring Int's might be
specialised to f<(int_eq,int_neq),?,?>. GHC and hbc both support a
specaialisation annotation which acheives this effect, however generic
specialisation acheives this easily.

There are of course disadvantages to specialisation, this is essentially a
whole program analysis transformation, and the performance is not stunning.
However, in practice it seems that the generated code corresponds much more
closely to what the original author had meant, and is significantly shorter --
which is not a massive surprise, Jones has shown this result in the specific
context of type classes before.

\section{A constraint language}
\label{chap:constraints}

Semantics

\subsection{An atomic constraint}

Introduce regular expressions, quantification


\subsection{Predicates on constraints}

Include the $\forall$ thing here.

\subsection{Representation of Predicates}

BDD's - why they aren't right, negation in the BDD context.

\section{Backward Analysis}
\label{chap:backward}

Backward analysis

The basic rule schema

Propagation

Templating

\section{Results}
\label{chap:results}

\subsection{Small Examples}

Map head, head reverse etc.


\subsection{Case Studies}

adjoxo, soda, clausify

\subsection{Nofib}

The whole nofib benchmark, in detail

\section{Conclusion}
\label{chap:conc}






\appendix
\section{Appendix Title}

This is the text of the appendix, if you need one.

\acks

Acknowledgments, if needed.

\begin{thebibliography}{}

\bibitem{smith02}
Smith, P. Q. reference text

\end{thebibliography}

\end{document}
1-59593-090-6/05/0007
